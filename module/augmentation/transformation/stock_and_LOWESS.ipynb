{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Title: To determine if data augmentation using the method proposed in 'Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning' will lead to better 1 day prediction results.\n",
    "\n"
   ],
   "id": "13a895133852f988"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Seed value\n",
    "seed_value= 42\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)\n"
   ],
   "id": "f1e893e41ddd851a",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import seaborn as sb\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import rfft, rfftfreq, irfft\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tcn import TCN  # If you have the tcn p /ackage installed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import optuna\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pykalman import KalmanFilter\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ],
   "id": "d2f73a74d9d2db4d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "np.random.seed(42)  # Set the random seed for reproducibility\n",
    "\n",
    "# Function to import stock data\n",
    "def get_stock_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return data\n",
    "\n",
    "def z_score_normalize(series):\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    return (series - mean) / std\n",
    "\n",
    "def denormalize_z_score(normalized_series, original_mean, original_std):\n",
    "    return (normalized_series * original_std) + original_mean\n",
    "\n",
    "# Function to create sequences and corresponding returns\n",
    "def create_sequences(data, returns, sequence_length=20):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i+sequence_length])\n",
    "        y.append(returns[i+sequence_length])\n",
    "    return np.array(X), np.array(y)"
   ],
   "id": "93cc9e8a6220db87",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# def cut_mix(data1, data2, alpha=0.2):\n",
    "#     assert len(data1) == len(data2)\n",
    "#     size = len(data1)\n",
    "#     cut_point = np.random.randint(0, size)\n",
    "#     cut_length = int(size * alpha)\n",
    "    \n",
    "#     mixed_data = np.copy(data1)\n",
    "#     mixed_data[cut_point:cut_point+cut_length] = data2[cut_point:cut_point+cut_length]\n",
    "    \n",
    "#     return mixed_data\n",
    "\n",
    "def cut_mix(df1, df2, alpha=0.2):\n",
    "    assert df1.shape == df2.shape\n",
    "    size = len(df1)\n",
    "    cut_point = np.random.randint(0, size)\n",
    "    cut_length = int(size * alpha)\n",
    "    \n",
    "    mixed_df = df1.copy()\n",
    "    mixed_df.iloc[cut_point:cut_point+cut_length] = df2.iloc[cut_point:cut_point+cut_length]\n",
    "    \n",
    "    return mixed_df\n",
    "\n",
    "def binary_mix(data1, data2, alpha=0.2):\n",
    "    assert len(data1) == len(data2)\n",
    "    size = data1.shape\n",
    "    mask = np.random.binomial(1, alpha, size=size).astype(bool)\n",
    "    \n",
    "    mixed_data = np.where(mask, data1, data2)\n",
    "    \n",
    "    return pd.DataFrame(mixed_data, columns=['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
    "\n",
    "def linear_mix(data1, data2, alpha=0.2):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    mixed_data = alpha * data1 + (1 - alpha) * data2\n",
    "    \n",
    "    return mixed_data\n",
    "\n",
    "def geometric_mix(data1, data2, alpha=0.2):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    mixed_data = data1**alpha * data2**(1 - alpha)\n",
    "    \n",
    "    return mixed_data\n",
    "\n",
    "def amplitude_mix(data1, data2, alpha=0.2):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    fft1 = np.fft.rfft(data1)\n",
    "    fft2 = np.fft.rfft(data2)\n",
    "    \n",
    "    # Mix the magnitudes\n",
    "    magnitude1 = np.abs(fft1)\n",
    "    magnitude2 = np.abs(fft2)\n",
    "    mixed_magnitude = alpha * magnitude1 + (1 - alpha) * magnitude2\n",
    "    \n",
    "    # Keep the phase of the first data\n",
    "    phase1 = np.angle(fft1)\n",
    "    mixed_fft = mixed_magnitude * np.exp(1j * phase1)\n",
    "    \n",
    "    mixed_data = np.fft.irfft(mixed_fft)\n",
    "    \n",
    "    return pd.DataFrame(mixed_data, columns=['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
    "\n",
    "### PROPOSE TECHNIQUE BELOW\n",
    "\n",
    "def proposed_mixup(df1, df2, threshold=0.1, alpha=0.5):\n",
    "    \n",
    "    def proposed_mixup_feature(data1, data2, threshold, alpha):\n",
    "        \n",
    "        def get_significant_frequencies(data, threshold):\n",
    "            \"\"\"\n",
    "            Perform Fourier Transform on data and identify frequencies with significant amplitude.\n",
    "\n",
    "            Args:\n",
    "            - data: Time series data.\n",
    "            - threshold: Threshold for significance, relative to the max amplitude.\n",
    "\n",
    "            Returns:\n",
    "            - significant_freq: Frequencies with significant amplitude.\n",
    "            - significant_ampl: Amplitude of the significant frequencies.\n",
    "            - full_spectrum: Full Fourier spectrum for all frequencies.\n",
    "            \"\"\"\n",
    "            # Perform Fourier Transform\n",
    "            spectrum = rfft(data)\n",
    "            frequencies = rfftfreq(data.size, d=1)  # Assuming unit time interval between samples\n",
    "\n",
    "            # Find significant amplitudes\n",
    "            amplitude = np.abs(spectrum)\n",
    "            significant_indices = amplitude > (amplitude.max() * threshold)\n",
    "            significant_freq = frequencies[significant_indices]\n",
    "            significant_ampl = amplitude[significant_indices]\n",
    "\n",
    "            return significant_freq, significant_ampl, spectrum\n",
    "\n",
    "        def phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2, alpha):\n",
    "            mixed_spectrum = np.copy(spectrum1)\n",
    "            freqs1 = rfftfreq(spectrum1.size, d=1)\n",
    "            freqs2 = rfftfreq(spectrum2.size, d=1)\n",
    "\n",
    "            for freq in sig_freq1:\n",
    "                index1 = np.argmin(np.abs(freqs1 - freq))\n",
    "                index2 = np.argmin(np.abs(freqs2 - freq))\n",
    "\n",
    "                if index1 >= len(sig_ampl1) or index2 >= len(sig_ampl2):\n",
    "                    continue  # Skip the frequency if the index is out of bounds\n",
    "\n",
    "                phase1 = np.angle(spectrum1[index1])\n",
    "                phase2 = np.angle(spectrum2[index2])\n",
    "\n",
    "                phase_diff = (phase2 - phase1) % (2 * np.pi)\n",
    "                phase_diff = phase_diff - 2 * np.pi if phase_diff > np.pi else phase_diff\n",
    "\n",
    "                new_amplitude = alpha * sig_ampl1[index1] + (1 - alpha) * sig_ampl2[index2]\n",
    "                new_phase = phase1 + alpha * phase_diff\n",
    "\n",
    "                mixed_spectrum[index1] = new_amplitude * np.exp(1j * new_phase)\n",
    "\n",
    "            return mixed_spectrum\n",
    "\n",
    "\n",
    "        def reconstruct_time_series(mixed_spectrum):\n",
    "            \"\"\"\n",
    "            Reconstruct time series from mixed spectrum using inverse Fourier Transform.\n",
    "\n",
    "            Returns:\n",
    "            - mixed_time_series: The reconstructed time series.\n",
    "            \"\"\"\n",
    "            # Perform inverse Fourier Transform\n",
    "            mixed_time_series = irfft(mixed_spectrum)\n",
    "\n",
    "            return mixed_time_series\n",
    "\n",
    "        # Step 1: Get significant frequencies and amplitude for both time series\n",
    "        sig_freq1, sig_ampl1, spectrum1 = get_significant_frequencies(data1, threshold)\n",
    "        sig_freq2, sig_ampl2, spectrum2 = get_significant_frequencies(data2, threshold)\n",
    "\n",
    "        # Step 2: Identify significant frequencies (already done in step 1)\n",
    "\n",
    "        # Step 3: Phase and Magnitude Mixup\n",
    "        mixed_spectrum = phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2, alpha)\n",
    "\n",
    "        # Step 4: Reconstruction of the time series\n",
    "        mixed_time_series = reconstruct_time_series(mixed_spectrum)\n",
    "        return mixed_time_series\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    \n",
    "    for feature in df1.columns:\n",
    "        output_df[feature] = proposed_mixup_feature(df1[feature].values, df2[feature].values, threshold, alpha)\n",
    "        \n",
    "    return output_df\n",
    "\n",
    "# def proposed_mixup(data1, data2, threshold=0.1, alpha=0.5):\n",
    "#     def get_significant_frequencies(data, threshold=0.1, axis=0):\n",
    "#         \"\"\"\n",
    "#         Perform Fourier Transform on data along the specified axis and identify frequencies \n",
    "#         with significant amplitude for each feature.\n",
    "\n",
    "#         Args:\n",
    "#         - data: Time series data (can be multidimensional).\n",
    "#         - threshold: Threshold for significance, relative to the max amplitude.\n",
    "#         - axis: Axis along which the Fourier Transform is applied.\n",
    "\n",
    "#         Returns:\n",
    "#         - significant_freq: Frequencies with significant amplitude for each feature.\n",
    "#         - significant_ampl: Amplitude of the significant frequencies for each feature.\n",
    "#         - full_spectrum: Full Fourier spectrum for all frequencies and features.\n",
    "#         \"\"\"\n",
    "#         # Perform Fourier Transform along the specified axis\n",
    "#         spectrum = rfft(data, axis=axis)\n",
    "#         frequencies = rfftfreq(data.shape[axis], d=1)  # Assuming unit time interval between samples\n",
    "\n",
    "#         # Find significant amplitudes for each feature\n",
    "#         amplitude = np.abs(spectrum)\n",
    "#         significant_indices = amplitude > (np.max(amplitude, axis=axis, keepdims=True) * threshold)\n",
    "\n",
    "#         # Use broadcasting to expand dimensions for proper indexing\n",
    "#         significant_freq = np.expand_dims(frequencies, axis=1) * significant_indices\n",
    "#         significant_ampl = amplitude * significant_indices\n",
    "\n",
    "#         return significant_freq, significant_ampl, spectrum\n",
    "\n",
    "#     def phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2, alpha=0.5):\n",
    "#         # Initialize the mixed spectrum with the same shape as the input spectrums\n",
    "#         mixed_spectrum = np.zeros_like(spectrum1)\n",
    "\n",
    "#         # Iterate over each feature (assuming features are along axis 1)\n",
    "#         for feature_index in range(spectrum1.shape[1]):\n",
    "#             freqs1 = rfftfreq(spectrum1.shape[0], d=1)\n",
    "#             freqs2 = rfftfreq(spectrum2.shape[0], d=1)\n",
    "\n",
    "#             # Assuming sig_freq1 and sig_freq2 are lists of arrays, one per feature\n",
    "#             for freq in sig_freq1[feature_index]:\n",
    "#                 index1 = np.argmin(np.abs(freqs1 - freq))\n",
    "#                 index2 = np.argmin(np.abs(freqs2 - freq))\n",
    "\n",
    "#                 if index1 >= len(sig_ampl1[feature_index]) or index2 >= len(sig_ampl2[feature_index]):\n",
    "#                     continue  # Skip the frequency if the index is out of bounds\n",
    "\n",
    "#                 phase1 = np.angle(spectrum1[index1, feature_index])\n",
    "#                 phase2 = np.angle(spectrum2[index2, feature_index])\n",
    "\n",
    "#                 phase_diff = (phase2 - phase1) % (2 * np.pi)\n",
    "#                 phase_diff = phase_diff - 2 * np.pi if phase_diff > np.pi else phase_diff\n",
    "\n",
    "#                 new_amplitude = alpha * sig_ampl1[feature_index][index1] + (1 - alpha) * sig_ampl2[feature_index][index2]\n",
    "#                 new_phase = phase1 + alpha * phase_diff\n",
    "\n",
    "#                 mixed_spectrum[index1, feature_index] = new_amplitude * np.exp(1j * new_phase)\n",
    "                \n",
    "#         print(pd.DataFrame(mixed_spectrum, columns=['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']))\n",
    "\n",
    "#         return mixed_spectrum\n",
    "\n",
    "#     def reconstruct_time_series(mixed_spectrum):\n",
    "#         \"\"\"\n",
    "#         Reconstruct time series from mixed spectrum using inverse Fourier Transform.\n",
    "#         Each column in mixed_spectrum corresponds to a feature.\n",
    "\n",
    "#         Args:\n",
    "#         - mixed_spectrum: 2D array where each column is the mixed spectrum of a feature.\n",
    "\n",
    "#         Returns:\n",
    "#         - mixed_time_series: 2D array where each column is the reconstructed time series of a feature.\n",
    "#         \"\"\"\n",
    "#         # Initialize an empty list to hold the reconstructed time series for each feature\n",
    "#         reconstructed_series = []\n",
    "\n",
    "#         # Perform inverse Fourier Transform for each column (feature)\n",
    "#         for i in range(mixed_spectrum.shape[1]):\n",
    "#             mixed_time_series = irfft(mixed_spectrum[:, i])\n",
    "#             reconstructed_series.append(mixed_time_series)\n",
    "\n",
    "#         # Convert the list of arrays into a 2D array where columns are features\n",
    "#         mixed_time_series = np.column_stack(reconstructed_series)\n",
    "#         return pd.DataFrame(mixed_time_series, columns=['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
    "    \n",
    "#     # Step 1: Get significant frequencies and amplitude for both time series\n",
    "#     sig_freq1, sig_ampl1, spectrum1 = get_significant_frequencies(data1)\n",
    "#     sig_freq2, sig_ampl2, spectrum2 = get_significant_frequencies(data2)\n",
    "\n",
    "#     # Step 2: Identify significant frequencies (already done in step 1)\n",
    "\n",
    "#     # Step 3: Phase and Magnitude Mixup\n",
    "#     mixed_spectrum = phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2)\n",
    "\n",
    "#     # Step 4: Reconstruction of the time series\n",
    "#     mixed_time_series = reconstruct_time_series(mixed_spectrum)\n",
    "\n",
    "#     return mixed_time_series"
   ],
   "id": "766fa6be8ef8eef3",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "def apply_lowess_smoothing(df, frac=0.1):\n",
    "    smoothed_data = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Apply LOWESS to each column\n",
    "    for column in df.columns:\n",
    "        smoothed_values = lowess(df[column], df.index, frac=frac, return_sorted=False)\n",
    "        smoothed_data[column] = smoothed_values\n",
    "    \n",
    "    return smoothed_data"
   ],
   "id": "963703436cce61af",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "def create_augmented_data(rets, df1, df2, method, alpha, window_size=20):\n",
    "    if method == 'cut_mix':\n",
    "         df = cut_mix(df1, df2, alpha)\n",
    "    elif method == 'binary_mix':\n",
    "         df = binary_mix(df1, df2, alpha)\n",
    "    elif method == 'linear_mix':\n",
    "         df = linear_mix(df1, df2, alpha)\n",
    "    elif method == 'geometrix_mix':\n",
    "         df = geometric_mix(df1, df2, alpha)\n",
    "    elif method == 'amplitude_mix':\n",
    "         df = amplitude_mix(df1, df2, alpha)\n",
    "    elif method == 'proposed_mix':\n",
    "         df = proposed_mixup(df1, df2, alpha)\n",
    "\n",
    "    # Original\n",
    "    else:\n",
    "        df = df1.copy()\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(df[df.columns].values)\n",
    "        \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(scaled_features, rets, window_size)\n",
    "    \n",
    "    return X, y, df"
   ],
   "id": "76e3ed25e8989295",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned by Optuna\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    lstm_units = trial.suggest_categorical('lstm_units', [50, 100, 150])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    \n",
    "    # Data preparation\n",
    "    scaler = StandardScaler()\n",
    "    feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    scaled_features = scaler.fit_transform(df[feature_columns])\n",
    "    sequence_length = 20  # Using 20 timesteps\n",
    "    X, y = create_sequences(scaled_features, df['Returns'].values, sequence_length)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Model architecture\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        model.add(LSTM(units=lstm_units, return_sequences=(i < n_layers - 1)))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    # Compilation\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "    \n",
    "    # Model training\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=50,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return mse"
   ],
   "id": "6aa75fcb2ed2bbff",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "def create_model(best_params, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(best_params['lstm_units'], input_shape=input_shape))\n",
    "    model.add(Dense(1))  # Assuming we are predicting one value\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ],
   "id": "d9f3a3908d94c947",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "def plot_TSNE(df1, df2):\n",
    "    df1_log = np.log(df1 + 1)  # Adding 1 to avoid log(0)\n",
    "    df2_log = np.log(df2 + 1)\n",
    "\n",
    "    combined_data = pd.concat([df1_log, df2_log])\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=100, n_iter=1000)\n",
    "    tsne_results = tsne.fit_transform(combined_data)\n",
    "\n",
    "    # Now we split the t-SNE results back into original and augmented parts\n",
    "    tsne_df1 = tsne_results[:len(df1), :]\n",
    "    tsne_df2 = tsne_results[len(df1):, :]\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.scatter(tsne_df1[:, 0], tsne_df1[:, 1], label='Original', alpha=0.5)\n",
    "    plt.scatter(tsne_df2[:, 0], tsne_df2[:, 1], label='Augmented', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "a2c5fc349fe6791",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Data from Yahoo Finance"
   ],
   "id": "5e9b3f03779e1ba0"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "start_date = '2010-01-01'\n",
    "end_date = '2023-01-01'\n",
    "\n",
    "# Define the list of Dow Jones Industrial Average companies\n",
    "tickers = [\n",
    "    \"MMM\", \"AXP\", \"AMGN\", \"AAPL\", \"BA\", \"CAT\", \"CVX\", \"CSCO\", \"KO\", \"DIS\",\n",
    "    \"DOW\", \"GS\", \"HD\", \"HON\", \"IBM\", \"INTC\", \"JNJ\", \"JPM\", \"MCD\", \"MRK\",\n",
    "    \"MSFT\", \"NKE\", \"PG\", \"CRM\", \"TRV\", \"UNH\", \"V\", \"WBA\", \"WMT\"\n",
    "]\n",
    "\n",
    "# Create a dictionary to store historical data for each company\n",
    "historical_data = {}\n",
    "\n",
    "# Loop through the Dow companies and retrieve historical data\n",
    "for ticker in tickers:\n",
    "    stock_data = get_stock_data(ticker, start_date, end_date)\n",
    "    historical_data[ticker] = stock_data"
   ],
   "id": "1c0094048b0e35b4",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original - Not needed here"
   ],
   "id": "f0bbf55b64bcbf3d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "seq_len = 20\n",
    "alpha = 0.2"
   ],
   "id": "b98c97b71acedb59",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df = historical_data['AAPL'].copy()\n",
    "rets = df['Close'].pct_change()"
   ],
   "id": "ffad5011d5eed232",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create df2 - LOWESS Smoothing"
   ],
   "id": "38cfb49adc7f371f"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "df1 = historical_data['AAPL']\n",
    "df2 = apply_lowess_smoothing(df1)"
   ],
   "id": "9abacd8af539b8a0",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CutMix"
   ],
   "id": "b29a86f2e46b62bf"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "# Create X, y for LSTM\n",
    "X, y, df3 = create_augmented_data(rets, df1, df2, 'cut_mix', alpha, seq_len)\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=50)"
   ],
   "id": "53aed0caaa53ec52",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "df3"
   ],
   "id": "a88edbbcb70a26ca",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "# # Best hyperparameters\n",
    "# print('Number of finished trials:', len(study.trials))\n",
    "# print('Best trial:', study.best_trial.params)"
   ],
   "id": "d65e517d7b551f4",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best trial: {'n_layers': 1, 'lstm_units': 50, 'dropout_rate': 0.3754947803049919, 'learning_rate': 0.007042149702171292, 'batch_size': 128}"
   ],
   "id": "49af9185ed93dc36"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "best_params = {'n_layers': 1, \n",
    "                'lstm_units': 50, \n",
    "                'dropout_rate': 0.5, \n",
    "                'learning_rate': 0.001, \n",
    "                'batch_size': 128,\n",
    "                'epochs': 50}\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_model(best_params, input_shape)\n",
    "model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'])\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_cut = np.sqrt(mean_squared_error(y_test, predictions))"
   ],
   "id": "8af94bf14cdf4301",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "plot_TSNE(df1,df3)"
   ],
   "id": "70e78f4a1502fb92",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Mix"
   ],
   "id": "148103770bc2d2b9"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "# Create X, y for LSTM\n",
    "X, y, df3 = create_augmented_data(rets, df1, df2, 'binary_mix', alpha, seq_len)\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=50)\n",
    "# print('Best trial:', study.best_trial.params)"
   ],
   "id": "f390fc30fb3657b3",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "best_params = {'n_layers': 2, \n",
    "               'lstm_units': 50, \n",
    "               'dropout_rate': 0.25, \n",
    "               'learning_rate': 0.005, \n",
    "               'batch_size': 64}\n",
    "best_params['epochs'] = 50\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_model(best_params, input_shape)\n",
    "model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'])\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_binary = np.sqrt(mean_squared_error(y_test, predictions))"
   ],
   "id": "221fac2480f62606",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "plot_TSNE(df1,df3)"
   ],
   "id": "572df69008011960",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Mix"
   ],
   "id": "d85ed74a1e32f6d6"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "# Create X, y for LSTM\n",
    "X, y, df3 = create_augmented_data(rets, df1, df2, 'linear_mix', alpha, seq_len)\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=50)\n",
    "# print('Best trial:', study.best_trial.params)"
   ],
   "id": "aba754d63a38012e",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "best_params = {'n_layers': 1, \n",
    "               'lstm_units': 150, \n",
    "               'dropout_rate': 0.45, \n",
    "               'learning_rate': 0.0025, \n",
    "               'batch_size': 32}\n",
    "best_params['epochs'] = 50\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_model(best_params, input_shape)\n",
    "model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'])\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_test, predictions))"
   ],
   "id": "1fed81b8a7f935e6",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "plot_TSNE(df1,df3)"
   ],
   "id": "b080094dbf324662",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geommetric Mix"
   ],
   "id": "1d601abef962ed16"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "# Create X, y for LSTM\n",
    "X, y, df3 = create_augmented_data(rets, df1, df2, 'geometrix_mix', alpha, seq_len)\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=50)\n",
    "# print('Best trial:', study.best_trial.params)"
   ],
   "id": "b4257e9a1030f885",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "best_params = {'n_layers': 1, \n",
    "               'lstm_units': 50, \n",
    "               'dropout_rate': 0.25, \n",
    "               'learning_rate': 0.006, \n",
    "               'batch_size': 64}\n",
    "best_params['epochs'] = 50\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_model(best_params, input_shape)\n",
    "model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'])\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_geom = np.sqrt(mean_squared_error(y_test, predictions))"
   ],
   "id": "a6f6f13bec1b6f95",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "plot_TSNE(df1,df3)"
   ],
   "id": "ee62ba8999679c27",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amplitude Mix"
   ],
   "id": "a84c4fcacc1d961b"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Create X, y for LSTM\n",
    "X, y, df3 = create_augmented_data(rets, df1, df2, 'amplitude_mix', alpha, seq_len)\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=50)\n",
    "# print('Best trial:', study.best_trial.params)"
   ],
   "id": "43e7beb884a373fa",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "best_params = {'n_layers': 2, \n",
    "               'lstm_units': 100, \n",
    "               'dropout_rate': 0.28, \n",
    "               'learning_rate': 0.0035, \n",
    "               'batch_size': 64}\n",
    "best_params['epochs'] = 50\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_model(best_params, input_shape)\n",
    "model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'])\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_amplitude = np.sqrt(mean_squared_error(y_test, predictions))"
   ],
   "id": "bcc87cf306747d8d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "source": [
    "plot_TSNE(df1,df3)"
   ],
   "id": "d33f76a4dc3fc02c",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Mix"
   ],
   "id": "915afb0477d295e8"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Create X, y for LSTM\n",
    "X, y, df3 = create_augmented_data(rets, df1, df2, 'proposed_mix', alpha, seq_len)\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=50)\n",
    "# print('Best trial:', study.best_trial.params)"
   ],
   "id": "db0a738176ae0bf3",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "best_params = {'n_layers': 2, \n",
    "               'lstm_units': 100, \n",
    "               'dropout_rate': 0.4, \n",
    "               'learning_rate': 0.002, \n",
    "               'batch_size': 64}\n",
    "best_params['epochs'] = 50\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_model(best_params, input_shape)\n",
    "model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'])\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_proposed = np.sqrt(mean_squared_error(y_test, predictions))"
   ],
   "id": "394bec8cefda5b05",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "plot_TSNE(df1,df3)"
   ],
   "id": "72add145d97a5ca5",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ],
   "id": "c4a4c451a744084e"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "source": [
    "# print('====    Original RMSE  ====')\n",
    "# print(f'The RMSE of original is: {round(rmse_og,5)}\\n')\n",
    "print('====    Augmented/Mixup RMSE  ====')\n",
    "print(f'The RMSE of cut_mix is: {round(rmse_cut,5)}')\n",
    "print(f'The RMSE of binary_mix is: {round(rmse_binary,5)}')\n",
    "print(f'The RMSE of linear_mix is: {round(rmse_linear,5)}')\n",
    "print(f'The RMSE of geometric_mix is: {round(rmse_geom,5)}')\n",
    "print(f'The RMSE of amplitude_mix is: {round(rmse_amplitude,5)}')\n",
    "print(f'The RMSE of proposed_mix is: {round(rmse_proposed,5)}')"
   ],
   "id": "3638c0164c689170",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next\n",
    "\n",
    "Use mutual information to determine if the augmented data really contains more information which is the epistemic one. Aleatoric is handled by data augmentation already."
   ],
   "id": "c05aa7365750161d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
