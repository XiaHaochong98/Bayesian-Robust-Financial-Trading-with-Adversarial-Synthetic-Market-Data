{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ef451e",
   "metadata": {},
   "source": [
    "## Project Title: To determine if data augmentation using the method proposed in 'Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning' will lead to better 1 day prediction results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdc2ec3b",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Seed value\n",
    "seed_value= 42\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df0a2bb5",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from scipy.fft import rfft, rfftfreq, irfft\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tcn import TCN  # If you have the tcn p /ackage installed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from pykalman import KalmanFilter\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import mutual_info_score\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "19bbf623",
   "metadata": {},
   "source": [
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac472934",
   "metadata": {},
   "source": [
    "# Assuming cfg is a configuration object with a seed attribute\n",
    "cfg = type('config', (object,), {'seed': 42})\n",
    "# Make the sampler behave in a deterministic way.\n",
    "sampler = TPESampler(seed=cfg.seed)\n",
    "\n",
    "alpha = 0.4\n",
    "seq_len = 20\n",
    "test_size = 0.3"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "15f8e6d1",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f208b0",
   "metadata": {},
   "source": [
    "# Function to import stock data\n",
    "def get_stock_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return data\n",
    "\n",
    "def z_score_normalize(series):\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    return (series - mean) / std\n",
    "\n",
    "def denormalize_z_score(normalized_series, original_mean, original_std):\n",
    "    return (normalized_series * original_std) + original_mean\n",
    "\n",
    "# Function to create model (make sure this is defined in your environment)\n",
    "def create_model(best_params, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(best_params['lstm_units'], input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(best_params['dropout_rate']))\n",
    "    model.add(LSTM(best_params['lstm_units']))  # Stacking LSTM for deep learning\n",
    "    model.add(Dropout(best_params['dropout_rate']))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mse')\n",
    "    return model\n",
    "\n",
    "def create_sequences(features, target, seq_len):\n",
    "    X, y = [], []\n",
    "    for i in range(len(target) - seq_len):\n",
    "        X.append(features[i:(i + seq_len)])\n",
    "        y.append(target[i + seq_len])\n",
    "    return np.array(X), np.array(y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81f25f9f",
   "metadata": {},
   "source": [
    "def plot_correlation(df):\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm',\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "    # Adjust the plot as needed\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()  # Adjusts the plot to ensure everything fits without overlap\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9207d9c",
   "metadata": {},
   "source": [
    "def engineer_features(data):\n",
    "    df = data.copy(deep=True)\n",
    "    delta = df['Close'].diff()\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "    roll_up = up.rolling(window=14).mean()\n",
    "    roll_down = down.abs().rolling(window=14).mean()\n",
    "    RS = roll_up / roll_down\n",
    "    df['RSI'] = 100.0 - (100.0 / (1.0 + RS))\n",
    "\n",
    "    # Volume Weighted Average Price (VWAP)\n",
    "    vwap = (df['Volume'] * (df['High'] + df['Low'] + df['Close']) / 3).cumsum() / df['Volume'].cumsum()\n",
    "    df['VWAP'] = vwap\n",
    "\n",
    "    # Price Ratios\n",
    "    df['high_to_low_ratio'] = df['High'] / df['Low']\n",
    "    df['open_to_close_ratio'] = df['Open'] / df['Close']\n",
    "\n",
    "    # Volatility\n",
    "    df['volatility_10'] = df['Close'].rolling(window=10).std()\n",
    "\n",
    "    df1 = df.drop(columns=['Open', 'High', 'Low', 'Adj Close']).dropna()\n",
    "    return df1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0657941b",
   "metadata": {},
   "source": [
    "# def cut_mix(data1, data2, alpha=0.2):\n",
    "#     assert len(data1) == len(data2)\n",
    "#     size = len(data1)\n",
    "#     cut_point = np.random.randint(0, size)\n",
    "#     cut_length = int(size * alpha)\n",
    "    \n",
    "#     mixed_data = np.copy(data1)\n",
    "#     mixed_data[cut_point:cut_point+cut_length] = data2[cut_point:cut_point+cut_length]\n",
    "    \n",
    "#     return mixed_data\n",
    "\n",
    "def cut_mix(df1, df2, alpha=0.2):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    assert df1.shape == df2.shape\n",
    "    size = len(df1)\n",
    "    cut_point = np.random.randint(0, size,)\n",
    "    cut_length = int(size * alpha)\n",
    "    \n",
    "    mixed_df = df1.copy()\n",
    "    mixed_df.iloc[cut_point:cut_point+cut_length] = df2.iloc[cut_point:cut_point+cut_length]\n",
    "    \n",
    "    return mixed_df\n",
    "\n",
    "def binary_mix(data1, data2, alpha=0.2):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    assert len(data1) == len(data2)\n",
    "    size = data1.shape\n",
    "    mask = np.random.binomial(1, alpha, size=size).astype(bool)\n",
    "    \n",
    "    mixed_data = np.where(mask, data1, data2)\n",
    "    \n",
    "    return pd.DataFrame(mixed_data, columns=data1.columns)\n",
    "\n",
    "def linear_mix(data1, data2, alpha=0.2):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    mixed_data = alpha * data1 + (1 - alpha) * data2\n",
    "    \n",
    "    return mixed_data\n",
    "\n",
    "def geometric_mix(data1, data2, alpha=0.2):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    mixed_data = data1**alpha * data2**(1 - alpha)\n",
    "    \n",
    "    return mixed_data\n",
    "\n",
    "def amplitude_mix(data1, data2, alpha=0.2):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    # Apply Fourier Transform to each column\n",
    "    fft1 = np.fft.rfft(data1, axis=0)\n",
    "    fft2 = np.fft.rfft(data2, axis=0)\n",
    "    \n",
    "    # Mix the magnitudes\n",
    "    magnitude1 = np.abs(fft1)\n",
    "    magnitude2 = np.abs(fft2)\n",
    "    mixed_magnitude = alpha * magnitude1 + (1 - alpha) * magnitude2\n",
    "    \n",
    "    # Keep the phase of the first data\n",
    "    phase1 = np.angle(fft1)\n",
    "    mixed_fft = mixed_magnitude * np.exp(1j * phase1)\n",
    "    \n",
    "    # Perform the inverse FFT and ensure the result is two-dimensional\n",
    "    mixed_data = np.fft.irfft(mixed_fft, axis=0)\n",
    "    if mixed_data.ndim == 1:\n",
    "        mixed_data = mixed_data.reshape(-1, 1)  # Reshape if the data is one-dimensional\n",
    "    \n",
    "    # Return a DataFrame with the same column names as data1\n",
    "    return pd.DataFrame(mixed_data, columns=data1.columns)\n",
    "\n",
    "\n",
    "### PROPOSE TECHNIQUE BELOW\n",
    "\n",
    "def proposed_mixup(df1, df2, threshold=0.1, alpha=0.5):\n",
    "    \n",
    "    def proposed_mixup_feature(data1, data2, threshold, alpha):\n",
    "        \n",
    "        def get_significant_frequencies(data, threshold):\n",
    "            \"\"\"\n",
    "            Perform Fourier Transform on data and identify frequencies with significant amplitude.\n",
    "\n",
    "            Args:\n",
    "            - data: Time series data.\n",
    "            - threshold: Threshold for significance, relative to the max amplitude.\n",
    "\n",
    "            Returns:\n",
    "            - significant_freq: Frequencies with significant amplitude.\n",
    "            - significant_ampl: Amplitude of the significant frequencies.\n",
    "            - full_spectrum: Full Fourier spectrum for all frequencies.\n",
    "            \"\"\"\n",
    "            # Perform Fourier Transform\n",
    "            spectrum = rfft(data)\n",
    "            frequencies = rfftfreq(data.size, d=1)  # Assuming unit time interval between samples\n",
    "\n",
    "            # Find significant amplitudes\n",
    "            amplitude = np.abs(spectrum)\n",
    "            significant_indices = amplitude > (amplitude.max() * threshold)\n",
    "            significant_freq = frequencies[significant_indices]\n",
    "            significant_ampl = amplitude[significant_indices]\n",
    "\n",
    "            return significant_freq, significant_ampl, spectrum\n",
    "\n",
    "        def phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2, alpha):\n",
    "            mixed_spectrum = np.copy(spectrum1)\n",
    "            freqs1 = rfftfreq(spectrum1.size, d=1)\n",
    "            freqs2 = rfftfreq(spectrum2.size, d=1)\n",
    "\n",
    "            for freq in sig_freq1:\n",
    "                index1 = np.argmin(np.abs(freqs1 - freq))\n",
    "                index2 = np.argmin(np.abs(freqs2 - freq))\n",
    "\n",
    "                if index1 >= len(sig_ampl1) or index2 >= len(sig_ampl2):\n",
    "                    continue  # Skip the frequency if the index is out of bounds\n",
    "\n",
    "                phase1 = np.angle(spectrum1[index1])\n",
    "                phase2 = np.angle(spectrum2[index2])\n",
    "\n",
    "                phase_diff = (phase2 - phase1) % (2 * np.pi)\n",
    "                phase_diff = phase_diff - 2 * np.pi if phase_diff > np.pi else phase_diff\n",
    "\n",
    "                new_amplitude = alpha * sig_ampl1[index1] + (1 - alpha) * sig_ampl2[index2]\n",
    "                new_phase = phase1 + alpha * phase_diff\n",
    "\n",
    "                mixed_spectrum[index1] = new_amplitude * np.exp(1j * new_phase)\n",
    "\n",
    "            return mixed_spectrum\n",
    "\n",
    "\n",
    "        def reconstruct_time_series(mixed_spectrum):\n",
    "            \"\"\"\n",
    "            Reconstruct time series from mixed spectrum using inverse Fourier Transform.\n",
    "\n",
    "            Returns:\n",
    "            - mixed_time_series: The reconstructed time series.\n",
    "            \"\"\"\n",
    "            # Perform inverse Fourier Transform\n",
    "            mixed_time_series = irfft(mixed_spectrum)\n",
    "\n",
    "            return mixed_time_series\n",
    "\n",
    "        # Step 1: Get significant frequencies and amplitude for both time series\n",
    "        sig_freq1, sig_ampl1, spectrum1 = get_significant_frequencies(data1, threshold)\n",
    "        sig_freq2, sig_ampl2, spectrum2 = get_significant_frequencies(data2, threshold)\n",
    "\n",
    "        # Step 2: Identify significant frequencies (already done in step 1)\n",
    "\n",
    "        # Step 3: Phase and Magnitude Mixup\n",
    "        mixed_spectrum = phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2, alpha)\n",
    "\n",
    "        # Step 4: Reconstruction of the time series\n",
    "        mixed_time_series = reconstruct_time_series(mixed_spectrum)\n",
    "        return mixed_time_series\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    \n",
    "    for feature in df1.columns:\n",
    "        output_df[feature] = proposed_mixup_feature(df1[feature].values, df2[feature].values, threshold, alpha)\n",
    "        \n",
    "    return output_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913167a0",
   "metadata": {},
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned by Optuna\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    lstm_units = trial.suggest_categorical('lstm_units', [50, 100, 150])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    epochs = trial.suggest_int('epochs', 20, 100)\n",
    "    \n",
    "    # Dictionary to hold RMSE for each stock\n",
    "    stock_rmse = {}\n",
    "    \n",
    "    for stock, df in historical_data_augmented.items():\n",
    "        # Preprocess the data\n",
    "        df = df.copy()\n",
    "        rets = df['Close'].pct_change().dropna()\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(df[df.columns].values)\n",
    "        seq_len = 20\n",
    "        X, y = create_sequences(scaled_features, rets.values, seq_len)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        \n",
    "        # Model architecture\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "        model = Sequential()\n",
    "        for i in range(n_layers):\n",
    "            model.add(LSTM(units=lstm_units, return_sequences=(i < n_layers - 1)))\n",
    "            model.add(Dropout(rate=dropout_rate))\n",
    "        model.add(Dense(units=1))\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        # Predictions and evaluate\n",
    "        predictions = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "        stock_rmse[stock] = rmse\n",
    "    \n",
    "    # Calculate the average RMSE across all stocks\n",
    "    average_rmse = np.mean(list(stock_rmse.values()))\n",
    "    \n",
    "    return average_rmse"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3307f855",
   "metadata": {},
   "source": [
    "def create_augmented_data(df1, df2, method, alpha=0.2):\n",
    "    if method == 'cut_mix':\n",
    "        df = cut_mix(df1, df2, alpha)\n",
    "    elif method == 'binary_mix':\n",
    "        df = binary_mix(df1, df2, alpha)\n",
    "    elif method == 'linear_mix':\n",
    "        df = linear_mix(df1, df2, alpha)\n",
    "    elif method == 'geometrix_mix':\n",
    "        df = geometric_mix(df1, df2, alpha)\n",
    "    elif method == 'amplitude_mix':\n",
    "        df = amplitude_mix(df1, df2, alpha)\n",
    "    elif method == 'proposed_mix':\n",
    "        df = proposed_mixup(df1, df2, alpha)\n",
    "\n",
    "    # Original\n",
    "    else:\n",
    "        df = df1.copy()\n",
    "        \n",
    "    return df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6ad4f38",
   "metadata": {},
   "source": [
    "def plot_TSNE(df1, df2):\n",
    "    df1.columns = df1.columns.astype(str)\n",
    "    df2.columns = df2.columns.astype(str)\n",
    "\n",
    "    df1_log = np.log(df1 + 1)  # Adding 1 to avoid log(0)\n",
    "    df2_log = np.log(df2 + 1)\n",
    "\n",
    "    combined_data = pd.concat([df1_log, df2_log])\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=100, n_iter=1000, init='pca')\n",
    "    tsne_results = tsne.fit_transform(combined_data)\n",
    "\n",
    "    # Now we split the t-SNE results back into original and augmented parts\n",
    "    tsne_df1 = tsne_results[:len(df1), :]\n",
    "    tsne_df2 = tsne_results[len(df1):, :]\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.scatter(tsne_df1[:, 0], tsne_df1[:, 1], label='Original', alpha=0.5)\n",
    "    plt.scatter(tsne_df2[:, 0], tsne_df2[:, 1], label='Augmented', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdbbc64e",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "def calculate_entropy(variable):\n",
    "    value,counts = np.unique(variable, return_counts=True)\n",
    "    return entropy(counts, base=2)\n",
    "\n",
    "# Function to calculate normalized mutual information\n",
    "def calculate_normalized_mi(variable_1, variable_2):\n",
    "    mi = mutual_info_score(variable_1, variable_2)\n",
    "    entropy_1 = calculate_entropy(variable_1)\n",
    "    entropy_2 = calculate_entropy(variable_2)\n",
    "    # Normalizing by the average entropy\n",
    "    normalized_mi = mi / ((entropy_1 + entropy_2) / 2)\n",
    "    return normalized_mi\n",
    "\n",
    "def calculate_MI(original, augmented):\n",
    "# Assuming df_original and df_augmented are your dataframes\n",
    "    for column in original.columns:\n",
    "        # Ensure the data is in the correct format, e.g., continuous or discrete\n",
    "        # For continuous variables, you'd typically bin them before calculating mutual information\n",
    "        original_data = original[column].to_numpy()\n",
    "        augmented_data = augmented[column].to_numpy()\n",
    "\n",
    "        # Calculate normalized MI for each column\n",
    "        normalized_mi = calculate_normalized_mi(original_data, augmented_data)\n",
    "        print(f'Normalized Mutual Information for {column}: {normalized_mi}')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57c7ac21",
   "metadata": {},
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "def apply_lowess_smoothing(df, frac=0.1):\n",
    "    smoothed_data = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Apply LOWESS to each column\n",
    "    for column in df.columns:\n",
    "        smoothed_values = lowess(df[column], df.index, frac=frac, return_sorted=False)\n",
    "        smoothed_data[column] = smoothed_values\n",
    "    \n",
    "    return smoothed_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "185e7c1f",
   "metadata": {},
   "source": [
    "def compare_distributions(original_df, augmented_df, features):\n",
    "    num_features = len(features)\n",
    "    fig, axes = plt.subplots(nrows=num_features, ncols=2, figsize=(15, 5 * num_features))\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        # Original Data\n",
    "        sns.histplot(original_df[feature], kde=True, color=\"blue\", ax=axes[i, 0])\n",
    "        axes[i, 0].set_title(f'Original {feature}')\n",
    "        \n",
    "        # Augmented Data\n",
    "        sns.histplot(augmented_df[feature], kde=True, color=\"orange\", ax=axes[i, 1])\n",
    "        axes[i, 1].set_title(f'Augmented {feature}')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a1f71a2a",
   "metadata": {},
   "source": [
    "### Pull Data from Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5c62563",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "start_date = '2010-01-01'\n",
    "end_date = '2023-01-01'\n",
    "\n",
    "# Define the list of Dow Jones Industrial Average companies\n",
    "tickers = [\n",
    "    \"MMM\", \"AXP\", \"AMGN\", \"AAPL\", \"BA\", \"CAT\", \"CVX\", \"CSCO\", \"KO\", \"DIS\"\n",
    "    , \"GS\", \"HD\", \"HON\", \"IBM\", \"INTC\", \"JNJ\", \"JPM\", \"MCD\", \"MRK\",\n",
    "    \"MSFT\", \"NKE\", \"PG\", \"CRM\", \"TRV\", \"UNH\", \"V\", \"WBA\", \"WMT\"\n",
    "]\n",
    "\n",
    "# tickers = ['AAPL']\n",
    "# Create a dictionary to store historical data for each company\n",
    "historical_data = {}\n",
    "\n",
    "# Loop through the Dow companies and retrieve historical data\n",
    "for ticker in tickers:\n",
    "    stock_data = get_stock_data(ticker, start_date, end_date)\n",
    "    historical_data[ticker] = stock_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "006353c4",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "def jittering(ts, noise_level=0.05):\n",
    "    np.random.seed(42)\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=len(ts))\n",
    "    return pd.Series(ts + noise)\n",
    "\n",
    "def flipping(ts):\n",
    "    return pd.Series(np.flip(ts))\n",
    "\n",
    "def scaling(ts, scaling_factor=1.5):\n",
    "    return pd.Series(ts * scaling_factor)\n",
    "\n",
    "def magnitude_warping(ts, sigma=0.2, knot=4):\n",
    "    np.random.seed(42)\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, ))\n",
    "    indices = np.linspace(0, len(ts)-1, num=knot+2)\n",
    "    sp = CubicSpline(indices, random_warps)\n",
    "    warp_values = sp(np.arange(len(ts)))\n",
    "    return pd.Series(ts * warp_values)\n",
    "\n",
    "def permutation(ts, n_segments=5):\n",
    "    np.random.seed(42)\n",
    "    permutated_ts = np.copy(ts)\n",
    "    segments = np.array_split(permutated_ts, n_segments)\n",
    "    np.random.shuffle(segments)\n",
    "    return pd.Series(np.concatenate(segments))\n",
    "\n",
    "def time_warping(ts, sigma=0.2, knot=4):\n",
    "    np.random.seed(42)\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    time_steps = np.arange(ts.shape[0])\n",
    "    random_steps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, ts.shape[1]))\n",
    "    indices = np.linspace(0, len(ts)-1, num=knot+2)\n",
    "    sp = CubicSpline(indices, random_steps)\n",
    "    warp_values = sp(time_steps)\n",
    "    return pd.Series(warp_values * ts)\n",
    "\n",
    "def stl_augment(data, period=61):\n",
    "    ts = data.asfreq('B')\n",
    "    ts = ts.interpolate()\n",
    "    # Apply STL decomposition\n",
    "    stl = STL(ts, seasonal=period) \n",
    "    result = stl.fit()\n",
    "    seasonal, trend, remainder = result.seasonal, result.trend, result.resid\n",
    "    bootstrapped_remainder = resample(remainder, replace=True, n_samples=len(remainder), random_state=42)\n",
    "    bootstrapped_remainder.index = ts.index\n",
    "    augmented_signal = trend + seasonal + bootstrapped_remainder\n",
    "    augmented_signal = np.maximum(augmented_signal, 0)\n",
    "    augmented_signal = augmented_signal[data.index]\n",
    "    return augmented_signal\n",
    "\n",
    "# Function to plot original and augmented series\n",
    "def plot_augmented_ts(original_ts, augmented_ts, title='Time Series Augmentation'):\n",
    "    augmented_ts.index = original_ts.index\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(original_ts, label='Original')\n",
    "    plt.plot(augmented_ts, label='Augmented')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cafd73aa",
   "metadata": {},
   "source": [
    "### AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "752d61e4",
   "metadata": {},
   "source": [
    "aapl = historical_data['AAPL']\n",
    "aapl = engineer_features(aapl)\n",
    "aapl"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4297a5ee",
   "metadata": {},
   "source": [
    "def augment_dataframe(stock_df, method):\n",
    "    augmented_stockdf = pd.DataFrame()\n",
    "    for col in stock_df.columns:\n",
    "        val = aapl[col]\n",
    "        if method == 'jittering':\n",
    "            aug_val = jittering(val)\n",
    "        elif method == 'flipping':\n",
    "            aug_val = flipping(val)\n",
    "        elif method == 'scaling':\n",
    "            aug_val = scaling(val)\n",
    "        elif method == 'magnitude_warping':\n",
    "            aug_val = magnitude_warping(val)\n",
    "        elif method == 'permutation':\n",
    "            aug_val = permutation(val)\n",
    "        elif method == 'stl_augment':\n",
    "            aug_val = stl_augment(val)\n",
    "\n",
    "        augmented_stockdf[col] = aug_val\n",
    "    return augmented_stockdf"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3828d63e",
   "metadata": {},
   "source": [
    "lowess = apply_lowess_smoothing(aapl)\n",
    "cut_lowess = create_augmented_data(aapl, lowess, method='cut_mix')\n",
    "binary_lowess = create_augmented_data(aapl, lowess, method='binary_mix')\n",
    "linear_lowess = create_augmented_data(aapl, lowess, method='linear_mix')\n",
    "geometric_lowess = create_augmented_data(aapl, lowess, method='geometrix_mix')\n",
    "amplitude_lowess = create_augmented_data(aapl, lowess, method='amplitude_mix')\n",
    "proposedmix_lowess = create_augmented_data(aapl, lowess, method='proposed_mix')\n",
    "\n",
    "jittered_ts = augment_dataframe(aapl, 'jittering')\n",
    "flipped_ts = augment_dataframe(aapl, 'flipping')\n",
    "scaled_ts = augment_dataframe(aapl, 'scaling')\n",
    "mag_warped_ts = augment_dataframe(aapl, 'magnitude_warping')\n",
    "permuted_ts = augment_dataframe(aapl, 'permutation')\n",
    "stl_ts = augment_dataframe(aapl, 'stl_augment')\n",
    "\n",
    "augmented_datasets = {\n",
    "#     'cut_mix': cut_lowess,\n",
    "#     'binary_mix': binary_lowess,\n",
    "#     'linear_mix': linear_lowess,\n",
    "#     'geometric_mix': geometric_lowess,\n",
    "#     'amplitude_mix': amplitude_lowess,\n",
    "#     'proposed_mix': proposedmix_lowess,\n",
    "#     'jittering': jittered_ts,\n",
    "#     'flipping': flipped_ts,\n",
    "#     'scaling': scaled_ts,\n",
    "#     'magnitude_warping': mag_warped_ts,\n",
    "#     'permutation': permuted_ts,\n",
    "    'stl_augment': stl_ts\n",
    "}\n",
    "\n",
    "# Create a MultiIndex DataFrame\n",
    "keys = augmented_datasets.keys()\n",
    "multi_index_df = pd.concat(augmented_datasets.values(), keys=keys, axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fa357c8",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Check the plots of original vs \n",
    "time_series_original = aapl['Close']\n",
    "plot_augmented_ts(time_series_original, cut_lowess.Close, title='Cut_lowess Augmentation')\n",
    "plot_augmented_ts(time_series_original, binary_lowess.Close, title='Binary_lowess Augmentation')\n",
    "plot_augmented_ts(time_series_original, linear_lowess.Close, title='Linear_lowess Augmentation')\n",
    "plot_augmented_ts(time_series_original, geometric_lowess.Close, title='Geometric_lowess Augmentation')\n",
    "plot_augmented_ts(time_series_original, amplitude_lowess.Close, title='Amplitude_lowess Augmentation')\n",
    "plot_augmented_ts(time_series_original, proposedmix_lowess.Close, title='Proposedmix_lowess Augmentation')\n",
    "\n",
    "plot_augmented_ts(time_series_original, jittered_ts.Close, title='Jittering Augmentation')\n",
    "plot_augmented_ts(time_series_original, flipped_ts.Close, title='Flipped Augmentation')\n",
    "plot_augmented_ts(time_series_original, scaled_ts.Close, title='Scaled Augmentation')\n",
    "plot_augmented_ts(time_series_original, mag_warped_ts.Close, title='Magnitude Warped Augmentation')\n",
    "plot_augmented_ts(time_series_original, permuted_ts.Close, title='Permuted Augmentation')\n",
    "plot_augmented_ts(time_series_original, stl_ts.Close, title='STL Augmentation')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ae2401a",
   "metadata": {},
   "source": [
    "from scipy.stats import ks_2samp\n",
    "from scipy.signal import welch\n",
    "from scipy.fft import fft\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ff40a190",
   "metadata": {},
   "source": [
    "### Rules and tests with the original\n",
    "\n",
    "To determine how much information is gone. we will use the following\n",
    "- Mutual Information\n",
    "Factors to Consider\n",
    "Data Granularity: The frequency of your data (e.g., daily, weekly, monthly) affects how finely you might want to discretize it. More frequent data might benefit from more bins, but too many bins can lead to overfitting or spurious relationships.\n",
    "    1. Volatility: Financial data can be highly volatile. More bins can capture nuances in volatile periods, but they should not capture noise as signal.\n",
    "    2. Distribution Shape: The distribution of returns or price changes in financial data often deviates from normal, with heavy tails. A strategy that adapts to the data distribution can be more informative.\n",
    "- KS Test\n",
    "- Spectral Analysis\n",
    "- MACD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad984671",
   "metadata": {},
   "source": [
    "def calculate_ema(prices, span):\n",
    "    return prices.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def calculate_macd(close_prices):\n",
    "    ema_12 = calculate_ema(close_prices, 12)\n",
    "    ema_26 = calculate_ema(close_prices, 26)\n",
    "    macd = ema_12 - ema_26\n",
    "    signal_line = calculate_ema(macd, 9)\n",
    "    return macd, signal_line\n",
    "\n",
    "def calculate_rsi(close_prices, periods=14):\n",
    "    delta = close_prices.diff(1)\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=periods).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=periods).mean()\n",
    "\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "def discretize_series(series, n_bins=20):\n",
    "    # Assuming 'series' is a pandas Series\n",
    "    series_reshaped = series.values.reshape(-1, 1)\n",
    "    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    discretized = discretizer.fit_transform(series_reshaped).flatten()\n",
    "    return discretized"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed1396a3",
   "metadata": {},
   "source": [
    "def run_tests(original, augmented):\n",
    "    # Distributional Analysis\n",
    "    ks_stat, ks_pvalue = ks_2samp(original.Close, augmented.Close)\n",
    "\n",
    "    # Spectral Analysis\n",
    "    freqs, power_original = welch(original.Close)\n",
    "    _, power_jittered = welch(augmented.Close)\n",
    "    # Compare power_original and power_jittered\n",
    "\n",
    "    # Mutual Information\n",
    "    original_discretized = discretize_series(original.Close)\n",
    "    augmented_discretized = discretize_series(augmented.Close)\n",
    "    mi_score = mutual_info_score(original_discretized, augmented_discretized)\n",
    "    \n",
    "    # MACD and RSI visual inspection\n",
    "    macd, signal_line = calculate_macd(original.Close)\n",
    "    rsi = calculate_rsi(original.Close)\n",
    "    augmented_macd, augmented_signal_line = calculate_macd(augmented.Close)\n",
    "    augmented_rsi = calculate_rsi(augmented.Close)\n",
    "\n",
    "    # Output results\n",
    "    print(f\"KS Statistic: {ks_stat}, P-value: {ks_pvalue}\")\n",
    "    print(f\"Mutual Information Score: {mi_score}\")\n",
    "    \n",
    "    results = {\n",
    "        'KS Statistic': ks_stat,\n",
    "        'P-value': ks_pvalue,\n",
    "        'Mutual Information Score': mi_score,\n",
    "        # Include any other metrics you calculate\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(macd, label='Original MACD')\n",
    "    plt.plot(signal_line, label='Original Signal Line')\n",
    "    plt.plot(augmented_macd, label='Augmented MACD', linestyle='--')\n",
    "    plt.plot(augmented_signal_line, label='Augmented Signal Line', linestyle='--')\n",
    "    plt.title(\"MACD Comparison\")\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(rsi, label='Original RSI')\n",
    "    plt.plot(augmented_rsi, label='Augmented RSI', linestyle='--')\n",
    "    plt.title(\"RSI Comparison\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return results"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3433ee7c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "results_list = []\n",
    "for method, dataset in augmented_datasets.items():\n",
    "    result = run_tests(aapl, dataset)\n",
    "    result['Method'] = method  # Add the method name to the results\n",
    "    results_list.append(result)\n",
    "\n",
    "# Convert the list of results into a DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Get the list of all column names and remove 'Method'\n",
    "columns = [col for col in results_df.columns if col != 'Method']\n",
    "\n",
    "# Place 'Method' at the beginning of the list\n",
    "ordered_columns = ['Method'] + columns\n",
    "\n",
    "# Reindex the DataFrame with the new column order\n",
    "results_df = results_df[ordered_columns]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cf6bf103",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "435f40ee",
   "metadata": {},
   "source": [
    "aapl['Return'] = np.log(aapl['Close']).diff()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90fe59da",
   "metadata": {},
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc8a306a",
   "metadata": {},
   "source": [
    "# Define constants\n",
    "TIME_STEPS = 20\n",
    "ALPHA = 0.1  # Confidence level for prediction intervals\n",
    "\n",
    "# Define the LSTM model creation function\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=input_shape),\n",
    "        LSTM(50),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Prepare the data by scaling features and target\n",
    "def prepare_data(dataframe, feature_columns, target_column):\n",
    "    feature_scaler = StandardScaler()\n",
    "    target_scaler = StandardScaler()\n",
    "    \n",
    "    scaled_features = feature_scaler.fit_transform(dataframe[feature_columns])\n",
    "    scaled_target = target_scaler.fit_transform(dataframe[[target_column]].values)  \n",
    "    \n",
    "    return scaled_features, scaled_target.flatten(), feature_scaler, target_scaler\n",
    "\n",
    "# Train the LSTM model and return it along with scalers and the test set\n",
    "def train_evaluate_lstm(features, target, time_steps, epochs, batch_size):\n",
    "    X, y = create_sequences(features, target, time_steps)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    model = create_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "    \n",
    "    return model, y_test, test_predictions, test_rmse\n",
    "\n",
    "def create_sequences(features, target, time_steps):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(features) - time_steps):\n",
    "        Xs.append(features[i:(i + time_steps)])\n",
    "        ys.append(target[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "def create_lstm_model(input_shape, lstm_units, dropout_rate, optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units, return_sequences=True, input_shape=input_shape, dropout=dropout_rate))\n",
    "    model.add(LSTM(lstm_units, return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial, features, target, time_steps, batch_size):\n",
    "    # Hyperparameters to be tuned by Optuna\n",
    "    lstm_units = trial.suggest_categorical('lstm_units', [20, 50, 100])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "\n",
    "    # Split the data\n",
    "    X, y = create_sequences(features, target, time_steps)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Model creation\n",
    "    model = create_lstm_model((X_train.shape[1], X_train.shape[2]), lstm_units, dropout_rate, Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    # Model training\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Model evaluation\n",
    "    test_predictions = model.predict(X_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "    \n",
    "    return test_rmse\n",
    "\n",
    "# Hyperparameter tuning with Optuna\n",
    "def hyperparameter_tuning(features, target, time_steps, epochs, batch_size):\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, features, target, time_steps, batch_size), n_trials=20)\n",
    "    \n",
    "    # Best hyperparameters\n",
    "    best_params = study.best_params\n",
    "    best_rmse = study.best_value\n",
    "    print(f\"Best RMSE: {best_rmse}\")\n",
    "    print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "    # Retrain model with best hyperparameters\n",
    "    best_model = create_lstm_model(\n",
    "        (time_steps, features.shape[1]),\n",
    "        best_params['lstm_units'],\n",
    "        best_params['dropout_rate'],\n",
    "        Adam(learning_rate=best_params['learning_rate'])\n",
    "    )\n",
    "    X, y = create_sequences(features, target, time_steps)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    best_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # Final evaluation\n",
    "    test_predictions = best_model.predict(X_test)\n",
    "    final_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "    print(f\"Final Test RMSE: {final_rmse}\")\n",
    "\n",
    "    return best_model, best_params, y_test, test_predictions, final_rmse"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "941d71ab",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Main code\n",
    "augmented_results_lstm = {}\n",
    "\n",
    "aapl.dropna(subset=['Return'], inplace=True)\n",
    "for method, dataset in augmented_datasets.items():\n",
    "    features = [col for col in dataset.columns if col not in ('Close', 'Adj Close', 'Return')]\n",
    "    scaled_features, scaled_target, feature_scaler, target_scaler = prepare_data(aapl, features, 'Return')\n",
    "    best_model, best_params, y_test, test_predictions, final_rmse = hyperparameter_tuning(scaled_features, scaled_target, TIME_STEPS, 100, 128)    \n",
    "    \n",
    "    augmented_results_lstm[method] = {\n",
    "        'model': best_model,\n",
    "        'params': best_params,\n",
    "        'y_test': y_test,\n",
    "        'predictions': test_predictions\n",
    "    }"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d3b08c96",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Evaluation\n",
    "for method, result in augmented_results_lstm.items():\n",
    "    predictions = result['predictions']\n",
    "    y_test = result['y_test']\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    print(f\"Test RMSE for {method}: {rmse}\")\n",
    "    \n",
    "    # Calculate errors\n",
    "    errors = np.abs(pd.Series(predictions.flatten()) - pd.Series(y_test.flatten()))\n",
    "\n",
    "    # Create a 1x2 subplot structure\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "    # Plot Actual vs. Predicted Returns with Prediction Interval\n",
    "    axs[0].plot(y_test, label='Actual Returns', color='blue')\n",
    "    axs[0].plot(predictions, label='Predicted Returns', color='red')\n",
    "    axs[0].set_title(f'Actual vs Predicted Returns with ({method})')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plot the distribution of errors\n",
    "    axs[1].hist(errors, bins=30, edgecolor='skyblue', density=True)\n",
    "    axs[1].set_title(f'Error Distribution ({method})')\n",
    "    axs[1].set_xlabel('Magnitude of Error')\n",
    "    axs[1].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()  # Adjust the layout\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c500acfc",
   "metadata": {},
   "source": [
    "augmented_results_lstm['cut_mix']['params']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3a7793ca",
   "metadata": {},
   "source": [
    "df = augmented_datasets['cut_mix']\n",
    "features = [col for col in dataset.columns if col not in ('Close', 'Adj Close', 'Return')]\n",
    "scaled_features, scaled_target, feature_scaler, target_scaler = prepare_data(aapl, features, 'Return')\n",
    "X, y = create_sequences(scaled_features, scaled_target, 10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    " # Model creation\n",
    "model = create_lstm_model((X_train.shape[1], X_train.shape[2]), 50, 0.20462373842624892, Adam(learning_rate=0.07439518885658228))\n",
    "\n",
    "# Model training\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=128, verbose=0)\n",
    "\n",
    "# Model evaluation\n",
    "test_predictions = model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e9579b26",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "plt.plot(test_predictions[:50])\n",
    "plt.plot(y_test[:50])\n",
    "plt.legend()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4da51348",
   "metadata": {},
   "source": [
    "# Create a 1x2 subplot structure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "# Plot Actual vs. Predicted Returns with Prediction Interval\n",
    "axs[0].plot(y_test, label='Actual Returns', color='blue')\n",
    "axs[0].plot(test_predictions, label='Predicted Returns', color='red')\n",
    "axs[0].set_title(f'Actual vs Predicted Returns with ({method})')\n",
    "axs[0].legend()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92414d29",
   "metadata": {},
   "source": [
    "augmented_results_lstm['cut_mix']['y_test']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c347f14",
   "metadata": {},
   "source": [
    "X_train"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "406d4a71",
   "metadata": {},
   "source": [
    "# TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77efc22",
   "metadata": {},
   "source": [
    "from tcn import TCN"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8895a9",
   "metadata": {},
   "source": [
    "def create_tcn_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    tcn_layer = TCN(nb_filters=64, kernel_size=3, dilations=[1, 2, 4], padding='causal', use_skip_connections=True)(input_layer)\n",
    "    output_layer = Dense(1)(tcn_layer)\n",
    "    model = Model(inputs=[input_layer], outputs=[output_layer])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Train the LSTM model and return it along with scalers and the test set\n",
    "def train_evaluate_TCN(features, target, time_steps, epochs, batch_size):\n",
    "    X, y = create_sequences(features, target, time_steps)\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    model = create_tcn_model((X_train.shape[1], X_train.shape[2]))\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    test_predictions = model.predict(X_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "    \n",
    "    return model, X_test, y_test"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c475cd16",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Train and evaluate LSTM model\n",
    "tcn_model, X_test, y_test = train_evaluate_TCN(scaled_features, scaled_target, TIME_STEPS, 100, 128)\n",
    "\n",
    "# Dictionary to store results for augmented datasets\n",
    "augmented_results_tcn = {}\n",
    "for method, dataset in augmented_datasets.items():\n",
    "    # Ensure dataset is prepared with the correct columns\n",
    "    new_data_prepared = dataset[features]  # Make sure dataset has the right columns\n",
    "    \n",
    "    predictions = predict_new_data(tcn_model, new_data_prepared, feature_scaler, target_scaler, TIME_STEPS)\n",
    "        \n",
    "    augmented_results_tcn[method] = {\n",
    "        'Predictions': predictions\n",
    "    }"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc78a0",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Evaluation\n",
    "for method, result in augmented_results_tcn.items():\n",
    "    predictions = result['Predictions']\n",
    "    actuals = scaled_target[TIME_STEPS-1:]  # Adjust as needed\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    print(f\"Test RMSE for {method}: {rmse}\")\n",
    "    \n",
    "    # Calculate errors\n",
    "    errors = np.abs(pd.Series(predictions.flatten()) - pd.Series(actuals.flatten()))\n",
    "\n",
    "    # Create a 1x2 subplot structure\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "    # Plot Actual vs. Predicted Returns with Prediction Interval\n",
    "    axs[0].plot(actuals, label='Actual Returns', color='blue')\n",
    "    axs[0].plot(predictions, label='Predicted Returns', color='red')\n",
    "    axs[0].set_title(f'Actual vs Predicted Returns ({method})')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plot the distribution of errors\n",
    "    axs[1].hist(errors, bins=30, edgecolor='skyblue', density=True)\n",
    "    axs[1].set_title(f'Error Distribution ({method})')\n",
    "    axs[1].set_xlabel('Magnitude of Error')\n",
    "    axs[1].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()  # Adjust the layout\n",
    "    plt.show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
