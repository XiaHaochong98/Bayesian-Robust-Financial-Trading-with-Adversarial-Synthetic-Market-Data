{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8b6ea1",
   "metadata": {},
   "source": [
    "## Project Title: To determine if data augmentation using the method proposed in 'Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning' will lead to better 1 day prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34dfd4b9",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import seaborn as sb\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import rfft, rfftfreq, irfft\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tcn import TCN  # If you have the tcn package installed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78cce277",
   "metadata": {},
   "source": [
    "# Function to import stock data\n",
    "def get_stock_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return data['Close']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df63d07",
   "metadata": {},
   "source": [
    "# Function to perform Fourier Transform and plot\n",
    "def fourier_transform_and_plot(stock_data, title):\n",
    "    # Apply the Fourier Transform to the closing price data\n",
    "    yf = rfft(stock_data)\n",
    "    xf = rfftfreq(len(stock_data), 1)  # Assuming daily samples for frequency calculation\n",
    "    \n",
    "    # Plot the Fourier Transform results\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(xf, np.abs(yf))  # Plot the magnitude spectrum\n",
    "    plt.title(f'Fourier Transform of {title}')\n",
    "    plt.xlabel('Frequency (1/day)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Return the frequency and magnitude for further analysis if needed\n",
    "    return xf, np.abs(yf)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "68c0394a",
   "metadata": {},
   "source": [
    "### Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c55d9ca7",
   "metadata": {},
   "source": [
    "ticker1 = 'AAPL'\n",
    "ticker2 = 'MSFT'\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2023-01-01'\n",
    "\n",
    "# Import data\n",
    "close1 = get_stock_data(ticker1, start_date, end_date)\n",
    "close2 = get_stock_data(ticker2, start_date, end_date)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b9141999",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "1. Idenitify sinusoidal components with Fourier Transform\n",
    "2. Calculate Phase Difference for mixup coefficient for each significant coefficient\n",
    "3. Reconstruct time series using inverse Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09149fee",
   "metadata": {},
   "source": [
    "def get_significant_frequencies(data, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Perform Fourier Transform on data and identify frequencies with significant amplitude.\n",
    "    \n",
    "    Args:\n",
    "    - data: Time series data.\n",
    "    - threshold: Threshold for significance, relative to the max amplitude.\n",
    "    \n",
    "    Returns:\n",
    "    - significant_freq: Frequencies with significant amplitude.\n",
    "    - significant_ampl: Amplitude of the significant frequencies.\n",
    "    - full_spectrum: Full Fourier spectrum for all frequencies.\n",
    "    \"\"\"\n",
    "    # Perform Fourier Transform\n",
    "    spectrum = rfft(data)\n",
    "    frequencies = rfftfreq(data.size, d=1)  # Assuming unit time interval between samples\n",
    "    \n",
    "    # Find significant amplitudes\n",
    "    amplitude = np.abs(spectrum)\n",
    "    significant_indices = amplitude > (amplitude.max() * threshold)\n",
    "    significant_freq = frequencies[significant_indices]\n",
    "    significant_ampl = amplitude[significant_indices]\n",
    "    \n",
    "    return significant_freq, significant_ampl, spectrum\n",
    "\n",
    "# def phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2, alpha=0.5):\n",
    "#     \"\"\"\n",
    "#     Combine two signals based on phase and magnitude mixup technique.\n",
    "    \n",
    "#     Args:\n",
    "#     - sig_freq1: Significant frequencies of the first signal.\n",
    "#     - sig_ampl1: Significant amplitudes of the first signal.\n",
    "#     - spectrum1: Full Fourier spectrum of the first signal.\n",
    "#     - sig_freq2: Significant frequencies of the second signal.\n",
    "#     - sig_ampl2: Significant amplitudes of the second signal.\n",
    "#     - spectrum2: Full Fourier spectrum of the second signal.\n",
    "#     - alpha: Mixup coefficient (0 to 1).\n",
    "    \n",
    "#     Returns:\n",
    "#     - mixed_spectrum: The mixed spectrum after applying mixup.\n",
    "#     \"\"\"\n",
    "#     # Make a copy of the first spectrum to start with\n",
    "#     mixed_spectrum = np.copy(spectrum1)\n",
    "#     freqs = rfftfreq(spectrum1.size, d=1)\n",
    "    \n",
    "#     # Calculate mixup for each significant frequency\n",
    "#     for freq in sig_freq1:\n",
    "#         # Find the index of the nearest frequency\n",
    "#         index = np.argmin(np.abs(freqs - freq))\n",
    "#         phase1 = np.angle(spectrum1[index])\n",
    "#         phase2 = np.angle(spectrum2[index])\n",
    "        \n",
    "#         # Calculate shortest phase difference\n",
    "#         phase_diff = (phase2 - phase1) % (2 * np.pi)\n",
    "#         phase_diff = phase_diff - 2 * np.pi if phase_diff > np.pi else phase_diff\n",
    "        \n",
    "#         # Calculate new amplitude and phase\n",
    "#         new_amplitude = alpha * sig_ampl1[index] + (1 - alpha) * sig_ampl2[index]\n",
    "#         new_phase = phase1 + alpha * phase_diff\n",
    "        \n",
    "#         # Apply mixup\n",
    "#         mixed_spectrum[index] = new_amplitude * np.exp(1j * new_phase)\n",
    "    \n",
    "#     return mixed_spectrum\n",
    "\n",
    "def phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2, alpha=0.5):\n",
    "    mixed_spectrum = np.copy(spectrum1)\n",
    "    freqs1 = rfftfreq(spectrum1.size, d=1)\n",
    "    freqs2 = rfftfreq(spectrum2.size, d=1)\n",
    "    \n",
    "    for freq in sig_freq1:\n",
    "        index1 = np.argmin(np.abs(freqs1 - freq))\n",
    "        index2 = np.argmin(np.abs(freqs2 - freq))\n",
    "        \n",
    "        if index1 >= len(sig_ampl1) or index2 >= len(sig_ampl2):\n",
    "            continue  # Skip the frequency if the index is out of bounds\n",
    "\n",
    "        phase1 = np.angle(spectrum1[index1])\n",
    "        phase2 = np.angle(spectrum2[index2])\n",
    "        \n",
    "        phase_diff = (phase2 - phase1) % (2 * np.pi)\n",
    "        phase_diff = phase_diff - 2 * np.pi if phase_diff > np.pi else phase_diff\n",
    "        \n",
    "        new_amplitude = alpha * sig_ampl1[index1] + (1 - alpha) * sig_ampl2[index2]\n",
    "        new_phase = phase1 + alpha * phase_diff\n",
    "        \n",
    "        mixed_spectrum[index1] = new_amplitude * np.exp(1j * new_phase)\n",
    "    \n",
    "    return mixed_spectrum\n",
    "\n",
    "\n",
    "def reconstruct_time_series(mixed_spectrum):\n",
    "    \"\"\"\n",
    "    Reconstruct time series from mixed spectrum using inverse Fourier Transform.\n",
    "    \n",
    "    Returns:\n",
    "    - mixed_time_series: The reconstructed time series.\n",
    "    \"\"\"\n",
    "    # Perform inverse Fourier Transform\n",
    "    mixed_time_series = irfft(mixed_spectrum)\n",
    "    \n",
    "    return mixed_time_series"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfdddd32",
   "metadata": {},
   "source": [
    "# Step 1: Get significant frequencies and amplitude for both time series\n",
    "sig_freq1, sig_ampl1, spectrum1 = get_significant_frequencies(close1.values)\n",
    "sig_freq2, sig_ampl2, spectrum2 = get_significant_frequencies(close2.values)\n",
    "\n",
    "# Step 2: Identify significant frequencies (already done in step 1)\n",
    "\n",
    "# Step 3: Phase and Magnitude Mixup\n",
    "mixed_spectrum = phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2)\n",
    "\n",
    "# Step 4: Reconstruction of the time series\n",
    "mixed_time_series = reconstruct_time_series(mixed_spectrum)\n",
    "\n",
    "mixed_time_series[:10]  # Display the first 10 points of the mixed time series"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df30d83f",
   "metadata": {},
   "source": [
    "def augment_data(ts1, ts2):\n",
    "    # Step 1: Get significant frequencies and amplitude for both time series\n",
    "    sig_freq1, sig_ampl1, spectrum1 = get_significant_frequencies(ts1.values)\n",
    "    sig_freq2, sig_ampl2, spectrum2 = get_significant_frequencies(ts2.values)\n",
    "\n",
    "    # Step 2: Identify significant frequencies (already done in step 1)\n",
    "\n",
    "    # Step 3: Phase and Magnitude Mixup\n",
    "    mixed_spectrum = phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2)\n",
    "\n",
    "    # Step 4: Reconstruction of the time series\n",
    "    mixed_time_series = reconstruct_time_series(mixed_spectrum)\n",
    "    \n",
    "    return mixed_time_series"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "232292c1",
   "metadata": {},
   "source": [
    "# Plot the mixed time series and the original ones on the same plot\n",
    "plt.figure(figsize=(14, 7))  # Set the figure size for better visibility\n",
    "plt.plot(mixed_time_series, label='Mixed Time Series')\n",
    "plt.plot(close1.values, label='AAPL')\n",
    "plt.plot(close2.values, label='MSFT')\n",
    "plt.title('Mixed Time Series and Original Time Series')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()  # Show the legend to identify each line\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6329ed3c",
   "metadata": {},
   "source": [
    "### Gather all 30 stock data from DJIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f8a1d50",
   "metadata": {},
   "source": [
    "# Define the list of Dow Jones Industrial Average companies\n",
    "tickers = [\n",
    "    \"MMM\", \"AXP\", \"AMGN\", \"AAPL\", \"BA\", \"CAT\", \"CVX\", \"CSCO\", \"KO\", \"DIS\",\n",
    "    \"DOW\", \"GS\", \"HD\", \"HON\", \"IBM\", \"INTC\", \"JNJ\", \"JPM\", \"MCD\", \"MRK\",\n",
    "    \"MSFT\", \"NKE\", \"PG\", \"CRM\", \"TRV\", \"UNH\", \"V\", \"WBA\", \"WMT\"\n",
    "]\n",
    "\n",
    "# Create a dictionary to store historical data for each company\n",
    "historical_data = {}\n",
    "\n",
    "# Loop through the Dow companies and retrieve historical data\n",
    "for ticker in tickers:\n",
    "    stock_data = get_stock_data(ticker, start_date, end_date)\n",
    "    historical_data[ticker] = stock_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74282c2b",
   "metadata": {},
   "source": [
    "historical_data_df = pd.DataFrame(historical_data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "457e2f31",
   "metadata": {},
   "source": [
    "#### Question to ask myself.\n",
    "Let N be the number of new datasets and Let C be the set of N stocks in DJIA that are selected to be paired with the selected stock. i.e. if I want to predict the next day stock return of AAPL, which 10/15/20 stocks should I augment AAPL with to create new features out of them.\n",
    "\n",
    "How do I choose N?\n",
    "- Iteratively and optimise based on MSE/MAE/Sharpe\n",
    "- Run statistical tests such as paired t-tests to compare model's performance with different volumes of augmented data against the baseline of real data only (find the inflexion point)\n",
    "\n",
    "How do I select the N stocks?\n",
    "- correlation\n",
    "- sectors\n",
    "- clustering with the use of unsupervised learning techniques\n",
    "- random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba87c3",
   "metadata": {},
   "source": [
    "## Determine the number of augmented datasets to use\n",
    "In this case, we will use AAPL as our control factor.\n",
    "\n",
    "### Base case\n",
    "Using only real AAPL data\n",
    "\n",
    "sidenote: Train test split will be a 2/3, 1/3 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdd0b241",
   "metadata": {},
   "source": [
    "aapl = historical_data['AAPL']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c258c42e",
   "metadata": {},
   "source": [
    "aapl"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55bb5eda",
   "metadata": {},
   "source": [
    "def create_features(stock_data, ticker, windows=[10, 20, 40, 60, 80, 120, 240]):\n",
    "    \"\"\"\n",
    "    Create time series features for stock data including rolling volatility,\n",
    "    normalized simple moving averages, rolling median to mean ratio,\n",
    "    rolling standard deviation to mean ratio, and rolling return.\n",
    "\n",
    "    Args:\n",
    "    stock_data (pd.DataFrame): DataFrame with a 'Close' column containing stock prices.\n",
    "    windows (list of int): List of window sizes for calculating rolling features.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with new features.\n",
    "    \"\"\"\n",
    "    stock_data = pd.DataFrame(stock_data)\n",
    "\n",
    "    for window in windows:\n",
    "        # Rolling Volatility (standard deviation of returns)\n",
    "        stock_data[f'Volatility_{window}d_{ticker}'] = stock_data['Close'].pct_change().rolling(window=window).std()\n",
    "\n",
    "        # Simple Moving Average normalized by 5-day Simple Moving Average\n",
    "        sma = stock_data['Close'].rolling(window=window).mean()\n",
    "        sma_5d = stock_data['Close'].rolling(window=5).mean()\n",
    "        stock_data[f'SMA_{window}d_norm_{ticker}'] = sma / sma_5d\n",
    "\n",
    "        # Rolling Median to Mean Ratio\n",
    "        median = stock_data['Close'].rolling(window=window).median()\n",
    "        mean = stock_data['Close'].rolling(window=window).mean()\n",
    "        stock_data[f'Median_to_Mean_{window}d_{ticker}'] = median / mean\n",
    "\n",
    "        # Rolling Standard Deviation to Mean Ratio\n",
    "        std = stock_data['Close'].rolling(window=window).std()\n",
    "        stock_data[f'Std_to_Mean_{window}d_{ticker}'] = std / mean\n",
    "\n",
    "    # Daily returns\n",
    "    stock_data[f'Returns_{ticker}'] = stock_data['Close'].pct_change() * 100\n",
    "\n",
    "    # Drop the initial rows with NaN values due to rolling windows\n",
    "    stock_data = stock_data.dropna()\n",
    "    \n",
    "    y = stock_data[f\"Returns_{ticker}\"]\n",
    "    x = stock_data.drop(columns=['Close', f'Returns_{ticker}'])\n",
    "\n",
    "    return x, y"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0602edd4",
   "metadata": {},
   "source": [
    "x, y = create_features(aapl,'MSFT')\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "# Train Test Split\n",
    "x_train = x[:int(x.shape[0]*2/3)]\n",
    "x_test = x[int(x.shape[0]*2/3):]\n",
    "\n",
    "y_train = y[:int(y.shape[0]*2/3)]\n",
    "y_test = y[int(y.shape[0]*2/3):]\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33373cf6",
   "metadata": {},
   "source": [
    "x_train"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9bf13a41",
   "metadata": {},
   "source": [
    "### Fit a XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe948693",
   "metadata": {},
   "source": [
    "import xgboost as xgb"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00093292",
   "metadata": {},
   "source": [
    "# Base params\n",
    "params = {'max_depth': 2,\n",
    " 'min_child_weight': 2,\n",
    " 'gamma': 1,\n",
    " 'subsample': 1.0,\n",
    " 'colsample_bytree': 1.0,\n",
    " 'reg_lambda': 1,\n",
    " 'reg_alpha': 0,\n",
    " 'learning_rate': 0.01,\n",
    " 'n_estimators': 1000\n",
    "}\n",
    "\n",
    "matrix_train = xgb.DMatrix(x_train,label=y_train)\n",
    "matrix_test = xgb.DMatrix(x_test,label=y_test)\n",
    "\n",
    "evals = [(matrix_train, 'train'), (matrix_test, 'test')]\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "preds_train = model.predict(x_train)\n",
    "preds_test = model.predict(x_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b64eca7e",
   "metadata": {},
   "source": [
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_[2:]\n",
    "\n",
    "# Get feature names\n",
    "feature_names = x_train.columns.tolist()[2:]\n",
    "\n",
    "# Sort the feature importances in descending order and match them with their names\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "sorted_feature_names = [feature_names[i] for i in indices]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances in XGBoost\")\n",
    "plt.barh(range(len(feature_importances)), feature_importances[indices], align='center', color='skyblue')\n",
    "plt.yticks(range(len(feature_importances)), sorted_feature_names)\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a1d0941",
   "metadata": {},
   "source": [
    "# calculating RMSE\n",
    "rmse_train = np.sqrt(np.mean((preds_train-y_train.values)**2))\n",
    "rmse_test = np.sqrt(np.mean((preds_test-y_test.values)**2))\n",
    "print(f'The RMSE for train is: {round(rmse_train,3)} and test is {round(rmse_test,3)}.')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f142dea9",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8f2c8dc",
   "metadata": {},
   "source": [
    "# 1. N = 5 using top correlated pairs\n",
    "returns_df = pd.DataFrame(historical_data).pct_change().iloc[1:]\n",
    "returns_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34e8b004",
   "metadata": {},
   "source": [
    "# Calculate the correlation matrix of the returns\n",
    "correlation_matrix = returns_df.corr()\n",
    "aapl_corr = correlation_matrix['AAPL'].sort_values(ascending=False)\n",
    "top_5_pairs = list(aapl_corr[1:6].index)\n",
    "\n",
    "top_5_pairs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cbf6b5e",
   "metadata": {},
   "source": [
    "aapl_corr = correlation_matrix['AAPL'].sort_values(ascending=True)\n",
    "bot_5_pairs = list(aapl_corr[:5].index)\n",
    "\n",
    "bot_5_pairs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "488fe937",
   "metadata": {},
   "source": [
    "# Control is AAPL\n",
    "aapl = historical_data_df['AAPL']\n",
    "msft = historical_data_df['MSFT']\n",
    "\n",
    "# Create the augmented data for N = 1 \n",
    "aug_ts1 = augment_data(aapl, msft)\n",
    "aug_ts1 = pd.Series(aug_ts1, index=aapl.index, name='Close')\n",
    "\n",
    "# Create features\n",
    "x, y = create_features(aug_ts1, 'MSFT')\n",
    "\n",
    "# Train Test Split\n",
    "x_train = x[:int(x.shape[0]*2/3)]\n",
    "x_test = x[int(x.shape[0]*2/3):]\n",
    "\n",
    "y_train = y[:int(y.shape[0]*2/3)]\n",
    "y_test = y[int(y.shape[0]*2/3):]\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7456b338",
   "metadata": {},
   "source": [
    "# Base params\n",
    "params = {'max_depth': 2,\n",
    " 'min_child_weight': 2,\n",
    " 'gamma': 1,\n",
    " 'subsample': 1.0,\n",
    " 'colsample_bytree': 1.0,\n",
    " 'reg_lambda': 1,\n",
    " 'reg_alpha': 0,\n",
    " 'learning_rate': 0.01,\n",
    " 'n_estimators': 1000\n",
    "}\n",
    "\n",
    "matrix_train = xgb.DMatrix(x_train,label=y_train)\n",
    "matrix_test = xgb.DMatrix(x_test,label=y_test)\n",
    "\n",
    "evals = [(matrix_train, 'train'), (matrix_test, 'test')]\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "preds_train = model.predict(x_train)\n",
    "preds_test = model.predict(x_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10a38b3c",
   "metadata": {},
   "source": [
    "# calculating RMSE\n",
    "rmse_train = np.sqrt(np.mean((preds_train-y_train.values)**2))\n",
    "rmse_test = np.sqrt(np.mean((preds_test-y_test.values)**2))\n",
    "print(f'The RMSE for train is: {round(rmse_train,3)} and test is {round(rmse_test,3)}.')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5d8c3df2",
   "metadata": {},
   "source": [
    "#### Mini experiment using the least correlated ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "687ff7b0",
   "metadata": {},
   "source": [
    "# Control is AAPL\n",
    "aapl = historical_data_df['AAPL']\n",
    "mrk = historical_data_df['WMT']\n",
    "\n",
    "# Create the augmented data for N = 1 \n",
    "aug_ts1 = augment_data(aapl, mrk)\n",
    "aug_ts1 = pd.Series(aug_ts1, index=aapl.index, name='Close')\n",
    "\n",
    "# Create features\n",
    "x, y = create_features(aug_ts1, 'WMT')\n",
    "\n",
    "# Train Test Split\n",
    "x_train = x[:int(x.shape[0]*2/3)]\n",
    "x_test = x[int(x.shape[0]*2/3):]\n",
    "\n",
    "y_train = y[:int(y.shape[0]*2/3)]\n",
    "y_test = y[int(y.shape[0]*2/3):]\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db9c3146",
   "metadata": {},
   "source": [
    "# Base params\n",
    "params = {'max_depth': 2,\n",
    " 'min_child_weight': 2,\n",
    " 'gamma': 1,\n",
    " 'subsample': 1.0,\n",
    " 'colsample_bytree': 1.0,\n",
    " 'reg_lambda': 1,\n",
    " 'reg_alpha': 0,\n",
    " 'learning_rate': 0.01,\n",
    " 'n_estimators': 1000\n",
    "}\n",
    "\n",
    "matrix_train = xgb.DMatrix(x_train,label=y_train)\n",
    "matrix_test = xgb.DMatrix(x_test,label=y_test)\n",
    "\n",
    "evals = [(matrix_train, 'train'), (matrix_test, 'test')]\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "preds_train = model.predict(x_train)\n",
    "preds_test = model.predict(x_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b59816ac",
   "metadata": {},
   "source": [
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_[2:]\n",
    "\n",
    "# Get feature names\n",
    "feature_names = x_train.columns.tolist()[2:]\n",
    "\n",
    "# Sort the feature importances in descending order and match them with their names\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "sorted_feature_names = [feature_names[i] for i in indices]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances in XGBoost\")\n",
    "plt.barh(range(len(feature_importances)), feature_importances[indices], align='center', color='skyblue')\n",
    "plt.yticks(range(len(feature_importances)), sorted_feature_names)\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8216548",
   "metadata": {},
   "source": [
    "# calculating RMSE\n",
    "rmse_train = np.sqrt(np.mean((preds_train-y_train.values)**2))\n",
    "rmse_test = np.sqrt(np.mean((preds_test-y_test.values)**2))\n",
    "print(f'The RMSE for train is: {round(rmse_train,3)} and test is {round(rmse_test,3)}.')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1ddae154",
   "metadata": {},
   "source": [
    "Observations. We can see that using a ts of a lower correlation can lead to greater overfit. Overall still a better fit than using the original data though.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99cfd28",
   "metadata": {},
   "source": [
    "#### N = 5 (for fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0cc05ab",
   "metadata": {},
   "source": [
    "top_5_pairs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0b3dc43",
   "metadata": {},
   "source": [
    "df_x, df_y = pd.DataFrame(), pd.DataFrame()\n",
    "for ticker in top_5_pairs:\n",
    "    ts = historical_data_df[ticker]\n",
    "    aug_ts = augment_data(aapl, ts)\n",
    "    aug_ts1 = pd.Series(aug_ts1, index=aapl.index, name='Close')\n",
    "    \n",
    "    # Create features\n",
    "    x, y = create_features(aug_ts1, ticker)\n",
    "    df_x = pd.concat([df_x, x], axis = 1)\n",
    "    df_y = pd.concat([df_y, y], axis = 1)    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eeadbb92",
   "metadata": {},
   "source": [
    "# Train Test Split\n",
    "x_train = df_x[:int(df_x.shape[0]*2/3)]\n",
    "x_test = df_x[int(df_x.shape[0]*2/3):]\n",
    "\n",
    "y_train = df_y[:int(df_y.shape[0]*2/3)]\n",
    "y_test = df_y[int(df_y.shape[0]*2/3):]\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddba6938",
   "metadata": {},
   "source": [
    "# Base params\n",
    "params = {'max_depth': 2,\n",
    " 'min_child_weight': 2,\n",
    " 'gamma': 1,\n",
    " 'subsample': 1.0,\n",
    " 'colsample_bytree': 1.0,\n",
    " 'reg_lambda': 1,\n",
    " 'reg_alpha': 0,\n",
    " 'learning_rate': 0.01,\n",
    " 'n_estimators': 1000\n",
    "}\n",
    "\n",
    "matrix_train = xgb.DMatrix(x_train,label=y_train)\n",
    "matrix_test = xgb.DMatrix(x_test,label=y_test)\n",
    "\n",
    "evals = [(matrix_train, 'train'), (matrix_test, 'test')]\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "preds_train = model.predict(x_train)\n",
    "preds_test = model.predict(x_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "227d09b3",
   "metadata": {},
   "source": [
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_[2:]\n",
    "\n",
    "# Get feature names\n",
    "feature_names = x_train.columns.tolist()[2:]\n",
    "\n",
    "# Sort the feature importances in descending order and match them with their names\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "sorted_feature_names = [feature_names[i] for i in indices]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances in XGBoost\")\n",
    "plt.barh(range(len(feature_importances)), feature_importances[indices], align='center', color='skyblue')\n",
    "plt.yticks(range(len(feature_importances)), sorted_feature_names)\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3534e101",
   "metadata": {},
   "source": [
    "# calculating RMSE\n",
    "rmse_train = np.sqrt(np.mean((preds_train-y_train.values)**2))\n",
    "rmse_test = np.sqrt(np.mean((preds_test-y_test.values)**2))\n",
    "print(f'The RMSE for train is: {round(rmse_train,3)} and test is {round(rmse_test,3)}.')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "db487182",
   "metadata": {},
   "source": [
    "Observation: Clearly can see there is an overfit and too many features were being created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afdca29",
   "metadata": {},
   "source": [
    "### Add 3 other models into play\n",
    "1. LSTM\n",
    "2. GRU\n",
    "3. TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "002ab11a",
   "metadata": {},
   "source": [
    "# Control is AAPL\n",
    "aapl = historical_data_df['AAPL']\n",
    "msft = historical_data_df['MSFT']\n",
    "\n",
    "# Create the augmented data for N = 1 \n",
    "aug_ts1 = augment_data(aapl, mrk)\n",
    "aug_ts1 = pd.Series(aug_ts1, index=aapl.index, name='Close')\n",
    "\n",
    "# Create features\n",
    "x, y = create_features(aug_ts1, 'MSFT')\n",
    "\n",
    "# Train Test Split\n",
    "x_train = x[:int(x.shape[0]*2/3)]\n",
    "x_test = x[int(x.shape[0]*2/3):]\n",
    "\n",
    "y_train = y[:int(y.shape[0]*2/3)]\n",
    "y_test = y[int(y.shape[0]*2/3):]\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "af4e66fd",
   "metadata": {},
   "source": [
    "# Convert DataFrame to correct shape for LSTM, GRU, and TCN\n",
    "# Assuming each sample is one time step and each feature is one column in the DataFrame\n",
    "x_train_reshaped = x_train.values.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_test_reshaped = x_test.values.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "# Convert target arrays to 1D numpy arrays for regression output\n",
    "y_train_reshaped = y_train.values.flatten()\n",
    "y_test_reshaped = y_test.values.flatten()\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Define a function to create and train an LSTM model\n",
    "def create_train_lstm(x_train, y_train, x_test, y_test):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_test, y_test), callbacks=[early_stopping], verbose=0)\n",
    "    return model\n",
    "\n",
    "# Define a function to create and train a GRU model\n",
    "def create_train_gru(x_train, y_train, x_test, y_test):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(50, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_test, y_test), callbacks=[early_stopping], verbose=0)\n",
    "    return model\n",
    "\n",
    "# Define a function to create and train a TCN model\n",
    "def create_train_tcn(x_train, y_train, x_test, y_test):\n",
    "    model = Sequential()\n",
    "    model.add(TCN(50, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_test, y_test), callbacks=[early_stopping], verbose=0)\n",
    "    return model\n",
    "\n",
    "# Function to evaluate RMSE\n",
    "def evaluate_rmse(model, x, y_true):\n",
    "    y_pred = model.predict(x)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return rmse\n",
    "\n",
    "# Train and evaluate LSTM\n",
    "lstm_model = create_train_lstm(x_train_reshaped, y_train_reshaped, x_test_reshaped, y_test_reshaped)\n",
    "lstm_train_rmse = evaluate_rmse(lstm_model, x_train_reshaped, y_train_reshaped)\n",
    "lstm_test_rmse = evaluate_rmse(lstm_model, x_test_reshaped, y_test_reshaped)\n",
    "\n",
    "# Train and evaluate GRU\n",
    "gru_model = create_train_gru(x_train_reshaped, y_train_reshaped, x_test_reshaped, y_test_reshaped)\n",
    "gru_train_rmse = evaluate_rmse(gru_model, x_train_reshaped, y_train_reshaped)\n",
    "gru_test_rmse = evaluate_rmse(gru_model, x_test_reshaped, y_test_reshaped)\n",
    "\n",
    "# # Train and evaluate TCN\n",
    "tcn_model = create_train_tcn(x_train_reshaped, y_train_reshaped, x_test_reshaped, y_test_reshaped)\n",
    "tcn_train_rmse = evaluate_rmse(tcn_model, x_train_reshaped, y_train_reshaped)\n",
    "tcn_test_rmse = evaluate_rmse(tcn_model, x_test_reshaped, y_test_reshaped)\n",
    "\n",
    "# Compare RMSEs\n",
    "print(f\"LSTM Train RMSE: {round(lstm_train_rmse,4)}, Test RMSE: {round(lstm_test_rmse,4)}\")\n",
    "print(f\"GRU Train RMSE: {round(gru_train_rmse,4)}, Test RMSE: {round(gru_test_rmse,4)}\")\n",
    "print(f\"TCN Train RMSE: {round(tcn_train_rmse,4)}, Test RMSE: {round(tcn_test_rmse,4)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "27635283",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from kerastuner import HyperModel, RandomSearch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define your hypermodel\n",
    "class LSTMHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu', input_shape=self.input_shape))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        return model\n",
    "\n",
    "# Create a function that returns a new EarlyStopping callback for each trial\n",
    "def build_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    ]\n",
    "\n",
    "# Set up the tuner\n",
    "tuner = RandomSearch(\n",
    "    hypermodel=LSTMHyperModel(input_shape=(x_train_reshaped.shape[1], x_train_reshaped.shape[2])),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='lstm_hyperopt'\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(\n",
    "    x_train_reshaped, y_train_reshaped,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    callbacks=build_callbacks()\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Train the best model\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "best_model.fit(\n",
    "    x_train_reshaped, y_train_reshaped,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    callbacks=build_callbacks()\n",
    ")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7841112c",
   "metadata": {},
   "source": [
    "best_hps"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271e843",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
