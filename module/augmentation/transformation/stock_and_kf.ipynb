{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Title: To determine if data augmentation using the method proposed in 'Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning' will lead to better 1 day prediction results.\n",
    "\n"
   ],
   "id": "a00630f065c13814"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Seed value\n",
    "seed_value= 42\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)\n"
   ],
   "id": "efafe1f80b19d57d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import seaborn as sb\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import rfft, rfftfreq, irfft\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tcn import TCN  # If you have the tcn p /ackage installed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ],
   "id": "b4477cfa02d57b62",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(1337)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Assuming cfg is a configuration object with a seed attribute\n",
    "cfg = type('config', (object,), {'seed': 42})\n",
    "# Make the sampler behave in a deterministic way.\n",
    "sampler = TPESampler(seed=cfg.seed)"
   ],
   "id": "c4716bd0ef872ea6",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "source": [
    "# Function to import stock data\n",
    "def get_stock_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return data\n",
    "\n",
    "def z_score_normalize(series):\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    return (series - mean) / std\n",
    "\n",
    "def denormalize_z_score(normalized_series, original_mean, original_std):\n",
    "    return (normalized_series * original_std) + original_mean\n",
    "\n",
    "# Function to create sequences and corresponding returns\n",
    "def create_sequences(data, returns, sequence_length=20):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i+sequence_length])\n",
    "        y.append(returns[i+sequence_length])\n",
    "    return np.array(X), np.array(y)"
   ],
   "id": "663d6aa9dcc74dc3",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "source": [
    "def cut_mix(df1, df2, alpha=0.2):\n",
    "    assert df1.shape == df2.shape\n",
    "    size = len(df1)\n",
    "    cut_point = np.random.randint(0, size)\n",
    "    cut_length = int(size * alpha)\n",
    "    \n",
    "    mixed_df = df1.copy()\n",
    "    mixed_df.iloc[cut_point:cut_point+cut_length] = df2.iloc[cut_point:cut_point+cut_length]\n",
    "    \n",
    "    return mixed_df\n",
    "\n",
    "def binary_mix(data1, data2, alpha=0.2):\n",
    "    assert len(data1) == len(data2)\n",
    "    size = data1.shape\n",
    "    mask = np.random.binomial(1, alpha, size=size).astype(bool)\n",
    "    \n",
    "    mixed_data = np.where(mask, data1, data2)\n",
    "    \n",
    "    return pd.DataFrame(mixed_data, columns=['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
    "\n",
    "def linear_mix(data1, data2, alpha=0.2):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    mixed_data = alpha * data1 + (1 - alpha) * data2\n",
    "    \n",
    "    return mixed_data\n",
    "\n",
    "def geometric_mix(data1, data2, alpha=0.2):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    mixed_data = data1**alpha * data2**(1 - alpha)\n",
    "    \n",
    "    return mixed_data\n",
    "\n",
    "def amplitude_mix(data1, data2, alpha=0.2):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    fft1 = np.fft.rfft(data1)\n",
    "    fft2 = np.fft.rfft(data2)\n",
    "    \n",
    "    # Mix the magnitudes\n",
    "    magnitude1 = np.abs(fft1)\n",
    "    magnitude2 = np.abs(fft2)\n",
    "    mixed_magnitude = alpha * magnitude1 + (1 - alpha) * magnitude2\n",
    "    \n",
    "    # Keep the phase of the first data\n",
    "    phase1 = np.angle(fft1)\n",
    "    mixed_fft = mixed_magnitude * np.exp(1j * phase1)\n",
    "    \n",
    "    mixed_data = np.fft.irfft(mixed_fft)\n",
    "    \n",
    "    return pd.DataFrame(mixed_data, columns=['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
    "\n",
    "### PROPOSE TECHNIQUE BELOW\n",
    "\n",
    "def proposed_mixup(df1, df2, threshold=0.1, alpha=0.5):\n",
    "    \n",
    "    def proposed_mixup_feature(data1, data2, threshold, alpha):\n",
    "        \n",
    "        def get_significant_frequencies(data, threshold):\n",
    "            \"\"\"\n",
    "            Perform Fourier Transform on data and identify frequencies with significant amplitude.\n",
    "\n",
    "            Args:\n",
    "            - data: Time series data.\n",
    "            - threshold: Threshold for significance, relative to the max amplitude.\n",
    "\n",
    "            Returns:\n",
    "            - significant_freq: Frequencies with significant amplitude.\n",
    "            - significant_ampl: Amplitude of the significant frequencies.\n",
    "            - full_spectrum: Full Fourier spectrum for all frequencies.\n",
    "            \"\"\"\n",
    "            # Perform Fourier Transform\n",
    "            spectrum = rfft(data)\n",
    "            frequencies = rfftfreq(data.size, d=1)  # Assuming unit time interval between samples\n",
    "\n",
    "            # Find significant amplitudes\n",
    "            amplitude = np.abs(spectrum)\n",
    "            significant_indices = amplitude > (amplitude.max() * threshold)\n",
    "            significant_freq = frequencies[significant_indices]\n",
    "            significant_ampl = amplitude[significant_indices]\n",
    "\n",
    "            return significant_freq, significant_ampl, spectrum\n",
    "\n",
    "        def phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2, alpha):\n",
    "            mixed_spectrum = np.copy(spectrum1)\n",
    "            freqs1 = rfftfreq(spectrum1.size, d=1)\n",
    "            freqs2 = rfftfreq(spectrum2.size, d=1)\n",
    "\n",
    "            for freq in sig_freq1:\n",
    "                index1 = np.argmin(np.abs(freqs1 - freq))\n",
    "                index2 = np.argmin(np.abs(freqs2 - freq))\n",
    "\n",
    "                if index1 >= len(sig_ampl1) or index2 >= len(sig_ampl2):\n",
    "                    continue  # Skip the frequency if the index is out of bounds\n",
    "\n",
    "                phase1 = np.angle(spectrum1[index1])\n",
    "                phase2 = np.angle(spectrum2[index2])\n",
    "\n",
    "                phase_diff = (phase2 - phase1) % (2 * np.pi)\n",
    "                phase_diff = phase_diff - 2 * np.pi if phase_diff > np.pi else phase_diff\n",
    "\n",
    "                new_amplitude = alpha * sig_ampl1[index1] + (1 - alpha) * sig_ampl2[index2]\n",
    "                new_phase = phase1 + alpha * phase_diff\n",
    "\n",
    "                mixed_spectrum[index1] = new_amplitude * np.exp(1j * new_phase)\n",
    "\n",
    "            return mixed_spectrum\n",
    "\n",
    "\n",
    "        def reconstruct_time_series(mixed_spectrum):\n",
    "            \"\"\"\n",
    "            Reconstruct time series from mixed spectrum using inverse Fourier Transform.\n",
    "\n",
    "            Returns:\n",
    "            - mixed_time_series: The reconstructed time series.\n",
    "            \"\"\"\n",
    "            # Perform inverse Fourier Transform\n",
    "            mixed_time_series = irfft(mixed_spectrum)\n",
    "\n",
    "            return mixed_time_series\n",
    "\n",
    "        # Step 1: Get significant frequencies and amplitude for both time series\n",
    "        sig_freq1, sig_ampl1, spectrum1 = get_significant_frequencies(data1, threshold)\n",
    "        sig_freq2, sig_ampl2, spectrum2 = get_significant_frequencies(data2, threshold)\n",
    "\n",
    "        # Step 2: Identify significant frequencies (already done in step 1)\n",
    "\n",
    "        # Step 3: Phase and Magnitude Mixup\n",
    "        mixed_spectrum = phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2, alpha)\n",
    "\n",
    "        # Step 4: Reconstruction of the time series\n",
    "        mixed_time_series = reconstruct_time_series(mixed_spectrum)\n",
    "        return mixed_time_series\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    \n",
    "    for feature in df1.columns:\n",
    "        output_df[feature] = proposed_mixup_feature(df1[feature].values, df2[feature].values, threshold, alpha)\n",
    "        \n",
    "    return output_df"
   ],
   "id": "23284b3d35081d59",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "source": [
    "def create_kalman_filters(df):\n",
    "    kalman_filters = {}\n",
    "    filtered_data = pd.DataFrame(index=df.index)\n",
    "\n",
    "    for column in df.columns:\n",
    "        # Set up the Kalman Filter\n",
    "        kf = KalmanFilter(initial_state_mean=0, n_dim_obs=1)\n",
    "\n",
    "        # Use observed data to estimate parameters\n",
    "        kf = kf.em(df[column].values, n_iter=5)\n",
    "\n",
    "        # Apply the Kalman Filter to smooth data\n",
    "        (filtered_state_means, _) = kf.filter(df[column].values)\n",
    "        \n",
    "        # Store the filtered data in a DataFrame\n",
    "        filtered_data[column] = filtered_state_means.flatten()\n",
    "        \n",
    "        # Save the Kalman Filter for each column\n",
    "        kalman_filters[column] = kf\n",
    "\n",
    "    return kalman_filters, filtered_data"
   ],
   "id": "db4e128d8733c1c9",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "source": [
    "def create_augmented_data(rets, df1, df2, method, alpha, window_size=20):\n",
    "    if method == 'cut_mix':\n",
    "         df = cut_mix(df1, df2, alpha)\n",
    "    elif method == 'binary_mix':\n",
    "         df = binary_mix(df1, df2, alpha)\n",
    "    elif method == 'linear_mix':\n",
    "         df = linear_mix(df1, df2, alpha)\n",
    "    elif method == 'geometrix_mix':\n",
    "         df = geometric_mix(df1, df2, alpha)\n",
    "    elif method == 'amplitude_mix':\n",
    "         df = amplitude_mix(df1, df2, alpha)\n",
    "    elif method == 'proposed_mix':\n",
    "         df = proposed_mixup(df1, df2, alpha)\n",
    "            \n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']])\n",
    "        \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(scaled_features, rets, window_size)\n",
    "    \n",
    "    return X, y, df"
   ],
   "id": "56346931694f9274",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned by Optuna\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    lstm_units = trial.suggest_categorical('lstm_units', [50, 100, 150])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    \n",
    "    # Data preparation\n",
    "    scaler = StandardScaler()\n",
    "    feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    scaled_features = scaler.fit_transform(df[feature_columns])\n",
    "    sequence_length = 20  # Using 20 timesteps\n",
    "    X, y = create_sequences(scaled_features, rets, sequence_length)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "    # Model architecture\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        model.add(LSTM(units=lstm_units, return_sequences=(i < n_layers - 1)))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    # Compilation\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "    \n",
    "    # Model training\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=50,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return mse"
   ],
   "id": "a9ea8dcee7dbf01c",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "source": [
    "def create_model(best_params, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(best_params['lstm_units'], input_shape=input_shape))\n",
    "    model.add(Dense(1))  # Assuming we are predicting one value\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ],
   "id": "c8391a214734f53f",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "source": [
    "def plot_TSNE(df1, df2):\n",
    "    df1_log = np.log(df1 + 1)  # Adding 1 to avoid log(0)\n",
    "    df2_log = np.log(df2 + 1)\n",
    "\n",
    "    combined_data = pd.concat([df1_log, df2_log])\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=100, n_iter=1000)\n",
    "    tsne_results = tsne.fit_transform(combined_data)\n",
    "\n",
    "    # Now we split the t-SNE results back into original and augmented parts\n",
    "    tsne_df1 = tsne_results[:len(df1), :]\n",
    "    tsne_df2 = tsne_results[len(df1):, :]\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.scatter(tsne_df1[:, 0], tsne_df1[:, 1], label='Original', alpha=0.5)\n",
    "    plt.scatter(tsne_df2[:, 0], tsne_df2[:, 1], label='Augmented', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "90774e7d403db1e5",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Data from Yahoo Finance"
   ],
   "id": "de290669d5e7d246"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "start_date = '2010-01-01'\n",
    "end_date = '2023-01-01'\n",
    "\n",
    "# Define the list of Dow Jones Industrial Average companies\n",
    "tickers = [\n",
    "    \"MMM\", \"AXP\", \"AMGN\", \"AAPL\", \"BA\", \"CAT\", \"CVX\", \"CSCO\", \"KO\", \"DIS\",\n",
    "    \"DOW\", \"GS\", \"HD\", \"HON\", \"IBM\", \"INTC\", \"JNJ\", \"JPM\", \"MCD\", \"MRK\",\n",
    "    \"MSFT\", \"NKE\", \"PG\", \"CRM\", \"TRV\", \"UNH\", \"V\", \"WBA\", \"WMT\"\n",
    "]\n",
    "\n",
    "# Create a dictionary to store historical data for each company\n",
    "historical_data = {}\n",
    "\n",
    "# Loop through the Dow companies and retrieve historical data\n",
    "for ticker in tickers:\n",
    "    stock_data = get_stock_data(ticker, start_date, end_date)\n",
    "    historical_data[ticker] = stock_data"
   ],
   "id": "efa6024176bf21ee",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original"
   ],
   "id": "528838a26ffc5a5a"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "source": [
    "seq_len = 20\n",
    "alpha = 0.2"
   ],
   "id": "a417ae72f0583baa",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df = historical_data['AAPL'].copy()\n",
    "rets = df['Close'].pct_change()"
   ],
   "id": "8c4c84815a744ca1",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']])\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(scaled_features, rets, seq_len)"
   ],
   "id": "b239fd3a87302e2d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "# study.optimize(objective, n_trials=50)\n",
    "# print('Best trial:', study.best_trial.params)"
   ],
   "id": "fdddf48c70d7ed71",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "best_params = {'n_layers': 2, \n",
    "               'lstm_units': 100, \n",
    "               'dropout_rate': 0.24, \n",
    "               'learning_rate': 0.01,\n",
    "               'batch_size': 128}\n",
    "# best_params = study.best_trial.params\n",
    "best_params['epochs'] = 50\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_model(best_params, input_shape)\n",
    "model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'])\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_og = np.sqrt(mean_squared_error(y_test, predictions))"
   ],
   "id": "92ec84c7bcda7414",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create df2 - Kalman Filter"
   ],
   "id": "857046c99879d5fc"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "source": [
    "df1 = historical_data['AAPL']\n",
    "_, df2 = create_kalman_filters(df1)"
   ],
   "id": "af4336b12791a090",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CutMix"
   ],
   "id": "d4b69af499883b04"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "source": [
    "# Create X, y for LSTM\n",
    "X, y, df3 = create_augmented_data(rets, df1, df2, 'cut_mix', alpha, seq_len)\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "# study.optimize(objective, n_trials=50)\n",
    "# print('Best trial:', study.best_trial.params)"
   ],
   "id": "7071136eb3fb6ff1",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "best_params = {'n_layers': 1, \n",
    "               'lstm_units': 50, \n",
    "               'dropout_rate': 0.38, \n",
    "               'learning_rate': 0.007, \n",
    "               'batch_size': 128}\n",
    "# best_params = study.best_trial.params\n",
    "best_params['epochs'] = 50\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_model(best_params, input_shape)\n",
    "model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'])\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_cut = np.sqrt(mean_squared_error(y_test, predictions))"
   ],
   "id": "19532d3625f428d8",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best trial: {'n_layers': 1, 'lstm_units': 50, 'dropout_rate': 0.3754947803049919, 'learning_rate': 0.007042149702171292, 'batch_size': 128}"
   ],
   "id": "288b5131635762d3"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "source": [
    "plot_TSNE(df1,df3)"
   ],
   "id": "61630126015d22b2",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Mix"
   ],
   "id": "3ddab76091f80928"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "source": [
    "# Create X, y for LSTM\n",
    "X, y, df3 = create_augmented_data(rets, df1, df2, 'binary_mix', alpha, seq_len)\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "# study.optimize(objective, n_trials=50)\n",
    "# print('Best trial:', study.best_trial.params)"
   ],
   "id": "6d1d07c550e2b068",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "source": [
    "best_params = {'n_layers': 2, \n",
    "               'lstm_units': 100, \n",
    "               'dropout_rate': 0.18, \n",
    "               'learning_rate': 0.008, \n",
    "               'batch_size': 64}\n",
    "# best_params = study.best_trial.params\n",
    "best_params['epochs'] = 50\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_model(best_params, input_shape)\n",
    "model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'])\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_binary = np.sqrt(mean_squared_error(y_test, predictions))"
   ],
   "id": "2132539f679da7b0",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "source": [
    "plot_TSNE(df1,df3)"
   ],
   "id": "c39b54979fac0e6f",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Mix"
   ],
   "id": "51e962e847b0e98c"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "source": [
    "# Create X, y for LSTM\n",
    "X, y, df3 = create_augmented_data(rets, df1, df2, 'linear_mix', alpha, seq_len)\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "# study.optimize(objective, n_trials=50)\n",
    "# print('Best trial:', study.best_trial.params)"
   ],
   "id": "540ba732028502f6",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "best_params = {'n_layers': 2, \n",
    "               'lstm_units': 100, \n",
    "               'dropout_rate': 0.43, \n",
    "               'learning_rate': 0.0036, \n",
    "               'batch_size': 32}\n",
    "# best_params = study.best_trial.params\n",
    "best_params['epochs'] = 50\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_model(best_params, input_shape)\n",
    "model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'])\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_test, predictions))"
   ],
   "id": "325eac84bffcebf4",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "source": [
    "plot_TSNE(df1,df3)"
   ],
   "id": "e1bb3b8072135340",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geommetric Mix"
   ],
   "id": "2dfab26a8368c231"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "source": [
    "# Create X, y for LSTM\n",
    "X, y, df3 = create_augmented_data(rets, df1, df2, 'geometrix_mix', alpha, seq_len)\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "# study.optimize(objective, n_trials=50)\n",
    "# print('Best trial:', study.best_trial.params)"
   ],
   "id": "e1fdaa83e6d647d0",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "best_params = {'n_layers': 3, \n",
    "               'lstm_units': 100, \n",
    "               'dropout_rate': 0.3, \n",
    "               'learning_rate': 0.007, \n",
    "               'batch_size': 32}\n",
    "best_params['epochs'] = 50\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_model(best_params, input_shape)\n",
    "model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'])\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_geom = np.sqrt(mean_squared_error(y_test, predictions))"
   ],
   "id": "b0a27a562bfcc58c",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "source": [
    "plot_TSNE(df1,df3)"
   ],
   "id": "d1883ceb573aecaf",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amplitude Mix"
   ],
   "id": "5f09617f098947fe"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "source": [
    "# Create X, y for LSTM\n",
    "X, y, df3 = create_augmented_data(rets, df1, df2, 'amplitude_mix', alpha, seq_len)\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "# study.optimize(objective, n_trials=50)\n",
    "# print('Best trial:', study.best_trial.params)"
   ],
   "id": "7dbc2263ddecaf65",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "best_params = {'n_layers': 2, \n",
    "               'lstm_units': 50, \n",
    "               'dropout_rate': 0.18, \n",
    "               'learning_rate': 0.0025, \n",
    "               'batch_size': 32}\n",
    "# best_params = study.best_trial.params\n",
    "best_params['epochs'] = 50\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_model(best_params, input_shape)\n",
    "model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'])\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_amplitude = np.sqrt(mean_squared_error(y_test, predictions))"
   ],
   "id": "1d0b348b96c36045",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "source": [
    "plot_TSNE(df1,df3)"
   ],
   "id": "fad324c8748386dc",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Mix"
   ],
   "id": "66d20d9d7c3ec6e3"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "source": [
    "# Create X, y for LSTM\n",
    "X, y, df3 = create_augmented_data(rets, df1, df2, 'proposed_mix', alpha, seq_len)\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "# study.optimize(objective, n_trials=50)\n",
    "# print('Best trial:', study.best_trial.params)"
   ],
   "id": "d138cf89cde847a0",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "best_params = {'n_layers': 1, \n",
    "               'lstm_units': 150, \n",
    "               'dropout_rate': 0.4, \n",
    "               'learning_rate': 0.006, \n",
    "               'batch_size': 64}\n",
    "# best_params = study.best_trial.params\n",
    "best_params['epochs'] = 50\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_model(best_params, input_shape)\n",
    "model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'])\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_proposed = np.sqrt(mean_squared_error(y_test, predictions))"
   ],
   "id": "aae4f9b22b0a2f5a",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "source": [
    "plot_TSNE(df1,df3)"
   ],
   "id": "fc600f98034effbd",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ],
   "id": "3e3125b94357d46d"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "source": [
    "print('====    Original RMSE  ====')\n",
    "print(f'The RMSE of original is: {round(rmse_og,5)}\\n')\n",
    "\n",
    "print('====    Augmented/Mixup RMSE  ====')\n",
    "print(f'The RMSE of cut_mix is: {round(rmse_cut,5)}')\n",
    "print(f'The RMSE of binary_mix is: {round(rmse_binary,5)}')\n",
    "print(f'The RMSE of linear_mix is: {round(rmse_linear,5)}')\n",
    "print(f'The RMSE of geometric_mix is: {round(rmse_geom,5)}')\n",
    "print(f'The RMSE of amplitude_mix is: {round(rmse_amplitude,5)}')\n",
    "print(f'The RMSE of proposed_mix is: {round(rmse_proposed,5)}')"
   ],
   "id": "e4d4326b1f675f77",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
