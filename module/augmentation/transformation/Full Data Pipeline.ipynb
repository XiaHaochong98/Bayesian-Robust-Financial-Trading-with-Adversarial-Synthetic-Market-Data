{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Title: To determine if data augmentation using the method proposed in 'Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning' will lead to better 1 day prediction results.\n",
    "\n"
   ],
   "id": "b6e6ed238fc586ee"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Seed value\n",
    "seed_value= 42\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)\n"
   ],
   "id": "2ed6fcc652e7db7c",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from scipy.fft import rfft, rfftfreq, irfft\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tcn import TCN  # If you have the tcn p /ackage installed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ],
   "id": "32a433ec95afeab",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters"
   ],
   "id": "80db3ba82f751c13"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# Assuming cfg is a configuration object with a seed attribute\n",
    "cfg = type('config', (object,), {'seed': 42})\n",
    "# Make the sampler behave in a deterministic way.\n",
    "sampler = TPESampler(seed=cfg.seed)\n",
    "\n",
    "alpha = 0.4\n",
    "seq_len = 20\n",
    "test_size = 0.3"
   ],
   "id": "cf34781b80887c57",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ],
   "id": "461216ffb6d76c34"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# Function to import stock data\n",
    "def get_stock_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return data\n",
    "\n",
    "def z_score_normalize(series):\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    return (series - mean) / std\n",
    "\n",
    "def denormalize_z_score(normalized_series, original_mean, original_std):\n",
    "    return (normalized_series * original_std) + original_mean\n",
    "\n",
    "# Function to create model (make sure this is defined in your environment)\n",
    "def create_model(best_params, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(best_params['lstm_units'], input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(best_params['dropout_rate']))\n",
    "    model.add(LSTM(best_params['lstm_units']))  # Stacking LSTM for deep learning\n",
    "    model.add(Dropout(best_params['dropout_rate']))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mse')\n",
    "    return model\n",
    "\n",
    "def create_sequences(features, target, seq_len):\n",
    "    X, y = [], []\n",
    "    for i in range(len(target) - seq_len):\n",
    "        X.append(features[i:(i + seq_len)])\n",
    "        y.append(target[i + seq_len])\n",
    "    return np.array(X), np.array(y)"
   ],
   "id": "3c975d3bf07c7336",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "def plot_correlation(df):\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm',\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "    # Adjust the plot as needed\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()  # Adjusts the plot to ensure everything fits without overlap\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ],
   "id": "1840ae24342d0d8",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "def engineer_features(data):\n",
    "    df = data.copy(deep=True)\n",
    "    delta = df['Close'].diff()\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "    roll_up = up.rolling(window=14).mean()\n",
    "    roll_down = down.abs().rolling(window=14).mean()\n",
    "    RS = roll_up / roll_down\n",
    "    df['RSI'] = 100.0 - (100.0 / (1.0 + RS))\n",
    "\n",
    "    # Volume Weighted Average Price (VWAP)\n",
    "    vwap = (df['Volume'] * (df['High'] + df['Low'] + df['Close']) / 3).cumsum() / df['Volume'].cumsum()\n",
    "    df['VWAP'] = vwap\n",
    "\n",
    "    # Price Ratios\n",
    "    df['high_to_low_ratio'] = df['High'] / df['Low']\n",
    "    df['open_to_close_ratio'] = df['Open'] / df['Close']\n",
    "\n",
    "    # Volatility\n",
    "    df['volatility_10'] = df['Close'].rolling(window=10).std()\n",
    "\n",
    "    df1 = df.drop(columns=['Open', 'High', 'Low', 'Adj Close']).dropna()\n",
    "    return df1"
   ],
   "id": "169bda6524d115b1",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "def cut_mix(df1, df2, alpha=0.4):\n",
    "    assert df1.shape == df2.shape\n",
    "    size = len(df1)\n",
    "    cut_point = np.random.randint(0, size)\n",
    "    cut_length = int(size * alpha)\n",
    "    \n",
    "    mixed_df = df1.copy()\n",
    "    mixed_df.iloc[cut_point:cut_point+cut_length] = df2.iloc[cut_point:cut_point+cut_length]\n",
    "    \n",
    "    return mixed_df\n",
    "\n",
    "def binary_mix(data1, data2, alpha=0.4):\n",
    "    assert len(data1) == len(data2)\n",
    "    size = data1.shape\n",
    "    mask = np.random.binomial(1, alpha, size=size).astype(bool)\n",
    "    \n",
    "    mixed_data = np.where(mask, data1, data2)\n",
    "    \n",
    "    return pd.DataFrame(mixed_data, columns=['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
    "\n",
    "def linear_mix(data1, data2, alpha=0.4):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    mixed_data = alpha * data1 + (1 - alpha) * data2\n",
    "    \n",
    "    return mixed_data\n",
    "\n",
    "def geometric_mix(data1, data2, alpha=0.4):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    mixed_data = data1**alpha * data2**(1 - alpha)\n",
    "    \n",
    "    return mixed_data\n",
    "\n",
    "def amplitude_mix(data1, data2, alpha=0.4):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    fft1 = np.fft.rfft(data1)\n",
    "    fft2 = np.fft.rfft(data2)\n",
    "    \n",
    "    # Mix the magnitudes\n",
    "    magnitude1 = np.abs(fft1)\n",
    "    magnitude2 = np.abs(fft2)\n",
    "    mixed_magnitude = alpha * magnitude1 + (1 - alpha) * magnitude2\n",
    "    \n",
    "    # Keep the phase of the first data\n",
    "    phase1 = np.angle(fft1)\n",
    "    mixed_fft = mixed_magnitude * np.exp(1j * phase1)\n",
    "    \n",
    "    mixed_data = np.fft.irfft(mixed_fft)\n",
    "    \n",
    "    return pd.DataFrame(mixed_data, columns=['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
    "\n",
    "### PROPOSE TECHNIQUE BELOW\n",
    "\n",
    "def proposed_mixup(df1, df2, threshold=0.1, alpha=0.4):\n",
    "    \n",
    "    def proposed_mixup_feature(data1, data2, threshold, alpha):\n",
    "        \n",
    "        def get_significant_frequencies(data, threshold):\n",
    "            \"\"\"\n",
    "            Perform Fourier Transform on data and identify frequencies with significant amplitude.\n",
    "\n",
    "            Args:\n",
    "            - data: Time series data.\n",
    "            - threshold: Threshold for significance, relative to the max amplitude.\n",
    "\n",
    "            Returns:\n",
    "            - significant_freq: Frequencies with significant amplitude.\n",
    "            - significant_ampl: Amplitude of the significant frequencies.\n",
    "            - full_spectrum: Full Fourier spectrum for all frequencies.\n",
    "            \"\"\"\n",
    "            # Perform Fourier Transform\n",
    "            spectrum = rfft(data)\n",
    "            frequencies = rfftfreq(data.size, d=1)  # Assuming unit time interval between samples\n",
    "\n",
    "            # Find significant amplitudes\n",
    "            amplitude = np.abs(spectrum)\n",
    "            significant_indices = amplitude > (amplitude.max() * threshold)\n",
    "            significant_freq = frequencies[significant_indices]\n",
    "            significant_ampl = amplitude[significant_indices]\n",
    "\n",
    "            return significant_freq, significant_ampl, spectrum\n",
    "\n",
    "        def phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2, alpha):\n",
    "            mixed_spectrum = np.copy(spectrum1)\n",
    "            freqs1 = rfftfreq(spectrum1.size, d=1)\n",
    "            freqs2 = rfftfreq(spectrum2.size, d=1)\n",
    "\n",
    "            for freq in sig_freq1:\n",
    "                index1 = np.argmin(np.abs(freqs1 - freq))\n",
    "                index2 = np.argmin(np.abs(freqs2 - freq))\n",
    "\n",
    "                if index1 >= len(sig_ampl1) or index2 >= len(sig_ampl2):\n",
    "                    continue  # Skip the frequency if the index is out of bounds\n",
    "\n",
    "                phase1 = np.angle(spectrum1[index1])\n",
    "                phase2 = np.angle(spectrum2[index2])\n",
    "\n",
    "                phase_diff = (phase2 - phase1) % (2 * np.pi)\n",
    "                phase_diff = phase_diff - 2 * np.pi if phase_diff > np.pi else phase_diff\n",
    "\n",
    "                new_amplitude = alpha * sig_ampl1[index1] + (1 - alpha) * sig_ampl2[index2]\n",
    "                new_phase = phase1 + alpha * phase_diff\n",
    "\n",
    "                mixed_spectrum[index1] = new_amplitude * np.exp(1j * new_phase)\n",
    "\n",
    "            return mixed_spectrum\n",
    "\n",
    "\n",
    "        def reconstruct_time_series(mixed_spectrum):\n",
    "            \"\"\"\n",
    "            Reconstruct time series from mixed spectrum using inverse Fourier Transform.\n",
    "\n",
    "            Returns:\n",
    "            - mixed_time_series: The reconstructed time series.\n",
    "            \"\"\"\n",
    "            # Perform inverse Fourier Transform\n",
    "            mixed_time_series = irfft(mixed_spectrum)\n",
    "\n",
    "            return mixed_time_series\n",
    "\n",
    "        # Step 1: Get significant frequencies and amplitude for both time series\n",
    "        sig_freq1, sig_ampl1, spectrum1 = get_significant_frequencies(data1, threshold)\n",
    "        sig_freq2, sig_ampl2, spectrum2 = get_significant_frequencies(data2, threshold)\n",
    "\n",
    "        # Step 2: Identify significant frequencies (already done in step 1)\n",
    "\n",
    "        # Step 3: Phase and Magnitude Mixup\n",
    "        mixed_spectrum = phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2, alpha)\n",
    "\n",
    "        # Step 4: Reconstruction of the time series\n",
    "        mixed_time_series = reconstruct_time_series(mixed_spectrum)\n",
    "        return mixed_time_series\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    \n",
    "    for feature in df1.columns:\n",
    "        output_df[feature] = proposed_mixup_feature(df1[feature].values, df2[feature].values, threshold, alpha)\n",
    "        \n",
    "    return output_df"
   ],
   "id": "938be38b5dcbbf27",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned by Optuna\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    lstm_units = trial.suggest_categorical('lstm_units', [50, 100, 150])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    epochs = trial.suggest_int('epochs', 20, 100)\n",
    "    \n",
    "    # Dictionary to hold RMSE for each stock\n",
    "    stock_rmse = {}\n",
    "    \n",
    "    for stock, df in historical_data_augmented.items():\n",
    "        # Preprocess the data\n",
    "        df = df.copy()\n",
    "        rets = df['Close'].pct_change().dropna()\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(df[df.columns].values)\n",
    "        seq_len = 20\n",
    "        X, y = create_sequences(scaled_features, rets.values, seq_len)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        \n",
    "        # Model architecture\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "        model = Sequential()\n",
    "        for i in range(n_layers):\n",
    "            model.add(LSTM(units=lstm_units, return_sequences=(i < n_layers - 1)))\n",
    "            model.add(Dropout(rate=dropout_rate))\n",
    "        model.add(Dense(units=1))\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        # Predictions and evaluate\n",
    "        predictions = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "        stock_rmse[stock] = rmse\n",
    "    \n",
    "    # Calculate the average RMSE across all stocks\n",
    "    average_rmse = np.mean(list(stock_rmse.values()))\n",
    "    \n",
    "    return average_rmse"
   ],
   "id": "4d48431fc805b7d9",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "def create_augmented_data(rets, df1, df2, method, alpha, window_size=20):\n",
    "    if method == 'cut_mix':\n",
    "        df = cut_mix(df1, df2, alpha)\n",
    "    elif method == 'binary_mix':\n",
    "        df = binary_mix(df1, df2, alpha)\n",
    "    elif method == 'linear_mix':\n",
    "        df = linear_mix(df1, df2, alpha)\n",
    "        print('linear mixing')\n",
    "    elif method == 'geometrix_mix':\n",
    "        df = geometric_mix(df1, df2, alpha)\n",
    "    elif method == 'amplitude_mix':\n",
    "        df = amplitude_mix(df1, df2, alpha)\n",
    "    elif method == 'proposed_mix':\n",
    "        df = proposed_mixup(df1, df2, alpha)\n",
    "\n",
    "    # Original\n",
    "    else:\n",
    "        df = df1.copy()\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(df[df.columns].values)\n",
    "        \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(scaled_features, rets, window_size)\n",
    "    \n",
    "    return X, y, df"
   ],
   "id": "163a1e1dd036e47e",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "def plot_TSNE(df1, df2):\n",
    "    df1_log = np.log(df1 + 1)  # Adding 1 to avoid log(0)\n",
    "    df2_log = np.log(df2 + 1)\n",
    "\n",
    "    combined_data = pd.concat([df1_log, df2_log])\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=100, n_iter=1000)\n",
    "    tsne_results = tsne.fit_transform(combined_data)\n",
    "\n",
    "    # Now we split the t-SNE results back into original and augmented parts\n",
    "    tsne_df1 = tsne_results[:len(df1), :]\n",
    "    tsne_df2 = tsne_results[len(df1):, :]\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.scatter(tsne_df1[:, 0], tsne_df1[:, 1], label='Original', alpha=0.8)\n",
    "    plt.scatter(tsne_df2[:, 0], tsne_df2[:, 1], label='Augmented', alpha=0.8)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "671e5c2e495a5f65",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Data from Yahoo Finance"
   ],
   "id": "171f9d5ec9849aa4"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "start_date = '2010-01-01'\n",
    "end_date = '2023-01-01'\n",
    "\n",
    "# Define the list of Dow Jones Industrial Average companies\n",
    "tickers = [\n",
    "    \"MMM\", \"AXP\", \"AMGN\", \"AAPL\", \"BA\", \"CAT\", \"CVX\", \"CSCO\", \"KO\", \"DIS\",\n",
    "    \"DOW\", \"GS\", \"HD\", \"HON\", \"IBM\", \"INTC\", \"JNJ\", \"JPM\", \"MCD\", \"MRK\",\n",
    "    \"MSFT\", \"NKE\", \"PG\", \"CRM\", \"TRV\", \"UNH\", \"V\", \"WBA\", \"WMT\"\n",
    "]\n",
    "\n",
    "# tickers = ['AAPL']\n",
    "# Create a dictionary to store historical data for each company\n",
    "historical_data = {}\n",
    "\n",
    "# Loop through the Dow companies and retrieve historical data\n",
    "for ticker in tickers:\n",
    "    stock_data = get_stock_data(ticker, start_date, end_date)\n",
    "    historical_data[ticker] = stock_data"
   ],
   "id": "a33932388a1c5ab2",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "source": [
    "historical_data"
   ],
   "id": "6518c0771e0e0a2b",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "# Create a DataFrame to hold the 'Close' prices of each stock\n",
    "close_prices = pd.DataFrame()\n",
    "\n",
    "# Extract 'Close' columns and merge them into the close_prices DataFrame\n",
    "for ticker, data in historical_data.items():\n",
    "    close_prices[ticker] = data['Close']\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = close_prices.corr()\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix for Closing Prices')\n",
    "plt.show()"
   ],
   "id": "1382c0395f7f5927",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'correlation_matrix' is your DataFrame containing the correlation coefficients\n",
    "\n",
    "# Define the low correlation threshold\n",
    "low_correlation_threshold = 0.6\n",
    "\n",
    "# Count the number of low correlations for each stock\n",
    "low_correlation_counts = (correlation_matrix < low_correlation_threshold).sum(axis=1)\n",
    "\n",
    "# Filter stocks that have low correlation with others more than a certain number of times\n",
    "# This threshold could be, for example, half the size of the correlation matrix\n",
    "threshold_num_low_correlations = len(correlation_matrix) // 2\n",
    "least_correlated_stocks = low_correlation_counts[low_correlation_counts > threshold_num_low_correlations].index\n",
    "\n",
    "# Now you have the tickers of the stocks that are least correlated with others\n",
    "# You can use this list to filter your original stock data\n",
    "\n",
    "# Assuming 'historical_data' is a dictionary with your stock data\n",
    "filtered_data = {ticker: historical_data[ticker] for ticker in least_correlated_stocks}\n",
    "\n",
    "# You can convert this dictionary back into a DataFrame if needed, for example:\n",
    "filtered_close_prices = pd.DataFrame({ticker: data['Close'] for ticker, data in filtered_data.items()})\n",
    "\n",
    "# Calculate the new correlation matrix for the filtered stocks\n",
    "filtered_correlation_matrix = filtered_close_prices.corr()\n",
    "\n",
    "# Plot the new correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(filtered_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Filtered Correlation Matrix for Least Correlated Closing Prices')\n",
    "plt.show()\n"
   ],
   "id": "484f4b5920240d3c",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple EDA"
   ],
   "id": "8fb3ad0ebeb74dab"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "plot_correlation(historical_data['AAPL'])"
   ],
   "id": "84cb8844792ac8e0",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "#     plot_correlation(historical_data['MSFT'])"
   ],
   "id": "61f10a9e3617b445",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - As our current features are all literally the same"
   ],
   "id": "5714096370f84972"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "historical_data_augmented = {}\n",
    "\n",
    "for key, value in historical_data.items():\n",
    "    df = historical_data[key]\n",
    "    new_df = engineer_features(df)\n",
    "    historical_data_augmented[key] = new_df"
   ],
   "id": "bc1ad07311055237",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "plot_correlation(historical_data_augmented['AAPL'])"
   ],
   "id": "a648d92c169931e8",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# plot_correlation(historical_data_augmented['MSFT'])"
   ],
   "id": "c3d6898241894e89",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original"
   ],
   "id": "2d7b805a1f50193d"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "def apply_lowess_smoothing(df, frac=0.1):\n",
    "    smoothed_data = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Apply LOWESS to each column\n",
    "    for column in df.columns:\n",
    "        smoothed_values = lowess(df[column], df.index, frac=frac, return_sorted=False)\n",
    "        smoothed_data[column] = smoothed_values\n",
    "    \n",
    "    return smoothed_data"
   ],
   "id": "bf2152a752204501",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess"
   ],
   "id": "bd83dda76b630a46",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "# Best hyperparameters\n",
    "best_params = {\n",
    "    'n_layers': 1, \n",
    "    'lstm_units': 50, \n",
    "    'dropout_rate': 0.5, \n",
    "    'learning_rate': 0.001, \n",
    "    'batch_size': 128,\n",
    "    'epochs': 50\n",
    "}\n",
    "\n",
    "def run_model(historical_data_augmented, best_params, method):\n",
    "    stock_rmse = {}\n",
    "    stock_augmented = {}\n",
    "    \n",
    "    for stock, df in historical_data_augmented.items():\n",
    "        df1 = df.copy(deep=True).dropna()\n",
    "        print(f'Processing stock: {stock}')\n",
    "        \n",
    "        if method == 'original':\n",
    "            df_to_augment = df1\n",
    "        else: \n",
    "            df_to_augment = apply_lowess_smoothing(df)\n",
    "\n",
    "        # Compute returns and drop NaN values\n",
    "        rets = df['Close'].pct_change().dropna()\n",
    "        \n",
    "        X, y, df_augmented = create_augmented_data(rets, df1, df_to_augment, method, alpha, seq_len)\n",
    "\n",
    "        # Train Test Split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Model creation\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "        model = create_model(best_params, input_shape)\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], verbose=0)\n",
    "\n",
    "        # Predictions\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        # Compute RMSE for the current stock\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "        stock_rmse[stock] = rmse\n",
    "        stock_augmented[stock] = df_augmented\n",
    "        print(f'Stock: {stock}, RMSE: {rmse}')\n",
    "\n",
    "        # Plotting t-SNE\n",
    "        plot_TSNE(df1, df_augmented)\n",
    "\n",
    "        return stock_rmse, df1, stock_augmented"
   ],
   "id": "7865fd9b4ce5793d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "rmse, df, df_to_augment = run_model(historical_data_augmented, best_params, 'cut_mix')"
   ],
   "id": "420675c5c487f51b",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "rmse, df, df_to_augment = run_model(historical_data, best_params, 'cut_mix')"
   ],
   "id": "a419fed5453f0883",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "df"
   ],
   "id": "29d3d35b5d17a47b",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "df_to_augment"
   ],
   "id": "6f96bda90cf7c4ee",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "plot_TSNE(df, df_to_augment['AAPL'])"
   ],
   "id": "1eb38d6c25e4300f",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "id": "3dc33a4ccf6a7ab7",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))  # Set seed for reproducibility\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print('Number of finished trials:', len(study.trials))\n",
    "# print('Best trial:', study.best_trial.params)"
   ],
   "id": "eba3e6ae7bd06afc",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
