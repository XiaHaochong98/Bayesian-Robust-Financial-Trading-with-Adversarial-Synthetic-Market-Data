{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Title: To determine if data augmentation using the method proposed in 'Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning' will lead to better 1 day prediction results.\n",
    "\n"
   ],
   "id": "d8b5c129cde4ddb3"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import random as python_random\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import yfinance as yf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from scipy.fft import rfft, rfftfreq, irfft\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils import resample\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from pykalman import KalmanFilter\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from IPython.display import display, HTML\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings \n",
    "\n",
    "# Display and warnings settings\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Optuna for hyperparameter tuning\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n"
   ],
   "id": "cfc189fac3c6a38b",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ],
   "id": "699d95215200dfcd"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# Seed value\n",
    "seed_value= 42\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)\n"
   ],
   "id": "1f757afa39ae0d6f",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# Define constants\n",
    "TIME_STEPS = 20\n",
    "alpha = 0.8\n",
    "seq_len = 20\n",
    "test_size = 0.3"
   ],
   "id": "2c76c8c23779a02",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ],
   "id": "766a03ca36e2312"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data and engineer them"
   ],
   "id": "f3ac568cb8c11b17"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# Function to import stock data\n",
    "def get_stock_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return data\n",
    "\n",
    "def z_score_normalize(series):\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    return (series - mean) / std\n",
    "\n",
    "def denormalize_z_score(normalized_series, original_mean, original_std):\n",
    "    return (normalized_series * original_std) + original_mean\n",
    "\n",
    "# Function to create model (make sure this is defined in your environment)\n",
    "def create_model(best_params, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(best_params['lstm_units'], input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(best_params['dropout_rate']))\n",
    "    model.add(LSTM(best_params['lstm_units']))  # Stacking LSTM for deep learning\n",
    "    model.add(Dropout(best_params['dropout_rate']))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mse')\n",
    "    return model\n",
    "\n",
    "def engineer_features(data):\n",
    "    df = data.copy(deep=True)\n",
    "    delta = df['Close'].diff()\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "    roll_up = up.rolling(window=14).mean()\n",
    "    roll_down = down.abs().rolling(window=14).mean()\n",
    "    RS = roll_up / roll_down\n",
    "    df['RSI'] = 100.0 - (100.0 / (1.0 + RS))\n",
    "\n",
    "    # Volume Weighted Average Price (VWAP)\n",
    "    vwap = (df['Volume'] * (df['High'] + df['Low'] + df['Close']) / 3).cumsum() / df['Volume'].cumsum()\n",
    "    df['VWAP'] = vwap\n",
    "\n",
    "    # Price Ratios\n",
    "    df['high_to_low_ratio'] = df['High'] / df['Low']\n",
    "    df['open_to_close_ratio'] = df['Open'] / df['Close']\n",
    "\n",
    "    # Volatility\n",
    "    df['volatility_10'] = df['Close'].rolling(window=10).std()\n",
    "\n",
    "    df1 = df.drop(columns=['Open', 'High', 'Low', 'Adj Close']).dropna()\n",
    "    return df1"
   ],
   "id": "abd964abc9b3b108",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create plots"
   ],
   "id": "9c043a90531b7be8"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "def plot_correlation(df):\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm',\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "    # Adjust the plot as needed\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()  # Adjusts the plot to ensure everything fits without overlap\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ],
   "id": "d2c4e34cf0abfe4e",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "def plot_TSNE(df1, df2):\n",
    "    df1.columns = df1.columns.astype(str)\n",
    "    df2.columns = df2.columns.astype(str)\n",
    "\n",
    "    df1_log = np.log(df1 + 1)  # Adding 1 to avoid log(0)\n",
    "    df2_log = np.log(df2 + 1)\n",
    "\n",
    "    combined_data = pd.concat([df1_log, df2_log])\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=100, n_iter=1000, init='pca')\n",
    "    tsne_results = tsne.fit_transform(combined_data)\n",
    "\n",
    "    # Now we split the t-SNE results back into original and augmented parts\n",
    "    tsne_df1 = tsne_results[:len(df1), :]\n",
    "    tsne_df2 = tsne_results[len(df1):, :]\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.scatter(tsne_df1[:, 0], tsne_df1[:, 1], label='Original', alpha=0.5)\n",
    "    plt.scatter(tsne_df2[:, 0], tsne_df2[:, 1], label='Augmented', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "7669936418d407c7",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests for augmented datasets"
   ],
   "id": "3c27a810c9657663"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "def calculate_entropy(variable):\n",
    "    value,counts = np.unique(variable, return_counts=True)\n",
    "    return entropy(counts, base=2)\n",
    "\n",
    "# Function to calculate normalized mutual information\n",
    "def calculate_normalized_mi(variable_1, variable_2):\n",
    "    mi = mutual_info_score(variable_1, variable_2)\n",
    "    entropy_1 = calculate_entropy(variable_1)\n",
    "    entropy_2 = calculate_entropy(variable_2)\n",
    "    # Normalizing by the average entropy\n",
    "    normalized_mi = mi / ((entropy_1 + entropy_2) / 2)\n",
    "    return normalized_mi\n",
    "\n",
    "def calculate_MI(original, augmented):\n",
    "# Assuming df_original and df_augmented are your dataframes\n",
    "    for column in original.columns:\n",
    "        # Ensure the data is in the correct format, e.g., continuous or discrete\n",
    "        # For continuous variables, you'd typically bin them before calculating mutual information\n",
    "        original_data = original[column].to_numpy()\n",
    "        augmented_data = augmented[column].to_numpy()\n",
    "\n",
    "        # Calculate normalized MI for each column\n",
    "        normalized_mi = calculate_normalized_mi(original_data, augmented_data)\n",
    "        print(f'Normalized Mutual Information for {column}: {normalized_mi}')\n"
   ],
   "id": "f212d6be2b25c64d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "def find_cointegrated_pairs(data):\n",
    "    n = data.shape[1] # Number of columns in dataset\n",
    "    score_matrix = np.zeros((n,n))\n",
    "    pvalue_matrix = np.ones((n,n))\n",
    "    keys = data.keys()\n",
    "    pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            S1 = data[keys[i]]\n",
    "            S2 = data[keys[j]]\n",
    "            result = coint(S1,S2)\n",
    "            score = result[0]\n",
    "            pvalue = result[1]\n",
    "            score_matrix[i,j] = score\n",
    "            pvalue_matrix[i,j] = pvalue\n",
    "            if pvalue < 0.05:\n",
    "                pairs.append((keys[i],keys[j]))\n",
    "    return score_matrix, pvalue_matrix, pairs"
   ],
   "id": "ff3c70a0c35890a9",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation functions"
   ],
   "id": "82452de96c4c2502"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "def apply_lowess_smoothing(df, frac=0.1):\n",
    "    smoothed_data = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Apply LOWESS to each column\n",
    "    for column in df.columns:\n",
    "        smoothed_values = lowess(df[column], df.index, frac=frac, return_sorted=False)\n",
    "        smoothed_data[column] = smoothed_values\n",
    "    \n",
    "    return smoothed_data"
   ],
   "id": "cde72db09ef0435f",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "def cut_mix(df1, df2, alpha):\n",
    "    np.random.seed(42)  # Set seed only once externally if needed for reproducibility\n",
    "    assert df1.shape == df2.shape\n",
    "    size = len(df1)\n",
    "    cut_length = int(size * alpha)\n",
    "    cut_point = np.random.randint(0, size - cut_length)  # Ensure slicing does not exceed the size\n",
    "    \n",
    "    mixed_df = df1.copy()\n",
    "    mixed_df.iloc[cut_point:cut_point + cut_length] = df2.iloc[cut_point:cut_point + cut_length]\n",
    "    \n",
    "    return mixed_df\n",
    "\n",
    "def binary_mix(data1, data2, alpha=alpha):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    assert len(data1) == len(data2)\n",
    "    size = data1.shape\n",
    "    mask = np.random.binomial(1, alpha, size=size).astype(bool)\n",
    "    \n",
    "    mixed_data = np.where(mask, data1, data2)\n",
    "    \n",
    "    return pd.DataFrame(mixed_data, columns=data1.columns)\n",
    "\n",
    "def linear_mix(data1, data2, alpha=alpha):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    mixed_data = alpha * data1 + (1 - alpha) * data2\n",
    "    \n",
    "    return mixed_data\n",
    "\n",
    "def geometric_mix(data1, data2, alpha=alpha):\n",
    "    if len(data1) != len(data2):\n",
    "        raise ValueError(\"The lengths of data1 and data2 must be the same.\")\n",
    "        \n",
    "    # Replace zeros and negative values to avoid NaNs or complex numbers\n",
    "    data1_clipped = np.clip(data1, a_min=1e-10, a_max=None)\n",
    "    data2_clipped = np.clip(data2, a_min=1e-10, a_max=None)\n",
    "    \n",
    "    mixed_data = np.power(data1_clipped, alpha) * np.power(data2_clipped, (1 - alpha))\n",
    "    \n",
    "    return mixed_data\n",
    "def amplitude_mix(data1, data2, alpha=alpha):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    # Apply Fourier Transform to each column\n",
    "    fft1 = np.fft.rfft(data1, axis=0)\n",
    "    fft2 = np.fft.rfft(data2, axis=0)\n",
    "    \n",
    "    # Mix the magnitudes\n",
    "    magnitude1 = np.abs(fft1)\n",
    "    magnitude2 = np.abs(fft2)\n",
    "    mixed_magnitude = alpha * magnitude1 + (1 - alpha) * magnitude2\n",
    "    \n",
    "    # Keep the phase of the first data\n",
    "    phase1 = np.angle(fft1)\n",
    "    mixed_fft = mixed_magnitude * np.exp(1j * phase1)\n",
    "    \n",
    "    # Perform the inverse FFT and ensure the result is two-dimensional\n",
    "    mixed_data = np.fft.irfft(mixed_fft, axis=0)\n",
    "    if mixed_data.ndim == 1:\n",
    "        mixed_data = mixed_data.reshape(-1, 1)  # Reshape if the data is one-dimensional\n",
    "    \n",
    "    # Return a DataFrame with the same column names as data1\n",
    "    return pd.DataFrame(mixed_data, columns=data1.columns)\n",
    "\n",
    "\n",
    "### PROPOSE TECHNIQUE BELOW\n",
    "def proposed_mixup(df1, df2, threshold=0.1, alpha=alpha):\n",
    "    \n",
    "    def proposed_mixup_feature(data1, data2, threshold, alpha):\n",
    "        \n",
    "        def get_significant_frequencies(data, threshold):\n",
    "            \"\"\"\n",
    "            Perform Fourier Transform on data and identify frequencies with significant amplitude.\n",
    "\n",
    "            Args:\n",
    "            - data: Time series data.\n",
    "            - threshold: Threshold for significance, relative to the max amplitude.\n",
    "\n",
    "            Returns:\n",
    "            - significant_freq: Frequencies with significant amplitude.\n",
    "            - significant_ampl: Amplitude of the significant frequencies.\n",
    "            - full_spectrum: Full Fourier spectrum for all frequencies.\n",
    "            \"\"\"\n",
    "            # Perform Fourier Transform\n",
    "            spectrum = rfft(data)\n",
    "            frequencies = rfftfreq(data.size, d=1)  # Assuming unit time interval between samples\n",
    "\n",
    "            # Find significant amplitudes\n",
    "            amplitude = np.abs(spectrum)\n",
    "            significant_indices = amplitude > (amplitude.max() * threshold)\n",
    "            significant_freq = frequencies[significant_indices]\n",
    "            significant_ampl = amplitude[significant_indices]\n",
    "\n",
    "            return significant_freq, significant_ampl, spectrum\n",
    "\n",
    "        def phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2, alpha):\n",
    "            mixed_spectrum = np.copy(spectrum1)\n",
    "            freqs1 = rfftfreq(spectrum1.size, d=1)\n",
    "            freqs2 = rfftfreq(spectrum2.size, d=1)\n",
    "\n",
    "            for freq in sig_freq1:\n",
    "                index1 = np.argmin(np.abs(freqs1 - freq))\n",
    "                index2 = np.argmin(np.abs(freqs2 - freq))\n",
    "\n",
    "                if index1 >= len(sig_ampl1) or index2 >= len(sig_ampl2):\n",
    "                    continue  # Skip the frequency if the index is out of bounds\n",
    "\n",
    "                phase1 = np.angle(spectrum1[index1])\n",
    "                phase2 = np.angle(spectrum2[index2])\n",
    "\n",
    "                phase_diff = (phase2 - phase1) % (2 * np.pi)\n",
    "                phase_diff = phase_diff - 2 * np.pi if phase_diff > np.pi else phase_diff\n",
    "\n",
    "                new_amplitude = alpha * sig_ampl1[index1] + (1 - alpha) * sig_ampl2[index2]\n",
    "                new_phase = phase1 + alpha * phase_diff\n",
    "\n",
    "                mixed_spectrum[index1] = new_amplitude * np.exp(1j * new_phase)\n",
    "\n",
    "            return mixed_spectrum\n",
    "\n",
    "\n",
    "        def reconstruct_time_series(mixed_spectrum):\n",
    "            \"\"\"\n",
    "            Reconstruct time series from mixed spectrum using inverse Fourier Transform.\n",
    "\n",
    "            Returns:\n",
    "            - mixed_time_series: The reconstructed time series.\n",
    "            \"\"\"\n",
    "            # Perform inverse Fourier Transform\n",
    "            mixed_time_series = irfft(mixed_spectrum)\n",
    "\n",
    "            return mixed_time_series\n",
    "\n",
    "        # Step 1: Get significant frequencies and amplitude for both time series\n",
    "        sig_freq1, sig_ampl1, spectrum1 = get_significant_frequencies(data1, threshold)\n",
    "        sig_freq2, sig_ampl2, spectrum2 = get_significant_frequencies(data2, threshold)\n",
    "\n",
    "        # Step 2: Identify significant frequencies (already done in step 1)\n",
    "\n",
    "        # Step 3: Phase and Magnitude Mixup\n",
    "        mixed_spectrum = phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2, alpha)\n",
    "\n",
    "        # Step 4: Reconstruction of the time series\n",
    "        mixed_time_series = reconstruct_time_series(mixed_spectrum)\n",
    "        return mixed_time_series\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    \n",
    "    for feature in df1.columns:\n",
    "        output_df[feature] = proposed_mixup_feature(df1[feature].values, df2[feature].values, threshold, alpha)\n",
    "        \n",
    "    return output_df"
   ],
   "id": "a83240806185e69",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "from sklearn.utils import resample\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "f54ae9046a487448",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "def jittering(ts, noise_level=0.05):\n",
    "    np.random.seed(42)\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=len(ts))\n",
    "    return pd.Series(ts + noise)\n",
    "\n",
    "def flipping(ts):\n",
    "    return pd.Series(np.flip(ts))\n",
    "\n",
    "def scaling(ts, scaling_factor=1.5):\n",
    "    return pd.Series(ts * scaling_factor)\n",
    "\n",
    "def magnitude_warping(ts, sigma=0.2, knot=4):\n",
    "    np.random.seed(42)\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, ))\n",
    "    indices = np.linspace(0, len(ts)-1, num=knot+2)\n",
    "    sp = CubicSpline(indices, random_warps)\n",
    "    warp_values = sp(np.arange(len(ts)))\n",
    "    return pd.Series(ts * warp_values)\n",
    "\n",
    "def permutation(ts, n_segments=5):\n",
    "    np.random.seed(42)\n",
    "    permutated_ts = np.copy(ts)\n",
    "    segments = np.array_split(permutated_ts, n_segments)\n",
    "    np.random.shuffle(segments)\n",
    "    return pd.Series(np.concatenate(segments))\n",
    "\n",
    "def time_warping(ts, sigma=0.2, knot=4):\n",
    "    np.random.seed(42)\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    time_steps = np.arange(ts.shape[0])\n",
    "    random_steps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, ts.shape[1]))\n",
    "    indices = np.linspace(0, len(ts)-1, num=knot+2)\n",
    "    sp = CubicSpline(indices, random_steps)\n",
    "    warp_values = sp(time_steps)\n",
    "    return pd.Series(warp_values * ts)\n",
    "\n",
    "def stl_augment(data, period=61):\n",
    "    ts = data.asfreq('B')\n",
    "    ts = ts.interpolate()\n",
    "    # Apply STL decomposition\n",
    "    stl = STL(ts, seasonal=period) \n",
    "    result = stl.fit()\n",
    "    seasonal, trend, remainder = result.seasonal, result.trend, result.resid\n",
    "    bootstrapped_remainder = resample(remainder, replace=True, n_samples=len(remainder), random_state=42)\n",
    "    bootstrapped_remainder.index = ts.index\n",
    "    augmented_signal = trend + seasonal + bootstrapped_remainder\n",
    "    augmented_signal = np.maximum(augmented_signal, 0)\n",
    "    augmented_signal = augmented_signal[data.index]\n",
    "    return augmented_signal\n",
    "\n",
    "# Function to plot original and augmented series\n",
    "def plot_augmented_ts(original_ts, augmented_ts, title='Time Series Augmentation'):\n",
    "    augmented_ts.index = original_ts.index\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(original_ts, label='Original')\n",
    "    plt.plot(augmented_ts, label='Augmented')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "2325f9a610e21a6e",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecasting"
   ],
   "id": "112c58fbadb0ea4f"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "def augment_dataframe(stock_df, method):\n",
    "    df = stock_df.copy()\n",
    "    augmented_stockdf = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        val = aapl[col]\n",
    "        if method == 'jittering':\n",
    "            aug_val = jittering(val)\n",
    "        elif method == 'flipping':\n",
    "            aug_val = flipping(val)\n",
    "        elif method == 'scaling':\n",
    "            aug_val = scaling(val)\n",
    "        elif method == 'magnitude_warping':\n",
    "            aug_val = magnitude_warping(val)\n",
    "        elif method == 'permutation':\n",
    "            aug_val = permutation(val)\n",
    "        elif method == 'stl_augment':\n",
    "            aug_val = stl_augment(val)\n",
    "\n",
    "        augmented_stockdf[col] = aug_val\n",
    "    return augmented_stockdf"
   ],
   "id": "51d1001dbdae24e9",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "def create_augmented_data(df1_, df2_, method, alpha=alpha):\n",
    "    df1 = df1_.copy()\n",
    "    df2 = df2_.copy()\n",
    "    \n",
    "    if method == 'cut_mix':\n",
    "        df = cut_mix(df1, df2, alpha)\n",
    "    elif method == 'binary_mix':\n",
    "        df = binary_mix(df1, df2, alpha)\n",
    "    elif method == 'linear_mix':\n",
    "        df = linear_mix(df1, df2, alpha)\n",
    "    elif method == 'geometrix_mix':\n",
    "        df = geometric_mix(df1, df2, alpha)\n",
    "    elif method == 'amplitude_mix':\n",
    "        df = amplitude_mix(df1, df2, alpha)\n",
    "    elif method == 'proposed_mix':\n",
    "        df = proposed_mixup(df1, df2, alpha)\n",
    "\n",
    "    # Original\n",
    "    else:\n",
    "        df = df1.copy()\n",
    "        \n",
    "    return df"
   ],
   "id": "864064400a85fd97",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "# Define the LSTM model creation function\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=input_shape),\n",
    "        LSTM(50),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def create_sequences(features, target, time_steps):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(features) - time_steps):\n",
    "        Xs.append(features[i:(i + time_steps)])\n",
    "        ys.append(target[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Train the LSTM model and return it along with scalers and the test set\n",
    "def train_evaluate_lstm(features, target, time_steps, epochs, batch_size):\n",
    "    X, y = create_sequences(features, target, time_steps)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    model = create_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "#     test_predictions = model.predict(X_test)\n",
    "#     test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "#     print(f\"Test RMSE: {test_rmse}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Predict on new data using the trained model and calculate prediction intervals\n",
    "def predict_new_data(model, new_data, feature_scaler, target_scaler, quantile, time_steps):\n",
    "    new_features_scaled = feature_scaler.transform(new_data)\n",
    "    X_new, _ = create_sequences(new_features_scaled, np.zeros((len(new_features_scaled), new_data.shape[1])), time_steps)\n",
    "    predictions = model.predict(X_new)\n",
    "    return predictions"
   ],
   "id": "3d52118fd0ea2340",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "def classification_accuracy(df_, features_list, X_test_og, y_test_og, scaler_aapl):\n",
    "    np.random.seed(seed_value)\n",
    "    python_random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    # df should be a dataframe which contains all the features and Close (no Return column)\n",
    "    df = df_.copy()\n",
    "    # df should be a dataframe which contains all the features and Close (no Return column)\n",
    "    df['Return'] = np.log(df['Close']).diff()\n",
    "    df.dropna(subset=['Return'], inplace=True)\n",
    "    features = df[features_list]\n",
    "    target = df['Return']\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_features = scaler.fit_transform(features.values)\n",
    "    scaled_target = scaler.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "    # Create sequences\n",
    "    time_steps = 20  # Number of time steps for LSTM\n",
    "    X, y = create_sequences(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(1))  # Prediction of the next closing price\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=128, verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    predicted_returns = model.predict(X_test_og)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_og, predicted_returns))\n",
    "    print('Test RMSE: ', rmse)\n",
    "\n",
    "    # Invert scaling to compare predictions against the actual returns\n",
    "    predicted_returns = scaler.inverse_transform(predicted_returns)\n",
    "\n",
    "    binary_predicted = (predicted_returns > 0).astype(int)\n",
    "\n",
    "    # Do the same for actual returns\n",
    "    binary_actual = (scaler_aapl.inverse_transform(y_test_og) > 0).astype(int)\n",
    "\n",
    "    # Calculate the proportion of correct directional predictions\n",
    "    directional_accuracy = np.mean(binary_predicted == binary_actual)\n",
    "    print(f'Directional Accuracy: {directional_accuracy * 100:.2f}%') \n",
    "    \n",
    "    return directional_accuracy, predicted_returns"
   ],
   "id": "a9be507576f06874",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Data from Yahoo Finance"
   ],
   "id": "489985b4fe383bcf"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "start_date = '2010-01-01'\n",
    "end_date = '2023-01-01'\n",
    "\n",
    "# Define the list of Dow Jones Industrial Average companies\n",
    "tickers = [\n",
    "    \"MMM\", \"AXP\", \"AMGN\", \"AAPL\", \"BA\", \"CAT\", \"CVX\", \"CSCO\", \"KO\", \"DIS\"\n",
    "    , \"GS\", \"HD\", \"HON\", \"IBM\", \"INTC\", \"JNJ\", \"JPM\", \"MCD\", \"MRK\",\n",
    "    \"MSFT\", \"NKE\", \"PG\", \"CRM\", \"TRV\", \"UNH\", \"V\", \"WBA\", \"WMT\"\n",
    "]\n",
    "\n",
    "# tickers = ['AAPL']\n",
    "# Create a dictionary to store historical data for each company\n",
    "historical_data = {}\n",
    "\n",
    "# Loop through the Dow companies and retrieve historical data\n",
    "for ticker in tickers:\n",
    "    stock_data = get_stock_data(ticker, start_date, end_date)\n",
    "    historical_data[ticker] = stock_data"
   ],
   "id": "743ef61e023b580b",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best way to concat augmented data to the original training set"
   ],
   "id": "ffe6d7592d7d10b2"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for stock, data in historical_data.items():\n",
    "    df[stock] = data['Adj Close']\n",
    "    \n",
    "display(df)"
   ],
   "id": "dd5239e55061b66c",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "aapl_df = historical_data['AAPL']\n",
    "aapl_df = engineer_features(aapl_df)\n",
    "aapl_df"
   ],
   "id": "152218f396eb57c0",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MA"
   ],
   "id": "60231a5e1bf1fe83"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONLY augmented data is used"
   ],
   "id": "b012a86532706334"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "aapl = aapl_df.copy()\n",
    "lowess = apply_lowess_smoothing(aapl)\n",
    "cut_lowess = create_augmented_data(aapl, lowess, method='cut_mix')\n",
    "binary_lowess = create_augmented_data(aapl, lowess, method='binary_mix')\n",
    "linear_lowess = create_augmented_data(aapl, lowess, method='linear_mix')\n",
    "geometric_lowess = create_augmented_data(aapl, lowess, method='geometrix_mix')\n",
    "amplitude_lowess = create_augmented_data(aapl, lowess, method='amplitude_mix')\n",
    "proposedmix_lowess = create_augmented_data(aapl, lowess, method='proposed_mix')\n",
    "\n",
    "jittered_ts = augment_dataframe(aapl, 'jittering')\n",
    "flipped_ts = augment_dataframe(aapl, 'flipping')\n",
    "scaled_ts = augment_dataframe(aapl, 'scaling')\n",
    "mag_warped_ts = augment_dataframe(aapl, 'magnitude_warping')\n",
    "permuted_ts = augment_dataframe(aapl, 'permutation')\n",
    "stl_ts = augment_dataframe(aapl, 'stl_augment')\n",
    "\n",
    "augmented_datasets = {\n",
    "    'original': aapl,\n",
    "    'cut_mix': cut_lowess,\n",
    "    'linear_mix': linear_lowess,\n",
    "    'geometric_mix': geometric_lowess,\n",
    "    'amplitude_mix': amplitude_lowess,\n",
    "    'proposed_mix': proposedmix_lowess,\n",
    "    'jittering': jittered_ts,\n",
    "    'scaling': scaled_ts,\n",
    "    'magnitude_warping': mag_warped_ts,\n",
    "    'permutation': permuted_ts,\n",
    "    'stl_augment': stl_ts\n",
    "}"
   ],
   "id": "517f000723d15525",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def calculate_dtw_distance(df1, df2, feature_list):\n",
    "    # Calculate DTW distance for each common feature\n",
    "    dtw_distances = {}\n",
    "    i = 0\n",
    "    for col in df1.columns:\n",
    "        series1 = df1[col].values\n",
    "        series2 = df2[col].values\n",
    "        distance, path = fastdtw(series1.reshape(-1,1), series2.reshape(-1,1), dist=euclidean)\n",
    "        dtw_distances[feature_list[i]] = round(distance,2)\n",
    "        i += 1\n",
    "    return dtw_distances"
   ],
   "id": "81dc805720c926f3",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "def select_augmented_data(original_df, augmented_df, method, condition=None, short_window=10, long_window=50, alpha=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Select augmented data based on a specified method and condition.\n",
    "\n",
    "    Parameters:\n",
    "    - original_df: DataFrame with the original data.\n",
    "    - augmented_df: DataFrame with the augmented data.\n",
    "    - method: String indicating the method to use for selection ('ma_condition', 'random', etc.).\n",
    "    - condition: Condition function that takes the DataFrame and returns a boolean mask for selection.\n",
    "    - alpha: Proportion of the augmented data to add if method is 'random'.\n",
    "    - random_state: Seed for the random number generator.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the original and selected augmented data.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)  # For reproducibility\n",
    "    \n",
    "    if method == 'ma_condition' and condition is not None:\n",
    "        # Apply the condition to select rows from augmented_df\n",
    "        mask = condition(augmented_df)\n",
    "        selected_data = augmented_df[mask]\n",
    "    \n",
    "    elif method == 'random':\n",
    "        # Randomly select a portion of the augmented data\n",
    "        n_select = int(len(augmented_df) * alpha)\n",
    "        selected_data = augmented_df.sample(n=n_select, random_state=random_state)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "    combined_df = pd.concat([original_df, selected_data]).reset_index(drop=True)\n",
    "\n",
    "    df = original_df.copy()\n",
    "    df['Return'] = np.log(df['Close']).diff()\n",
    "    df.dropna(subset=['Return'], inplace=True)\n",
    "    features = df[features_list]\n",
    "    target = df['Return']\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_features = scaler.fit_transform(features.values)\n",
    "    scaled_target = scaler.fit_transform(target.values.reshape(-1, 1))\n",
    "    \n",
    "    # Create sequences\n",
    "    time_steps = 20  # Number of time steps for LSTM\n",
    "    original_sequences, original_targets = create_sequences(scaled_features, scaled_target, time_steps)\n",
    "    \n",
    "    # Split the original sequences into training and validation sets\n",
    "    X_train_orig, X_test, y_train_orig, y_test = train_test_split(\n",
    "        original_sequences, original_targets, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    selected_data_copy = selected_data.copy()\n",
    "    selected_data_copy['Return'] = np.log(selected_data_copy['Close']).diff()\n",
    "    selected_data_copy.dropna(subset=['Return'], inplace=True)\n",
    "    features_aug = selected_data_copy[features_list]\n",
    "    target_aug = selected_data_copy['Return']\n",
    "\n",
    "#     print(\"DTW Scores:\")\n",
    "#     for feature, score in dtw_distance.items():\n",
    "#         print(f\"{feature}: {score:.4f}\")\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler_aug = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_features_aug = scaler_aug.fit_transform(features_aug.values)\n",
    "    scaled_target_aug = scaler_aug.fit_transform(target_aug.values.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate mutual information scores between augmented features and original target\n",
    "    dtw_distance = calculate_dtw_distance(pd.DataFrame(scaled_features_aug), pd.DataFrame(scaled_features), features_list)\n",
    "    \n",
    "    # Create sequences for augmented data\n",
    "    augmented_sequences, augmented_targets = create_sequences(scaled_features_aug, scaled_target_aug, time_steps)\n",
    "    \n",
    "    # Combine the original training sequences with the augmented sequences\n",
    "    X_train_combined = np.concatenate((X_train_orig, augmented_sequences), axis=0)\n",
    "    y_train_combined = np.concatenate((y_train_orig, augmented_targets), axis=0)\n",
    "\n",
    "    # Shuffle the combined training sequences to ensure random distribution\n",
    "    p = np.random.permutation(len(X_train_combined))\n",
    "    X_train = X_train_combined[p]\n",
    "    y_train = y_train_combined[p]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler, dtw_distance\n",
    "\n",
    "# Example condition function for moving average criteria\n",
    "def ma_condition(df, short_window=20, long_window=240):\n",
    "    ma_short = df['Close'].rolling(window=short_window).mean()\n",
    "    ma_long = df['Close'].rolling(window=long_window).mean()\n",
    "    return ma_short > ma_long"
   ],
   "id": "5b2a62cfddd3b62c",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "source": [
    "def classification_accuracy2(X_train, y_train, X_test, y_test, scaler):\n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(1))  # Prediction of the next closing price\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    predicted_returns = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predicted_returns))\n",
    "    print('Test RMSE: ', rmse)\n",
    "\n",
    "    # Invert scaling to compare predictions against the actual returns\n",
    "    predicted_returns = scaler.inverse_transform(predicted_returns)\n",
    "\n",
    "    binary_predicted = (predicted_returns > 0).astype(int)\n",
    "\n",
    "    # Do the same for actual returns\n",
    "    binary_actual = (scaler.inverse_transform(y_test) > 0).astype(int)\n",
    "\n",
    "    # Calculate the proportion of correct directional predictions\n",
    "    directional_accuracy = np.mean(binary_predicted == binary_actual)\n",
    "    print(f'Directional Accuracy: {directional_accuracy * 100:.2f}%') \n",
    "    \n",
    "    return directional_accuracy, predicted_returns"
   ],
   "id": "54087a86aadb81a",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def generate_ma_pairs(short_ma_range, long_ma_range, step=10):\n",
    "    \"\"\"\n",
    "    Generate tuples of (short_ma, long_ma) where short_ma < long_ma with a specified step.\n",
    "\n",
    "    Parameters:\n",
    "    - short_ma_range: Tuple specifying the start and end of the range for short moving averages.\n",
    "    - long_ma_range: Tuple specifying the start and end of the range for long moving averages.\n",
    "    - step: The increment between each moving average in the range.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples with (short_ma, long_ma).\n",
    "    \"\"\"\n",
    "    ma_pairs = []\n",
    "    for short_ma in range(short_ma_range[0], short_ma_range[1] + 1, step):\n",
    "        for long_ma in range(long_ma_range[0], long_ma_range[1] + 1, step):\n",
    "            if short_ma < long_ma:\n",
    "                ma_pairs.append((short_ma, long_ma))\n",
    "    return ma_pairs\n",
    "\n",
    "# Example usage:\n",
    "short_ma_range = (10, 50)  # Start and end range for short moving averages\n",
    "long_ma_range = (60, 200)  # Start and end range for long moving averages\n",
    "step = 10  # The increment step\n",
    "ma_pairs = generate_ma_pairs(short_ma_range, long_ma_range, step)\n",
    "\n",
    "print(ma_pairs)\n"
   ],
   "id": "c23785055c3a148f",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "source": [
    "features_list = ['Close', 'Volume', 'RSI', 'VWAP', 'high_to_low_ratio', 'open_to_close_ratio', 'volatility_10']\n",
    "data = augmented_datasets.copy()"
   ],
   "id": "623a72026d21a4a2",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "results = {}\n",
    "# Original data as the baseline\n",
    "original = data['original']\n",
    "\n",
    "# Iterate over the augmentation methods\n",
    "for method, dataset in data.items():\n",
    "    if method == 'original':\n",
    "        continue  # Skip the original dataset\n",
    "    \n",
    "    # Initialize a nested dictionary for each method\n",
    "    results[method] = {}\n",
    "    \n",
    "    # Iterate over the moving average pairs\n",
    "    for pairs in ma_pairs:\n",
    "        short_window, long_window = pairs\n",
    "        \n",
    "        # Apply the condition to select augmented data\n",
    "        X_train, y_train, X_test, y_test, scaler, dtw_distance_dict = select_augmented_data(\n",
    "            original, \n",
    "            dataset, \n",
    "            method='ma_condition', \n",
    "            condition=ma_condition,\n",
    "            short_window=short_window, \n",
    "            long_window=long_window, \n",
    "            alpha=0.4,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Calculate the classification accuracy\n",
    "        direction_accuracy, predicted_returns = classification_accuracy2(X_train, y_train, X_test, y_test, scaler)\n",
    "        \n",
    "        # Store the results indexed by the method and MA pair\n",
    "        ma_pair_key = f'MA_{short_window}_{long_window}'  # Unique key for each MA pair\n",
    "        results[method][ma_pair_key] = {\n",
    "            'dtw_distance': dtw_distance_dict,\n",
    "            'direction_accuracy': round(direction_accuracy * 100, 2),\n",
    "        }"
   ],
   "id": "39f07de6028679c3",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# To store the top 3 results for each augmentation method\n",
    "top_results_per_method = {}\n",
    "\n",
    "for method, ma_results in results.items():\n",
    "    # Sort the MA pairs based on direction accuracy in descending order\n",
    "    sorted_ma_results = sorted(ma_results.items(), key=lambda x: x[1]['direction_accuracy'], reverse=True)\n",
    "    \n",
    "    # Select the top 3 results\n",
    "    top_3_ma_pairs = sorted_ma_results[:3]\n",
    "    \n",
    "    # Store in the dictionary\n",
    "    top_results_per_method[method] = top_3_ma_pairs\n",
    "\n",
    "# Print the top 3 results for each augmentation method\n",
    "for method, top_results in top_results_per_method.items():\n",
    "    print(f\"Top 3 MA pairs for {method}:\")\n",
    "    for ma_pair, result in top_results:\n",
    "        direction_accuracy = result['direction_accuracy']\n",
    "        print(f\"  {ma_pair}: Direction Accuracy = {direction_accuracy}%\")\n",
    "    print(\"\\n\")\n"
   ],
   "id": "b1767f032bc68300",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
