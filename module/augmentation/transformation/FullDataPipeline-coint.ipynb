{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ef451e",
   "metadata": {},
   "source": [
    "## Project Title: To determine if data augmentation using the method proposed in 'Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning' will lead to better 1 day prediction results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df0a2bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:37.926785Z",
     "start_time": "2024-05-06T08:00:37.911932Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import random as python_random\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import yfinance as yf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from scipy.fft import rfft, rfftfreq, irfft\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils import resample\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from pykalman import KalmanFilter\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from IPython.display import display, HTML\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings \n",
    "\n",
    "\n",
    "# Display and warnings settings\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Optuna for hyperparameter tuning\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3add606e",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "167fda41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.063665Z",
     "start_time": "2024-05-06T08:00:38.050124Z"
    }
   },
   "source": [
    "# Seed value\n",
    "seed_value= 42\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff4f5076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.064693Z",
     "start_time": "2024-05-06T08:00:38.064598Z"
    }
   },
   "source": [
    "# Define constants\n",
    "TIME_STEPS = 20\n",
    "alpha = 0.8\n",
    "seq_len = 20\n",
    "test_size = 0.3"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "15f8e6d1",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ff3ac3",
   "metadata": {},
   "source": [
    "#### Get data and engineer them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f208b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.110850Z",
     "start_time": "2024-05-06T08:00:38.103721Z"
    }
   },
   "source": [
    "# Function to import stock data\n",
    "def get_stock_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return data\n",
    "\n",
    "def z_score_normalize(series):\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    return (series - mean) / std\n",
    "\n",
    "def denormalize_z_score(normalized_series, original_mean, original_std):\n",
    "    return (normalized_series * original_std) + original_mean\n",
    "\n",
    "# Function to create model (make sure this is defined in your environment)\n",
    "def create_model(best_params, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(best_params['lstm_units'], input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(best_params['dropout_rate']))\n",
    "    model.add(LSTM(best_params['lstm_units']))  # Stacking LSTM for deep learning\n",
    "    model.add(Dropout(best_params['dropout_rate']))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mse')\n",
    "    return model\n",
    "\n",
    "def engineer_features(data):\n",
    "    df = data.copy(deep=True)\n",
    "    delta = df['Close'].diff()\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "    roll_up = up.rolling(window=14).mean()\n",
    "    roll_down = down.abs().rolling(window=14).mean()\n",
    "    RS = roll_up / roll_down\n",
    "    df['RSI'] = 100.0 - (100.0 / (1.0 + RS))\n",
    "\n",
    "    # Volume Weighted Average Price (VWAP)\n",
    "    vwap = (df['Volume'] * (df['High'] + df['Low'] + df['Close']) / 3).cumsum() / df['Volume'].cumsum()\n",
    "    df['VWAP'] = vwap\n",
    "\n",
    "    # Price Ratios\n",
    "    df['high_to_low_ratio'] = df['High'] / df['Low']\n",
    "    df['open_to_close_ratio'] = df['Open'] / df['Close']\n",
    "\n",
    "    # Volatility\n",
    "    df['volatility_10'] = df['Close'].rolling(window=10).std()\n",
    "\n",
    "    df1 = df.drop(columns=['Open', 'High', 'Low', 'Adj Close']).dropna()\n",
    "    return df1"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2ccc590e",
   "metadata": {},
   "source": [
    "#### Create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81f25f9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.228315Z",
     "start_time": "2024-05-06T08:00:38.224829Z"
    }
   },
   "source": [
    "def plot_correlation(df):\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm',\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "    # Adjust the plot as needed\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()  # Adjusts the plot to ensure everything fits without overlap\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe69172",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.239628Z",
     "start_time": "2024-05-06T08:00:38.235493Z"
    }
   },
   "source": [
    "def plot_TSNE(df1, df2):\n",
    "    df1.columns = df1.columns.astype(str)\n",
    "    df2.columns = df2.columns.astype(str)\n",
    "\n",
    "    df1_log = np.log(df1 + 1)  # Adding 1 to avoid log(0)\n",
    "    df2_log = np.log(df2 + 1)\n",
    "\n",
    "    combined_data = pd.concat([df1_log, df2_log])\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=100, n_iter=1000, init='pca')\n",
    "    tsne_results = tsne.fit_transform(combined_data)\n",
    "\n",
    "    # Now we split the t-SNE results back into original and augmented parts\n",
    "    tsne_df1 = tsne_results[:len(df1), :]\n",
    "    tsne_df2 = tsne_results[len(df1):, :]\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.scatter(tsne_df1[:, 0], tsne_df1[:, 1], label='Original', alpha=0.5)\n",
    "    plt.scatter(tsne_df2[:, 0], tsne_df2[:, 1], label='Augmented', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8a8107e2",
   "metadata": {},
   "source": [
    "#### Tests for augmented datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e03afaf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.252023Z",
     "start_time": "2024-05-06T08:00:38.248295Z"
    }
   },
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "def calculate_entropy(variable):\n",
    "    value,counts = np.unique(variable, return_counts=True)\n",
    "    return entropy(counts, base=2)\n",
    "\n",
    "# Function to calculate normalized mutual information\n",
    "def calculate_normalized_mi(variable_1, variable_2):\n",
    "    mi = mutual_info_score(variable_1, variable_2)\n",
    "    entropy_1 = calculate_entropy(variable_1)\n",
    "    entropy_2 = calculate_entropy(variable_2)\n",
    "    # Normalizing by the average entropy\n",
    "    normalized_mi = mi / ((entropy_1 + entropy_2) / 2)\n",
    "    return normalized_mi\n",
    "\n",
    "def calculate_MI(original, augmented):\n",
    "# Assuming df_original and df_augmented are your dataframes\n",
    "    for column in original.columns:\n",
    "        # Ensure the data is in the correct format, e.g., continuous or discrete\n",
    "        # For continuous variables, you'd typically bin them before calculating mutual information\n",
    "        original_data = original[column].to_numpy()\n",
    "        augmented_data = augmented[column].to_numpy()\n",
    "\n",
    "        # Calculate normalized MI for each column\n",
    "        normalized_mi = calculate_normalized_mi(original_data, augmented_data)\n",
    "        print(f'Normalized Mutual Information for {column}: {normalized_mi}')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af999c46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.317597Z",
     "start_time": "2024-05-06T08:00:38.313489Z"
    }
   },
   "source": [
    "def find_cointegrated_pairs(data):\n",
    "    n = data.shape[1] # Number of columns in dataset\n",
    "    score_matrix = np.zeros((n,n))\n",
    "    pvalue_matrix = np.ones((n,n))\n",
    "    keys = data.keys()\n",
    "    pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            S1 = data[keys[i]]\n",
    "            S2 = data[keys[j]]\n",
    "            result = coint(S1,S2)\n",
    "            score = result[0]\n",
    "            pvalue = result[1]\n",
    "            score_matrix[i,j] = score\n",
    "            pvalue_matrix[i,j] = pvalue\n",
    "            if pvalue < 0.05:\n",
    "                pairs.append((keys[i],keys[j]))\n",
    "    return score_matrix, pvalue_matrix, pairs"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c11eee41",
   "metadata": {},
   "source": [
    "#### Data augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cf31a6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.327740Z",
     "start_time": "2024-05-06T08:00:38.324797Z"
    }
   },
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "def apply_lowess_smoothing(df, frac=0.1):\n",
    "    smoothed_data = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Apply LOWESS to each column\n",
    "    for column in df.columns:\n",
    "        smoothed_values = lowess(df[column], df.index, frac=frac, return_sorted=False)\n",
    "        smoothed_data[column] = smoothed_values\n",
    "    \n",
    "    return smoothed_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0657941b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.349700Z",
     "start_time": "2024-05-06T08:00:38.333422Z"
    }
   },
   "source": [
    "def cut_mix(df1, df2, alpha):\n",
    "    np.random.seed(42)  # Set seed only once externally if needed for reproducibility\n",
    "    assert df1.shape == df2.shape\n",
    "    size = len(df1)\n",
    "    cut_length = int(size * alpha)\n",
    "    cut_point = np.random.randint(0, size - cut_length)  # Ensure slicing does not exceed the size\n",
    "    \n",
    "    mixed_df = df1.copy()\n",
    "    mixed_df.iloc[cut_point:cut_point + cut_length] = df2.iloc[cut_point:cut_point + cut_length]\n",
    "    \n",
    "    return mixed_df\n",
    "\n",
    "def binary_mix(data1, data2, alpha=alpha):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    assert len(data1) == len(data2)\n",
    "    size = data1.shape\n",
    "    mask = np.random.binomial(1, alpha, size=size).astype(bool)\n",
    "    \n",
    "    mixed_data = np.where(mask, data1, data2)\n",
    "    \n",
    "    return pd.DataFrame(mixed_data, columns=data1.columns)\n",
    "\n",
    "def linear_mix(data1, data2, alpha=alpha):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    mixed_data = alpha * data1 + (1 - alpha) * data2\n",
    "    \n",
    "    return mixed_data\n",
    "\n",
    "def geometric_mix(data1, data2, alpha=alpha):\n",
    "    if len(data1) != len(data2):\n",
    "        raise ValueError(\"The lengths of data1 and data2 must be the same.\")\n",
    "        \n",
    "    # Replace zeros and negative values to avoid NaNs or complex numbers\n",
    "    data1_clipped = np.clip(data1, a_min=1e-10, a_max=None)\n",
    "    data2_clipped = np.clip(data2, a_min=1e-10, a_max=None)\n",
    "    \n",
    "    mixed_data = np.power(data1_clipped, alpha) * np.power(data2_clipped, (1 - alpha))\n",
    "    \n",
    "    return mixed_data\n",
    "def amplitude_mix(data1, data2, alpha=alpha):\n",
    "    assert len(data1) == len(data2)\n",
    "    \n",
    "    # Apply Fourier Transform to each column\n",
    "    fft1 = np.fft.rfft(data1, axis=0)\n",
    "    fft2 = np.fft.rfft(data2, axis=0)\n",
    "    \n",
    "    # Mix the magnitudes\n",
    "    magnitude1 = np.abs(fft1)\n",
    "    magnitude2 = np.abs(fft2)\n",
    "    mixed_magnitude = alpha * magnitude1 + (1 - alpha) * magnitude2\n",
    "    \n",
    "    # Keep the phase of the first data\n",
    "    phase1 = np.angle(fft1)\n",
    "    mixed_fft = mixed_magnitude * np.exp(1j * phase1)\n",
    "    \n",
    "    # Perform the inverse FFT and ensure the result is two-dimensional\n",
    "    mixed_data = np.fft.irfft(mixed_fft, axis=0)\n",
    "    if mixed_data.ndim == 1:\n",
    "        mixed_data = mixed_data.reshape(-1, 1)  # Reshape if the data is one-dimensional\n",
    "    \n",
    "    # Return a DataFrame with the same column names as data1\n",
    "    return pd.DataFrame(mixed_data, columns=data1.columns)\n",
    "\n",
    "\n",
    "### PROPOSE TECHNIQUE BELOW\n",
    "def proposed_mixup(df1, df2, threshold=0.1, alpha=alpha):\n",
    "    \n",
    "    def proposed_mixup_feature(data1, data2, threshold, alpha):\n",
    "        \n",
    "        def get_significant_frequencies(data, threshold):\n",
    "            \"\"\"\n",
    "            Perform Fourier Transform on data and identify frequencies with significant amplitude.\n",
    "\n",
    "            Args:\n",
    "            - data: Time series data.\n",
    "            - threshold: Threshold for significance, relative to the max amplitude.\n",
    "\n",
    "            Returns:\n",
    "            - significant_freq: Frequencies with significant amplitude.\n",
    "            - significant_ampl: Amplitude of the significant frequencies.\n",
    "            - full_spectrum: Full Fourier spectrum for all frequencies.\n",
    "            \"\"\"\n",
    "            # Perform Fourier Transform\n",
    "            spectrum = rfft(data)\n",
    "            frequencies = rfftfreq(data.size, d=1)  # Assuming unit time interval between samples\n",
    "\n",
    "            # Find significant amplitudes\n",
    "            amplitude = np.abs(spectrum)\n",
    "            significant_indices = amplitude > (amplitude.max() * threshold)\n",
    "            significant_freq = frequencies[significant_indices]\n",
    "            significant_ampl = amplitude[significant_indices]\n",
    "\n",
    "            return significant_freq, significant_ampl, spectrum\n",
    "\n",
    "        def phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2, alpha):\n",
    "            mixed_spectrum = np.copy(spectrum1)\n",
    "            freqs1 = rfftfreq(spectrum1.size, d=1)\n",
    "            freqs2 = rfftfreq(spectrum2.size, d=1)\n",
    "\n",
    "            for freq in sig_freq1:\n",
    "                index1 = np.argmin(np.abs(freqs1 - freq))\n",
    "                index2 = np.argmin(np.abs(freqs2 - freq))\n",
    "\n",
    "                if index1 >= len(sig_ampl1) or index2 >= len(sig_ampl2):\n",
    "                    continue  # Skip the frequency if the index is out of bounds\n",
    "\n",
    "                phase1 = np.angle(spectrum1[index1])\n",
    "                phase2 = np.angle(spectrum2[index2])\n",
    "\n",
    "                phase_diff = (phase2 - phase1) % (2 * np.pi)\n",
    "                phase_diff = phase_diff - 2 * np.pi if phase_diff > np.pi else phase_diff\n",
    "\n",
    "                new_amplitude = alpha * sig_ampl1[index1] + (1 - alpha) * sig_ampl2[index2]\n",
    "                new_phase = phase1 + alpha * phase_diff\n",
    "\n",
    "                mixed_spectrum[index1] = new_amplitude * np.exp(1j * new_phase)\n",
    "\n",
    "            return mixed_spectrum\n",
    "\n",
    "\n",
    "        def reconstruct_time_series(mixed_spectrum):\n",
    "            \"\"\"\n",
    "            Reconstruct time series from mixed spectrum using inverse Fourier Transform.\n",
    "\n",
    "            Returns:\n",
    "            - mixed_time_series: The reconstructed time series.\n",
    "            \"\"\"\n",
    "            # Perform inverse Fourier Transform\n",
    "            mixed_time_series = irfft(mixed_spectrum)\n",
    "\n",
    "            return mixed_time_series\n",
    "\n",
    "        # Step 1: Get significant frequencies and amplitude for both time series\n",
    "        sig_freq1, sig_ampl1, spectrum1 = get_significant_frequencies(data1, threshold)\n",
    "        sig_freq2, sig_ampl2, spectrum2 = get_significant_frequencies(data2, threshold)\n",
    "\n",
    "        # Step 2: Identify significant frequencies (already done in step 1)\n",
    "\n",
    "        # Step 3: Phase and Magnitude Mixup\n",
    "        mixed_spectrum = phase_mixup(sig_freq1, sig_ampl1, spectrum1, sig_freq2, sig_ampl2, spectrum2, alpha)\n",
    "\n",
    "        # Step 4: Reconstruction of the time series\n",
    "        mixed_time_series = reconstruct_time_series(mixed_spectrum)\n",
    "        return mixed_time_series\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    \n",
    "    for feature in df1.columns:\n",
    "        output_df[feature] = proposed_mixup_feature(df1[feature].values, df2[feature].values, threshold, alpha)\n",
    "        \n",
    "    return output_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca52eb48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.383463Z",
     "start_time": "2024-05-06T08:00:38.374827Z"
    }
   },
   "source": [
    "def jittering(ts, noise_level=0.05):\n",
    "    np.random.seed(42)\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=len(ts))\n",
    "    return pd.Series(ts + noise)\n",
    "\n",
    "def flipping(ts):\n",
    "    return pd.Series(np.flip(ts))\n",
    "\n",
    "def scaling(ts, scaling_factor=1.5):\n",
    "    return pd.Series(ts * scaling_factor)\n",
    "\n",
    "def magnitude_warping(ts, sigma=0.2, knot=4):\n",
    "    np.random.seed(42)\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, ))\n",
    "    indices = np.linspace(0, len(ts)-1, num=knot+2)\n",
    "    sp = CubicSpline(indices, random_warps)\n",
    "    warp_values = sp(np.arange(len(ts)))\n",
    "    return pd.Series(ts * warp_values)\n",
    "\n",
    "def permutation(ts, n_segments=5):\n",
    "    np.random.seed(42)\n",
    "    permutated_ts = np.copy(ts)\n",
    "    segments = np.array_split(permutated_ts, n_segments)\n",
    "    np.random.shuffle(segments)\n",
    "    return pd.Series(np.concatenate(segments))\n",
    "\n",
    "def time_warping(ts, sigma=0.2, knot=4):\n",
    "    np.random.seed(42)\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    time_steps = np.arange(ts.shape[0])\n",
    "    random_steps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, ts.shape[1]))\n",
    "    indices = np.linspace(0, len(ts)-1, num=knot+2)\n",
    "    sp = CubicSpline(indices, random_steps)\n",
    "    warp_values = sp(time_steps)\n",
    "    return pd.Series(warp_values * ts)\n",
    "\n",
    "def stl_augment(data, period=61):\n",
    "    ts = data.asfreq('B')\n",
    "    ts = ts.interpolate()\n",
    "    # Apply STL decomposition\n",
    "    stl = STL(ts, seasonal=period) \n",
    "    result = stl.fit()\n",
    "    seasonal, trend, remainder = result.seasonal, result.trend, result.resid\n",
    "    bootstrapped_remainder = resample(remainder, replace=True, n_samples=len(remainder), random_state=42)\n",
    "    bootstrapped_remainder.index = ts.index\n",
    "    augmented_signal = trend + seasonal + bootstrapped_remainder\n",
    "    augmented_signal = np.maximum(augmented_signal, 0)\n",
    "    augmented_signal = augmented_signal[data.index]\n",
    "    return augmented_signal\n",
    "\n",
    "# Function to plot original and augmented series\n",
    "def plot_augmented_ts(original_ts, augmented_ts, title='Time Series Augmentation'):\n",
    "    augmented_ts.index = original_ts.index\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(original_ts, label='Original')\n",
    "    plt.plot(augmented_ts, label='Augmented')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "15292477",
   "metadata": {},
   "source": [
    "#### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b1842bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.389728Z",
     "start_time": "2024-05-06T08:00:38.386236Z"
    }
   },
   "source": [
    "def create_augmented_data(df1_, df2_, method, alpha=alpha):\n",
    "    df1 = df1_.copy()\n",
    "    df2 = df2_.copy()\n",
    "    \n",
    "    if method == 'cut_mix':\n",
    "        df = cut_mix(df1, df2, alpha)\n",
    "    elif method == 'binary_mix':\n",
    "        df = binary_mix(df1, df2, alpha)\n",
    "    elif method == 'linear_mix':\n",
    "        df = linear_mix(df1, df2, alpha)\n",
    "    elif method == 'geometrix_mix':\n",
    "        df = geometric_mix(df1, df2, alpha)\n",
    "    elif method == 'amplitude_mix':\n",
    "        df = amplitude_mix(df1, df2, alpha)\n",
    "    elif method == 'proposed_mix':\n",
    "        df = proposed_mixup(df1, df2, alpha)\n",
    "\n",
    "    # Original\n",
    "    else:\n",
    "        df = df1.copy()\n",
    "        \n",
    "    return df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2284786c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.401106Z",
     "start_time": "2024-05-06T08:00:38.393074Z"
    }
   },
   "source": [
    "# Define the LSTM model creation function\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=input_shape),\n",
    "        LSTM(50),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def create_sequences(features, target, time_steps):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(features) - time_steps):\n",
    "        Xs.append(features[i:(i + time_steps)])\n",
    "        ys.append(target[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Train the LSTM model and return it along with scalers and the test set\n",
    "def train_evaluate_lstm(features, target, time_steps, epochs, batch_size):\n",
    "    X, y = create_sequences(features, target, time_steps)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    model = create_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "#     test_predictions = model.predict(X_test)\n",
    "#     test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "#     print(f\"Test RMSE: {test_rmse}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Predict on new data using the trained model and calculate prediction intervals\n",
    "def predict_new_data(model, new_data, feature_scaler, target_scaler, quantile, time_steps):\n",
    "    new_features_scaled = feature_scaler.transform(new_data)\n",
    "    X_new, _ = create_sequences(new_features_scaled, np.zeros((len(new_features_scaled), new_data.shape[1])), time_steps)\n",
    "    predictions = model.predict(X_new)\n",
    "    return predictions"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90ffaac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.410822Z",
     "start_time": "2024-05-06T08:00:38.402791Z"
    }
   },
   "source": [
    "def classification_accuracy(df_, features_list, X_test_og, y_test_og, scaler_aapl):\n",
    "    np.random.seed(seed_value)\n",
    "    python_random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    # df should be a dataframe which contains all the features and Close (no Return column)\n",
    "    df = df_.copy()\n",
    "    # df should be a dataframe which contains all the features and Close (no Return column)\n",
    "    df['Return'] = np.log(df['Close']).diff()\n",
    "    df.dropna(subset=['Return'], inplace=True)\n",
    "    features = df[features_list]\n",
    "    target = df['Return']\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_features = scaler.fit_transform(features.values)\n",
    "    scaled_target = scaler.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "    # Create sequences\n",
    "    time_steps = 20  # Number of time steps for LSTM\n",
    "    X, y = create_sequences(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(1))  # Prediction of the next closing price\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=128, verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    predicted_returns = model.predict(X_test_og)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_og, predicted_returns))\n",
    "    print('Test RMSE: ', rmse)\n",
    "\n",
    "    # Invert scaling to compare predictions against the actual returns\n",
    "    predicted_returns = scaler.inverse_transform(predicted_returns)\n",
    "\n",
    "    binary_predicted = (predicted_returns > 0).astype(int)\n",
    "\n",
    "    # Do the same for actual returns\n",
    "    binary_actual = (scaler_aapl.inverse_transform(y_test_og) > 0).astype(int)\n",
    "\n",
    "    # Calculate the proportion of correct directional predictions\n",
    "    directional_accuracy = np.mean(binary_predicted == binary_actual)\n",
    "    print(f'Directional Accuracy: {directional_accuracy * 100:.2f}%') \n",
    "    \n",
    "    return directional_accuracy, predicted_returns"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "249aa186",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "913167a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.445241Z",
     "start_time": "2024-05-06T08:00:38.439836Z"
    }
   },
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned by Optuna\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    lstm_units = trial.suggest_categorical('lstm_units', [50, 100, 150])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    epochs = trial.suggest_int('epochs', 20, 100)\n",
    "    \n",
    "    # Dictionary to hold RMSE for each stock\n",
    "    stock_rmse = {}\n",
    "    \n",
    "    for stock, df in historical_data_augmented.items():\n",
    "        # Preprocess the data\n",
    "        df = df.copy()\n",
    "        rets = df['Close'].pct_change().dropna()\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(df[df.columns].values)\n",
    "        seq_len = 20\n",
    "        X, y = create_sequences(scaled_features, rets.values, seq_len)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        \n",
    "        # Model architecture\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "        model = Sequential()\n",
    "        for i in range(n_layers):\n",
    "            model.add(LSTM(units=lstm_units, return_sequences=(i < n_layers - 1)))\n",
    "            model.add(Dropout(rate=dropout_rate))\n",
    "        model.add(Dense(units=1))\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        # Predictions and evaluate\n",
    "        predictions = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "        stock_rmse[stock] = rmse\n",
    "    \n",
    "    # Calculate the average RMSE across all stocks\n",
    "    average_rmse = np.mean(list(stock_rmse.values()))\n",
    "    \n",
    "    return average_rmse"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a1f71a2a",
   "metadata": {},
   "source": [
    "### Pull Data from Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5c62563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.466430Z",
     "start_time": "2024-05-06T08:00:38.449128Z"
    }
   },
   "source": [
    "start_date = '2010-01-01'\n",
    "end_date = '2023-01-01'\n",
    "\n",
    "# Define the list of Dow Jones Industrial Average companies\n",
    "tickers = [\n",
    "    \"MMM\", \"AXP\", \"AMGN\", \"AAPL\", \"BA\", \"CAT\", \"CVX\", \"CSCO\", \"KO\", \"DIS\"\n",
    "    , \"GS\", \"HD\", \"HON\", \"IBM\", \"INTC\", \"JNJ\", \"JPM\", \"MCD\", \"MRK\",\n",
    "    \"MSFT\", \"NKE\", \"PG\", \"CRM\", \"TRV\", \"UNH\", \"V\", \"WBA\", \"WMT\"\n",
    "]\n",
    "\n",
    "# tickers = ['AAPL']\n",
    "# Create a dictionary to store historical data for each company\n",
    "historical_data = {}\n",
    "\n",
    "# Loop through the Dow companies and retrieve historical data\n",
    "for ticker in tickers:\n",
    "    stock_data = get_stock_data(ticker, start_date, end_date)\n",
    "    historical_data[ticker] = stock_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e644fd52",
   "metadata": {},
   "source": [
    "## Hypothesis: Augmenting a time series with another time series can lead to better forecasting of returns.\n",
    "### Find Cointegrated Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26532bce",
   "metadata": {},
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for stock, data in historical_data.items():\n",
    "    df[stock] = data['Adj Close']\n",
    "    \n",
    "display(df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "969e3a3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.509205Z",
     "start_time": "2024-05-06T08:00:38.506980Z"
    }
   },
   "source": [
    "# _, _, pairs = find_cointegrated_pairs(df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3786e6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.514932Z",
     "start_time": "2024-05-06T08:00:38.512637Z"
    }
   },
   "source": [
    "# pairs"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c0252f1c",
   "metadata": {},
   "source": [
    "[('AXP', 'HD'),\n",
    " ('AMGN', 'KO'),\n",
    " ('AMGN', 'HON'),\n",
    " ('AMGN', 'JNJ'),\n",
    " ('AMGN', 'TRV'),\n",
    " ('AAPL', 'MSFT'),\n",
    " ('CSCO', 'JPM'),\n",
    " ('KO', 'MRK'),\n",
    " ('KO', 'UNH'),\n",
    " ('HD', 'HON'),\n",
    " ('HD', 'JPM'),\n",
    " ('HD', 'NKE'),\n",
    " ('HD', 'V'),\n",
    " ('HD', 'WMT'),\n",
    " ('HON', 'JNJ'),\n",
    " ('HON', 'JPM'),\n",
    " ('IBM', 'MSFT'),\n",
    " ('IBM', 'WMT'),\n",
    " ('JNJ', 'TRV'),\n",
    " ('JPM', 'NKE'),\n",
    " ('JPM', 'V'),\n",
    " ('JPM', 'WMT'),\n",
    " ('NKE', 'WMT'),\n",
    " ('PG', 'WMT'),\n",
    " ('V', 'WMT')]\n",
    " \n",
    " These are the cointegrated pairs. Let me test to see if the augmenetation improves the forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ea4a069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.584166Z",
     "start_time": "2024-05-06T08:00:38.521301Z"
    }
   },
   "source": [
    "def test_coint_pairs(ticker1, ticker2):\n",
    "    aapl = historical_data[ticker1]\n",
    "    msft = historical_data[ticker2]\n",
    "\n",
    "    aapl_engineered = engineer_features(aapl)\n",
    "    msft_engineered = engineer_features(msft)\n",
    "\n",
    "    # Concatenate the dataframes before scaling to ensure the same scaling is applied\n",
    "    combined_engineered = pd.concat([aapl_engineered, msft_engineered])\n",
    "\n",
    "    # Fit the scaler on the combined dataset\n",
    "    main_scaler = MinMaxScaler(feature_range=(1, 2))\n",
    "\n",
    "    # Fit and transform the data\n",
    "    scaled_combined = main_scaler.fit_transform(combined_engineered)\n",
    "\n",
    "    # Split back into separate datasets after scaling\n",
    "    scaled_aapl = scaled_combined[:len(aapl_engineered), :]\n",
    "    scaled_msft = scaled_combined[len(aapl_engineered):, :]\n",
    "\n",
    "    # Re-convert to DataFrame if necessary, using the original indexes and columns\n",
    "    aapl = pd.DataFrame(scaled_aapl, index=aapl_engineered.index, columns=aapl_engineered.columns)\n",
    "    msft = pd.DataFrame(scaled_msft, index=msft_engineered.index, columns=msft_engineered.columns)\n",
    "    \n",
    "    dates = aapl.index\n",
    "    cols = aapl.columns\n",
    "    \n",
    "    cut_mix = create_augmented_data(aapl, msft, method='cut_mix')\n",
    "    binary_mix = create_augmented_data(aapl, msft, method='binary_mix')\n",
    "    linear_mix = create_augmented_data(aapl, msft, method='linear_mix')\n",
    "    geometric_mix = create_augmented_data(aapl, msft, method='geometrix_mix')\n",
    "    amplitude_mix = create_augmented_data(aapl, msft, method='amplitude_mix')\n",
    "    proposed_mix = create_augmented_data(aapl, msft, method='proposed_mix')\n",
    "\n",
    "    cut_mix = pd.DataFrame(main_scaler.inverse_transform(cut_mix), index=dates, columns=cols)\n",
    "    binary_mix = pd.DataFrame(main_scaler.inverse_transform(binary_mix), index=dates, columns=cols)\n",
    "    linear_mix = pd.DataFrame(main_scaler.inverse_transform(linear_mix), index=dates, columns=cols)\n",
    "    geometric_mix = pd.DataFrame(main_scaler.inverse_transform(geometric_mix), index=dates, columns=cols)\n",
    "    amplitude_mix = pd.DataFrame(main_scaler.inverse_transform(amplitude_mix), index=dates, columns=cols)\n",
    "    proposed_mix = pd.DataFrame(main_scaler.inverse_transform(proposed_mix), index=dates, columns=cols)\n",
    "    \n",
    "    augmented_datasets = {\n",
    "        'cut_mix': cut_mix,\n",
    "        'binary_mix': binary_mix,\n",
    "        'linear_mix': linear_mix,\n",
    "        'geometric_mix': geometric_mix,\n",
    "        'amplitude_mix': amplitude_mix,\n",
    "        'proposed_mix': proposed_mix,\n",
    "    }\n",
    "    \n",
    "    features_list = ['Close', 'Volume', 'RSI', 'VWAP', 'high_to_low_ratio', 'open_to_close_ratio', 'volatility_10']\n",
    "    data = augmented_datasets.copy()\n",
    "\n",
    "    # init\n",
    "    aapl['Return'] = np.log(aapl['Close']).diff()\n",
    "    aapl.dropna(subset=['Return'], inplace=True)\n",
    "\n",
    "    features_aapl = aapl[['Close', 'Volume', 'RSI', 'VWAP', 'high_to_low_ratio', 'open_to_close_ratio', 'volatility_10']]\n",
    "    target_aapl = aapl['Return']\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler_aapl = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_features_aapl = scaler_aapl.fit_transform(features_aapl.values)\n",
    "    scaled_target_aapl = scaler_aapl.fit_transform(target_aapl.values.reshape(-1, 1))\n",
    "\n",
    "    # Create sequences\n",
    "    time_steps = 20  # Number of time steps for LSTM\n",
    "    X, y = create_sequences(scaled_features_aapl, scaled_target_aapl, time_steps)\n",
    "\n",
    "    # Split the data\n",
    "    X_train_aapl, X_test_aapl, y_train_aapl, y_test_aapl = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(X_train_aapl.shape[1], X_train_aapl.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(1))  # Prediction of the next closing price\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_aapl, y_train_aapl, epochs=100, batch_size=128, verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    predicted_returns = model.predict(X_test_aapl)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_aapl, predicted_returns))\n",
    "    print('Test RMSE: ', rmse)\n",
    "\n",
    "    # Invert scaling to compare predictions against the actual returns\n",
    "    predicted_returns = scaler_aapl.inverse_transform(predicted_returns)\n",
    "\n",
    "    binary_predicted = (predicted_returns > 0).astype(int)\n",
    "\n",
    "    # Do the same for actual returns\n",
    "    binary_actual = (scaler_aapl.inverse_transform(y_test_aapl) > 0).astype(int)\n",
    "    \n",
    "    directional_accuracy_aapl = np.mean(binary_predicted == binary_actual)\n",
    "    print(f'Directional Accuracy of original TS: {directional_accuracy_aapl * 100:.2f}%') \n",
    "    \n",
    "    results = {}\n",
    "    results['original'] = {\n",
    "            'direction_accuracy': round((directional_accuracy_aapl * 100), 2),\n",
    "            'predictions': predicted_returns\n",
    "    }\n",
    "    \n",
    "    for method, dataset in augmented_datasets.items():\n",
    "        res, predicted_returns = classification_accuracy(dataset, features_list, X_test_aapl, y_test_aapl, scaler_aapl)\n",
    "        results[method] = {\n",
    "            'direction_accuracy': round((res * 100), 2),\n",
    "            'predictions': predicted_returns\n",
    "        }\n",
    "    return results"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99242f6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:00:38.614801Z",
     "start_time": "2024-05-06T08:00:38.585347Z"
    }
   },
   "source": [
    "ticker_pairs = [('AXP', 'HD'), ('AMGN', 'KO'), ('AMGN', 'HON'), ('AMGN', 'JNJ'), ('AMGN', 'TRV'), ('AAPL', 'MSFT'), ('CSCO', 'JPM'), ('KO', 'MRK'), ('KO', 'UNH'), ('HD', 'HON'), ('HD', 'JPM'), ('HD', 'NKE'), ('HD', 'V'), ('HD', 'WMT'), ('HON', 'JNJ'), ('HON', 'JPM'), ('IBM', 'MSFT'), ('IBM', 'WMT'), ('JNJ', 'TRV'), ('JPM', 'NKE'), ('JPM', 'V'), ('JPM', 'WMT'), ('NKE', 'WMT'), ('PG', 'WMT'), ('V', 'WMT')]\n",
    "scores = {}\n",
    "for pair in ticker_pairs:\n",
    "    res = test_coint_pairs(pair[0], pair[1])\n",
    "    score = []\n",
    "    for k,v in res.items():\n",
    "        score.append((k,v['direction_accuracy']))\n",
    "    scores[pair] = score"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e7363",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:02:27.672771Z",
     "start_time": "2024-05-06T08:02:27.667201Z"
    }
   },
   "source": [
    "scores"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7693aee",
   "metadata": {},
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame.from_dict({k: dict(v) for k, v in scores.items()}, orient='index')\n",
    "\n",
    "# # Optionally, you can give names to the index\n",
    "# df.index.names = ['Pair']\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22f11f",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
